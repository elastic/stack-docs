[chapter, role="xpack"]
[[ml-ad-finding-anomalies]]
= Finding anomalies in time series data
++++
<titleabbrev>Finding anomalies</titleabbrev>
++++

The {ml-features} automate the analysis of time series data by creating
accurate baselines of normal behavior in the data and identifying anomalous
patterns in that data.

Using <<ml-ad-algorithms,proprietary {ml} algorithms>>, the following
circumstances are detected, scored, and linked with statistically significant
influencers in the data:

* Anomalies related to temporal deviations in values, counts, or frequencies
* Statistical rarity
* Unusual behaviors for a member of a population

Automated periodicity detection and quick adaptation to changing data ensure
that you donâ€™t need to specify algorithms, models, or other data science-related
configurations in order to get the benefits of {ml}.

To use the {ml-features} to analyze your data, you can create an {anomaly-job}
and send your data to that job. The results of {ml} analysis are stored in {es}
and you can use {kib} to help you visualize and explore the results. For example,
charts illustrate the actual data values, the bounds for the expected values,
and the anomalies that occur outside these bounds:

[role="screenshot"]
image::images/overview-smv.jpg["Example screenshot from the Machine Learning Single Metric Viewer in Kibana"]

[discrete]
[[ml-ad-algorithms]]
== {anomaly-detect-cap} algorithms

The {anomaly-detect} {ml-features} use a bespoke amalgamation of different
techniques such as clustering, various types of time series decomposition,
Bayesian distribution modeling, and correlation analysis. These analytics
provide sophisticated real-time automated {anomaly-detect} for time series data.

The {ml} analytics statistically model the time-based characteristics of your
data by observing historical behavior and adapting to new data. The model
represents a baseline of normal behavior and can therefore be used to determine
how anomalous new events are.

{anomaly-detect-cap} results are written for each <<ml-buckets,bucket span>>.
These results include scores that are aggregated in order to reduce noise and
normalized in order to rank the most mathematically significant anomalies. For
more information, see <<ml-bucket-results>> and <<ml-influencer-results>>.

[discrete]
[[ml-ad-define-problem]]
== 1: Define the problem

The {ml-features} in {stack} enable you to seek anomalies in your data in many
different ways. For example, there are functions that calculate metrics,
analyze geographic data, or seek rare events in your data set. You can also
optionally analyze your data relative to a specific population or group the data
based on specific attributes. For the full list of functions, see
<<ml-functions>>.

The most important decision is what type of anomalous behavior you want to
detect and what type of data you have available for the task.

[discrete]
[[ml-ad-setup]]
== 2: Set up environment

If you want to use {ml-features}, there must be at least one {ml} node in
your cluster and all master-eligible nodes must have {ml} enabled. By default,
all nodes are {ml} nodes. For more information about these settings, see 
{ref}/modules-node.html#ml-node[{ml} nodes].

If {stack-security-features} are enabled, you must also ensure your users have
the necessary privileges. See <<setup>>.

[NOTE]
===============================
If your data is located outside of {es}, you cannot use {kib} to create
your jobs and you cannot use {dfeeds} to retrieve your data in real time.
Posting data directly to {anomaly-jobs} is deprecated, in a future major version
a {dfeed} will be required.
===============================

[discrete]
[[ml-ad-create-job]]
== 3: Create a job

{anomaly-jobs-cap} contain the configuration information and metadata
necessary to perform an analytics task.

You can create {anomaly-jobs} by using the
{ref}/ml-put-job.html[Create {anomaly-jobs} API]. {kib} also provides the
following wizards to make it easier to create jobs:

[role="screenshot"]
image::images/ml-create-job.jpg[Create New Job]

A _single metric job_ is a simple job that contains a single _detector_. A
detector defines the type of analysis that will occur and which fields to
analyze. In addition to limiting the number of detectors, the single metric job
creation wizard omits many of the more advanced configuration options.

A _multi-metric job_ can contain more than one detector, which is more efficient
than running multiple jobs against the same data.

A _population job_ detects activity that is unusual compared to the behavior of
the population. For more information, see <<ml-configuring-populations>>.

A _categorization job_ groups log messages into categories and uses
<<ml-count-functions,count>> or <<ml-rare-functions,rare>> functions to detect
anomalies within them. See <<ml-configuring-categories>>.

An _advanced job_ can contain multiple detectors and enables you to configure all
job settings.

{kib} can also recognize certain types of data and provide specialized wizards
for that context. For example, if you added the sample web log data set, the
following wizard appears:

[role="screenshot"]
image::images/ml-data-recognizer-sample.jpg[A screenshot of the {kib} sample data web log job creation wizard]

TIP: Alternatively, after you load a sample data set on the {kib} home page, you
can click *View data* > *ML jobs*. There are {anomaly-jobs} for both the sample
eCommerce orders data set and the sample web logs data set.

//TBD: Abbreviate this information and mention Fleet integration packages

If you use {apm-get-started-ref}/overview.html[Elastic APM], {kib} also detects
this data and provides wizards for {anomaly-jobs}. For example:

[role="screenshot"]
image::images/ml-data-recognizer-apm.jpg[A screenshot of the {kib} APM job creation wizard]

If you use {filebeat-ref}/index.html[{filebeat}]
to ship access logs from your
http://nginx.org/[Nginx] and https://httpd.apache.org/[Apache] HTTP servers to
{es} and store it using fields and datatypes from the
{ecs-ref}/ecs-reference.html[Elastic Common Schema (ECS)], the following wizards
appear:

[role="screenshot"]
image::images/ml-data-recognizer-filebeat.jpg[A screenshot of the {filebeat} job creation wizards]

If you use {auditbeat-ref}/index.html[{auditbeat}] to audit process
activity on your systems, the following wizards appear:

[role="screenshot"]
image::images/ml-data-recognizer-auditbeat.jpg[A screenshot of the {auditbeat} job creation wizards]

Likewise, if you use the {metricbeat-ref}/metricbeat-module-system.html[{metricbeat} system module] to monitor your servers, the following
wizards appear:

[role="screenshot"]
image::images/ml-data-recognizer-metricbeat.jpg[A screenshot of the {metricbeat} job creation wizards]

These wizards create {anomaly-jobs}, dashboards, searches, and visualizations 
that are customized to help you analyze your {auditbeat}, {filebeat}, and
{metricbeat} data.

For a list of all the customized jobs, see <<ootb-ml-jobs>>.

When you create an {anomaly-job} in {kib}, the job creation wizards can
provide advice based on the characteristics of your data. By heeding these
suggestions, you can create jobs that are more likely to produce insightful {ml}
results.

[discrete]
[[bucket-span]]
=== Bucket span

The bucket span is the time interval that {ml} analytics use to summarize and
model data for your job. When you create an {anomaly-job} in {kib}, you can
choose to estimate a bucket span value based on your data characteristics. 

NOTE: The bucket span must contain a valid time interval. See
{time-units}[Time units].

If you choose a value that is larger than one day or is significantly different 
than the estimated value, you receive an informational message. For more 
information about choosing an appropriate bucket span, see <<ml-buckets>>.

[discrete]
[[cardinality]]
=== Cardinality

If there are logical groupings of related entities in your data, {ml} analytics
can make data models and generate results that take these groupings into
consideration. For example, you might choose to split your data by user ID and
detect when users are accessing resources differently than they usually do.

If the field that you use to split your data has many different values, the
job uses more memory resources. In particular, if the cardinality of the
`by_field_name`, `over_field_name`, or `partition_field_name` is greater than 
1000, you are advised that there might be high memory usage. 

Likewise if you are performing population analysis and the cardinality of the
`over_field_name` is below 10, you are advised that this might not be a suitable
field to use. For more information, see <<ml-configuring-populations>>.

[discrete]
[[detectors]]
=== Detectors

Each {anomaly-job} must have one or more _detectors_. A detector applies an
analytical function to specific fields in your data. If your job does not
contain a  detector or the detector does not contain a 
<<ml-functions,valid function>>, you receive an error.

If a job contains duplicate detectors, you also receive an error. Detectors are 
duplicates if they have the same `function`, `field_name`, `by_field_name`, 
`over_field_name` and `partition_field_name`. 

[discrete]
[[influencers]]
=== Influencers

See <<ml-influencers>>.


[discrete]
[[model-memory-limits]]
=== Model memory limits

For each {anomaly-job}, you can optionally specify a `model_memory_limit`, which
is the approximate maximum amount of memory resources that are required for
analytical processing. The default value is 1 GB. Once this limit is approached,
data pruning becomes more aggressive. Upon exceeding this limit, new entities
are not modeled. 

You can also optionally specify the `xpack.ml.max_model_memory_limit` setting. 
By default, it's not set, which means there is no upper bound on the acceptable 
`model_memory_limit` values in your jobs. 

TIP: If you set the `model_memory_limit` too high, it will be impossible to open 
the job; jobs cannot be allocated to nodes that have insufficient memory to run 
them.

If the estimated model memory limit for an {anomaly-job} is greater than the
model memory limit for the job or the maximum model memory limit for the cluster,
the job creation wizards in {kib} generate a warning. If the estimated memory 
requirement is only a little higher than the `model_memory_limit`, the job will 
probably produce useful results. Otherwise, the actions you take to address 
these warnings vary depending on the resources available in your cluster:

* If you are using the default value for the `model_memory_limit` and the {ml} 
nodes in the cluster have lots of memory, the best course of action might be to 
simply increase the job's `model_memory_limit`. Before doing this, however, 
double-check that the chosen analysis makes sense. The default 
`model_memory_limit` is relatively low to avoid accidentally creating a job that 
uses a huge amount of memory.
* If the {ml} nodes in the cluster do not have sufficient memory to accommodate 
a job of the estimated size, the only options are:
** Add bigger {ml} nodes to the cluster, or 
** Accept that the job will hit its memory limit and will not necessarily find 
all the anomalies it could otherwise find.

If you are using {ece} or the hosted Elasticsearch Service on Elastic Cloud, 
`xpack.ml.max_model_memory_limit` is set to prevent you from creating jobs 
that cannot be allocated to any {ml} nodes in the cluster. If you find that you 
cannot increase `model_memory_limit` for your {ml} jobs, the solution is to 
increase the size of the {ml} nodes in your cluster.

[discrete]
[[dedicated-indices]]
=== Dedicated indices

For each {anomaly-job}, you can optionally specify a dedicated index to store 
the {anomaly-detect} results. As {anomaly-jobs} may produce a large amount 
of results (for example, jobs with many time series, small bucket span, or with 
long running period), it is recommended to use a dedicated results index by 
choosing the **Use dedicated index** option in {kib} or specifying the 
`results_index_name` via the {ref}/ml-put-job.html[Create {anomaly-jobs} API].

[discrete]
[[ml-ad-open-job]]
== 4: Open the job

An {anomaly-job} must be opened in order for it to be ready to receive and
analyze data. It can be opened and closed multiple times throughout its
lifecycle.

After you start the job, you can start the {dfeed}, which retrieves data from
your cluster. A {dfeed} can be started and stopped multiple times throughout its
lifecycle.

You can perform both these tasks in {kib} or use the
{ref}/ml-open-job.html[open {anomaly-jobs}] and
{ref}/ml-start-datafeed.html[start {dfeeds}] APIs.

[discrete]
[[ml-ad-view-results]]
== 5: View the job results

After the {anomaly-job} has processed some data, you can view the results in
{kib}.

TIP: Depending on the capacity of your machine, you might need to wait a few
seconds for the {ml} analysis to generate initial results.

There are two tools for examining the results from {anomaly-jobs} in {kib}: the
**Anomaly Explorer** and the **Single Metric Viewer**.

[discrete]
[[ml-ad-forecast]]
== 6: Forecast future behavior

After the {ml-features} create baselines of normal behavior for your data,
you can use that information to extrapolate future behavior.

You can use a forecast to estimate a time series value at a specific future date.
For example, you might want to determine how many users you can expect to visit
your website next Sunday at 0900.

You can also use it to estimate the probability of a time series value occurring
at a future date. For example, you might want to determine how likely it is that
your disk utilization will reach 100% before the end of next week.

Each forecast has a unique ID, which you can use to distinguish between forecasts
that you created at different times. You can create a forecast by using the
{ref}/ml-forecast.html[forecast {anomaly-jobs} API] or by using {kib}. For
example:

[role="screenshot"]
image::images/overview-forecast.jpg["Example screenshot from the Machine Learning Single Metric Viewer in Kibana"]

The yellow line in the chart represents the predicted data values. The
shaded yellow area represents the bounds for the predicted values, which also
gives an indication of the confidence of the predictions.

When you create a forecast, you specify its _duration_, which indicates how far
the forecast extends beyond the last record that was processed. By default, the
duration is 1 day. Typically the farther into the future that you forecast, the
lower the confidence levels become (that is to say, the bounds increase).
Eventually if the confidence levels are too low, the forecast stops.
For more information about limitations that affect your ability to create a
forecast, see <<ml-forecast-config-limitations>>.

You can also optionally specify when the forecast expires. By default, it
expires in 14 days and is deleted automatically thereafter. You can specify a
different expiration period by using the `expires_in` parameter in the
{ref}/ml-forecast.html[forecast {anomaly-jobs} API].

[discrete]
[[ml-ad-close-job]]
== 7: Close the job

If you need to stop your {anomaly-job}, an orderly shutdown ensures that:

* {dfeeds-cap} are stopped
* Buffers are flushed
* Model history is pruned
* Final results are calculated
* Model snapshots are saved
* {anomaly-jobs-cap} are closed

This process ensures that jobs are in a consistent state in case you want to
subsequently re-open them.

[discrete]
=== Stopping {dfeeds}

When you stop a {dfeed}, it ceases to retrieve data from {es}. You can stop a
{dfeed} by using {kib} or the
{ref}/ml-stop-datafeed.html[stop {dfeeds} API]. For example, the following
request stops the `feed1` {dfeed}:

[source,console]
--------------------------------------------------
POST _ml/datafeeds/feed1/_stop
--------------------------------------------------
// TEST[skip:setup:server_metrics_startdf]

NOTE: You must have `manage_ml`, or `manage` cluster privileges to stop {dfeeds}.
For more information, see {ref}/security-privileges.html[Security privileges].

A {dfeed} can be started and stopped multiple times throughout its lifecycle.

[discrete]
=== Stopping all {dfeeds}

If you are upgrading your cluster, you can use the following request to stop all
{dfeeds}:

[source,console]
----------------------------------
POST _ml/datafeeds/_all/_stop
----------------------------------
// TEST[skip:needs-licence]

[discrete]
=== Closing {anomaly-jobs}

When you close an {anomaly-job}, it cannot receive data or perform analysis
operations. If a job is associated with a {dfeed}, you must stop the {dfeed}
before you can close the job. If the {dfeed} has an end date, the job closes
automatically on that end date.

You can close a job by using the
{ref}/ml-close-job.html[close {anomaly-job} API]. For 
example, the following request closes the `job1` job:

[source,console]
--------------------------------------------------
POST _ml/anomaly_detectors/job1/_close
--------------------------------------------------
// TEST[skip:setup:server_metrics_openjob]

NOTE: You must have `manage_ml`, or `manage` cluster privileges to stop {dfeeds}.
For more information, see {ref}/security-privileges.html[Security privileges].

{anomaly-jobs-cap} can be opened and closed multiple times throughout their
lifecycle.

[discrete]
=== Closing all {anomaly-jobs}

If you are upgrading your cluster, you can use the following request to close
all open {anomaly-jobs} on the cluster:

[source,console]
----------------------------------
POST _ml/anomaly_detectors/_all/_close
----------------------------------
// TEST[skip:needs-licence]

[discrete]
== Next steps

For a more detailed walk-through of {ml-features}, see <<ml-getting-started>>.

For more advanced settings and scenarios, see <<anomaly-examples>>.

Refer to <<anomaly-detection-scale>> to learn more about the particularities of 
large {anomaly-jobs}.
