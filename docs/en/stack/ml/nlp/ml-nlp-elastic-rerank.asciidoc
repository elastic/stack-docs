[[ml-nlp-rerank]]
= Elastic Rerank

Elastic Rerank is a state-of-the-art cross-encoder reranking model trained by Elastic that helps you improve search relevance with a few simple API calls.
Elastic Rerank is Elastic's first semantic reranking model and is available out-of-the-box in supporting Elastic deployments.

This model is recommended for English language documents and queries.

Use Elastic Rerank to improve existing search applications including:

* Traditional BM25 scoring
* Hybrid semantic search
* Retrieval Augmented Generation (RAG)

The model can significantly improve search result quality by reordering results based on deeper semantic understanding of queries and documents.

When reranking BM25 results, it provides an average 40% improvement in ranking quality on a diverse benchmark of retrieval tasksâ€” matching the performance of models 11x its size.

[discrete]
[[ml-nlp-rerank-availability]]
== Availability and requirements 

IMPORTANT: This functionality is in technical preview and may be changed or removed in a future release. Elastic will work to fix any issues, but features in technical preview are not subject to the support SLA of official GA features.

[discrete]
[[ml-nlp-rerank-availability-serverless-]]
=== Elastic Cloud Serverless

Elastic Rerank is available in Elasticsearch Serverless projects as of November 25, 2024.

[discrete]
[[ml-nlp-rerank-availability-elastic-stack]]
=== Elastic stack (Cloud hosted and self-managed deployments)

Elastic Rerank will be available in Elastic Stack version 8.17+:

* To use Elastic Rerank, you must have the appropriate subscription level or the trial period activated.
* Requires ML nodes 
** Subject to ML trial node limitations

NOTE: BIG WIP TBD about trial deployments
//TODO

[discrete]
[[ml-nlp-rerank-deploy]]
== Download and deploy

To download and deploy Elastic Rerank, use the https://www.elastic.co/guide/en/elasticsearch/reference/current/infer-service-elasticsearch.html[inference service API] to create an Elasticsearch service rerank endpoint.

[discrete]
[[ml-nlp-rerank-deploy-steps]]
=== Create an inference endpoint

. In {kib}, navigate to the *Dev Console*.

. Create an {infer} endpoint with the Elastic Rerank service by running:
+
--
[source,console]
----------------------------------
PUT _inference/text_similarity/my-rerank-model
{
  "service": "elasticsearch",
  "service_settings": {
    "adaptive_allocations": {
      "enabled": true,
      "min_number_of_allocations": 1,
      "max_number_of_allocations": 10
    },
    "num_threads": 1,
    "model_id": ".elastic-rerank"
  }
}
----------------------------------
--

NOTE: The API request automatically downloads and deploys the model. This example uses <<ml-nlp-auto-scale,autoscaling>> through adaptive allocation.

After creating the Elastic Rerank {infer} endpoint, it's ready to use with a {ref}/retriever.html#text-similarity-reranker-retriever-example-elastic-rerank[`text_similarity_reranker`] retriever.

// Is air-gapped deployment supported?
[discrete]
[[ml-nlp-rerank-deploy-airgapped]]
=== Air-gapped deployment

[discrete]
[[ml-nlp-rerank-model-specs]]
== Model specifications

* Purpose-built for English language content

* Relatively small: 184M parameters (86M backbone + 98M embedding layer)

* Matches performance of billion-parameter reranking models

* Built directly into Elasticsearch - no external services or dependencies needed

[discrete]
[[ml-nlp-rerank-arch-overview]]
== Model architecture

Elastic Rerank is built on the https://arxiv.org/abs/2111.09543[DeBERTa v3] language model architecture.

The model employs several key architectural features that make it particularly effective for reranking:

* *Disentangled attention mechanism* enables the model to:
** Process word content and position separately
** Learn more nuanced relationships between query and document text
** Better understand the semantic importance of word positions and relationships

* *ELECTRA-style pre-training* uses:
** A GAN-like approach to token prediction
** Simultaneous training of token generation and detection
** Enhanced parameter efficiency compared to traditional masked language modeling

[discrete]
[[ml-nlp-rerank-arch-training]]
== Training process

Here is an overview of the Elastic Rerank model training process:

* *Initial relevance extraction*
** Fine-tunes the pre-trained DeBERTa [CLS] token representation
** Uses a GeLU activation and dropout layer
** Preserves important pre-trained knowledge while adapting to the reranking task

* *Trained by distillation*
** Uses an ensemble of bi-encoder and cross-encoder models as a teacher
** Bi-encoder provides nuanced negative example assessment
** Cross-encoder helps differentiate between positive and negative examples
** Combines strengths of both model types

[discrete]
[[ml-nlp-rerank-arch-data]]
=== Training data

The training data consists of:

* Open domain Question-Answering datasets
* Natural document pairs (like article headings and summaries)
* 180,000 synthetic query-passage pairs with varying relevance
* Total of approximately 3 million queries

The data preparation process includes:

* Basic cleaning and fuzzy deduplication
* Multi-stage prompting for diverse topics (on the synthetic portion of the training data only)
* Varied query types:
** Keyword search
** Exact phrase matching
** Short and long natural language questions

[discrete]
[[ml-nlp-rerank-arch-sampling]]
=== Negative sampling

The model uses an advanced sampling strategy to ensure high-quality rankings:

* Samples from top 128 documents per query using multiple retrieval methods
* Uses five negative samples per query - more than typical approaches
* Applies probability distribution shaped by document scores for sampling

* Deep sampling benefits:
** Improves model robustness across different retrieval depths
** Enhances score calibration
** Provides better handling of document diversity

[discrete]
[[ml-nlp-rerank-arch-optimization]]
=== Training optimization

The training process incorporates several key optimizations:

Uses cross-entropy loss function to:

* Model relevance as probability distribution
* Learn relationships between all document scores
* Fit scores through maximum likelihood estimation

Implemented parameter averaging along optimization trajectory:

* Eliminates need for traditional learning rate scheduling and provides improvement in the final model quality

[[ml-nlp-rerank-input-prep]]
== Input preparation
// Do we need guidance on preparing texts for reranking?

[[ml-nlp-rerank-testing]]
== Testing Elastic Rerank
// How do we test the model? What tools/UI are available?

[discrete]
[[ml-nlp-rerank-performance]]
== Performance

Elastic Rerank shows significant improvements in search quality across a wide range of retrieval tasks.

[discrete]
[[ml-nlp-rerank-performance-overview]]
=== Overview

* Average 40% improvement in ranking quality when reranking BM25 results
* 184M parameter model matches performance of 2B parameter alternatives
* Evaluated across 21 different datasets using the BEIR benchmark suite

[discrete]
[[ml-nlp-rerank-performance-benchmarks]]
=== Key benchmark results

* Natural Questions: 90% improvement
* MS MARCO: 85% improvement
* Climate-FEVER: 80% improvement
* FiQA-2018: 76% improvement

For detailed benchmark information, including complete dataset results and methodology, refer to the https://www.elastic.co/search-labs/introducing-elastic-rerank[Introducing Elastic Rerank blog].

[discrete]
[[ml-nlp-rerank-perf-considerations]]
=== Performance considerations
// What hardware-specific performance characteristics should users know about?

[discrete]
[[ml-nlp-rerank-benchmarks-hw]]
=== Hardware benchmarks
// Are there hardware-specific benchmark numbers we should include?

[discrete]
[[ml-nlp-rerank-limitations]]
== Limitations
// What are current known limitations beyond tech preview status?

[discrete]
[[ml-nlp-rerank-resources]]
== Further resources
// What additional resources should we link to?

