[[ml-dfa-finding-outliers]]
= Finding outliers

:frontmatter-description: An introduction to {ml} {oldetection}, which enables you to find unusual data points in a data set compared to the normal data points.
:frontmatter-tags-products: [ml] 
:frontmatter-tags-content-type: [how-to] 
:frontmatter-tags-user-goals: [analyze]

{oldetection-cap} is identification of data points that are significantly 
different from other values in the data set. For example, outliers could be 
errors or unusual entities in a data set. {oldetection-cap} is an unsupervised 
{ml} technique, there is no need to provide training data.

IMPORTANT: {oldetection-cap} is a batch analysis, it runs against your data 
once. If new data comes into the index, you need to do the analysis again on the 
altered data.


[discrete]
[[dfa-outlier-algorithms]]
== {oldetection-cap} algorithms

//tag::outlier-detection-algorithms[]
In the {stack}, we use an ensemble of four different distance and density based 
{oldetection} methods:

* distance of K^th^ nearest neighbor: computes the distance of the data point to 
its K^th^ nearest neighbor where K is a small number and usually independent of 
the total number of data points.
* distance of K-nearest neighbors: calculates the average distance of the data 
points to their nearest neighbors. Points with the largest average distance will 
be the most outlying.
* local outlier factor (`lof`): takes into account the distance of the points to 
their K nearest neighbors and also the distance of these neighbors to their 
neighbors.
* local distance-based outlier factor (`ldof`): is a ratio of two measures: the 
first computes the average distance of the data point to its K nearest 
neighbors; the second computes the average of the pairwise distances of the 
neighbors themselves.
//end::outlier-detection-algorithms[]

You don't need to select the methods or provide any parameters, but you can 
override the default behavior if you like. _Distance based methods_ assume 
that normal data points remain closer or similar in value while outliers are 
located far away or significantly differ in value. The drawback of these methods 
is that they don't take into account the density variations of a data set. 
_Density based methods_ are used for mitigating this problem.

The four algorithms don't always agree on which points are outliers. By default, 
{oldetection} jobs use all these methods, then normalize and combine their 
results and give every data point in the index an {olscore}. The {olscore} ranges 
from 0 to 1, where the higher number represents the chance that the data point 
is an outlier compared to the other data points in the index.


[discrete]
[[dfa-feature-influence]]
=== Feature influence

Feature influence – another score calculated while detecting outliers – provides 
a relative ranking of the different features and their contribution towards a 
point being an outlier. This score allows you to understand the context or 
the reasoning on why a certain data point is an outlier.


[discrete]
[[dfa-outlier-detection-problem]]
== 1. Define the problem

{oldetection-cap} in the {stack} can be used to detect any unusual entity in a 
given population. For example, to detect malicious software on a machine or
unusual user behavior on a network. As {oldetection} operates on the assumption 
that the outliers make up a small proportion of the overall data population, you 
can use this feature in such cases. {oldetection-cap} is a batch analysis that 
works best on an entity-centric index. If your use case is based on time series 
data, you might want to use <<ml-ad-overview,{anomaly-detect}>> 
instead.

The {ml-features} provide unsupervised {oldetection}, which means there is no 
need to provide a training data set.


[discrete]
[[dfa-outlier-detection-environment]]
== 2. Set up the environment

Before you can use the {stack-ml-features}, there are some configuration
requirements (such as security privileges) that must be addressed. Refer to
<<setup>>.


[discrete]
[[dfa-outlier-detection-prepare-data]]
== 3. Prepare and transform data

{oldetection-cap} requires specifically structured source data: a two 
dimensional tabular data structure. For this reason, you might need to 
{ref}/transforms.html[{transform}] your data to create a {dataframe} which can 
be used as the source for {oldetection}.

You can find an example of how to transform your data into an entity-centric 
index in <<weblogs-outliers, this section>>.


[discrete]
[[dfa-outlier-detection-create-job]]
== 4. Create a job

{dfanalytics-jobs-cap} contain the configuration information and metadata 
necessary to perform an analytics task. You can create {dfanalytics-jobs} via 
{kib} or using the {ref}/put-dfanalytics.html[create {dfanalytics-jobs} API]. 
Select {oldetection} as the analytics type that the {dfanalytics-job} performs. 
You can also decide to include and exclude fields to/from the analysis when you 
create the job.


[discrete]
[[dfa-outlier-detection-start]]
== 5. Start the job

You can start the job via {kib} or using the 
{ref}/start-dfanalytics.html[start {dfanalytics-jobs}] API. An {oldetection} job 
has four phases: 

* `reindexing`: documents are copied from the source index to the destination 
  index.
* `loading_data`: the job fetches the necessary data from the destination index.
* `computing_outliers`: the job identifies outliers in the data.
* `writing_results`: the job matches the results with the data rows in the 
  destination index, merges them, and indexes them back to the destination 
  index.

After the last phase is finished, the job stops and the results are ready for 
evaluation.

{oldetection-cap} jobs – unlike other {dfanalytics-jobs} – run one time in their 
life cycle. If you'd like to run the analysis again, you need to create a new 
job.


[discrete]
[[ml-outlier-detection-evaluate]]
== 6. Evaluate the results

include::ml-dfa-shared.asciidoc[tag=dfa-evaluation-intro]

The {oldetection} evaluation type offers the following metrics to evaluate the 
model performance:

* confusion matrix
* precision
* recall
* receiver operating characteristic (ROC) curve.

[discrete]
[[ml-dfanalytics-confusion-matrix]]
=== Confusion matrix

A confusion matrix provides four measures of how well the {dfanalytics} worked 
on your data set:

* True positives (TP): Class members that the analysis identified as class 
members.
* True negatives (TN): Not class members that the analysis identified as not 
class members.
* False positives (FP): Not class members that the analysis misidentified as 
class members.
* False negatives (FN): Class members that the analysis misidentified as not 
class members.

Although, the {evaluatedf-api} can compute the confusion matrix out of the 
analysis results, these results are not binary values (class member/not 
class member), but a number between 0 and 1 (which called the {olscore} in case 
of {oldetection}). This value captures how likely it is for a data point to be a 
member of a certain class. It means that it is up to the user to decide what is 
the threshold or cutoff point at which the data point will be considered as a 
member of the given class. For example, the user can say that all the data 
points with an {olscore} higher than 0.5 will be considered as outliers.

To take this complexity into account, the {evaluatedf-api} returns the confusion 
matrix at different thresholds (by default, 0.25, 0.5, and 0.75).

[discrete]
[[ml-dfanalytics-precision-recall]]
=== Precision and recall

Precision and recall values summarize the algorithm performance as a single 
number that makes it easier to compare the evaluation results.

Precision shows how many of the data points that were identified as class 
members are actually class members. It is the number of true positives divided 
by the sum of the true positives and false positives (TP/(TP+FP)).

Recall shows how many of the data points that are actual class members were 
identified correctly as class members. It is the number of true positives 
divided by the sum of the true positives and false negatives (TP/(TP+FN)).

Precision and recall are computed at different threshold levels.


[discrete]
[[ml-dfanalytics-roc]]
=== Receiver operating characteristic curve

The receiver operating characteristic (ROC) curve is a plot that represents the 
performance of the binary classification process at different thresholds. It 
compares the rate of true positives against the rate of false positives at the 
different threshold levels to create the curve. From this plot, you can compute 
the area under the curve (AUC) value, which is a number between 0 and 1. The 
closer to 1, the better the algorithm performance.

The {evaluatedf-api} can return the false positive rate (`fpr`) and the true 
positive rate (`tpr`) at the different threshold levels, so you can visualize 
the algorithm performance by using these values.


[discrete]
[[weblogs-outliers]]
== Detecting unusual behavior in the logs data set

The goal of {oldetection} is to find the most unusual documents in an index. 
Let's try to detect unusual behavior in the 
{kibana-ref}/get-started.html#gs-get-data-into-kibana[data logs sample data set]. 

. Verify that your environment is set up properly to use {ml-features}. If the 
{es} {security-features} are enabled, you need a user that has authority to 
create and manage {dfanalytics-jobs}. See <<setup>>.
+
--
Since we'll be creating {transforms}, you also need 
`manage_data_frame_transforms` cluster privileges.
--

. Create a {transform} that generates an entity-centric index with numeric or
boolean data to analyze.
+
--
In this example, we'll use the web logs sample data and pivot the data such 
that we get a new index that contains a network usage summary for each client 
IP.

In particular, create a {transform} that calculates the number of occasions when 
a specific client IP communicated with the network (`@timestamp.value_count`), 
the sum of the bytes that are exchanged between the network and the client's 
machine (`bytes.sum`), the maximum exchanged bytes during a single occasion 
(`bytes.max`), and the total number of requests (`request.value_count`) 
initiated by a specific client IP.

You can preview the {transform} before you create it in *{stack-manage-app}*
> *Transforms*:

[role="screenshot"]
image::images/logs-transform-preview.jpg["Creating a {transform} in {kib}"]

Alternatively, you can use the
{ref}/preview-transform.html[preview {transform} API] and the
{ref}/put-transform.html[create {transform} API].

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _transform/_preview
{
  "source": {
    "index": [
      "kibana_sample_data_logs"
    ]
  },
  "pivot": {
    "group_by": {
      "clientip": {
        "terms": {
          "field": "clientip"
        }
      }
    },
    "aggregations": {
      "@timestamp.value_count": {
        "value_count": {
          "field": "@timestamp"
        }
      },
      "bytes.max": {
        "max": {
          "field": "bytes"
        }
      },
      "bytes.sum": {
        "sum": {
          "field": "bytes"
        }
      },
      "request.value_count": {
        "value_count": {
          "field": "request.keyword"
        }
      }
    }
  }
}

PUT _transform/logs-by-clientip
{
  "source": {
    "index": [
      "kibana_sample_data_logs"
    ]
  },
  "pivot": {
    "group_by": {
      "clientip": {
        "terms": {
          "field": "clientip"
        }
      }
    },
    "aggregations": {
      "@timestamp.value_count": {
        "value_count": {
          "field": "@timestamp"
        }
      },
      "bytes.max": {
        "max": {
          "field": "bytes"
        }
      },
      "bytes.sum": {
        "sum": {
          "field": "bytes"
        }
      },
      "request.value_count": {
        "value_count": {
          "field": "request.keyword"
        }
      }
    }
  },
  "description": "Web logs by client IP",
  "dest": {
    "index": "weblog-clientip"
  }
}
--------------------------------------------------
// TEST[skip:set up sample data]
====

For more details about creating {transforms}, see
{ref}/ecommerce-transforms.html[Transforming the eCommerce sample data].
--

. Start the {transform}.
+
--

TIP: Even though resource utilization is automatically adjusted based on the
cluster load, a {transform} increases search and indexing load on your
cluster while it runs. If you're experiencing an excessive load, however, you
can stop it.

You can start, stop, and manage {transforms} in {kib}. Alternatively, you can
use the {ref}/start-data-frame-transform.html[start {transforms}] API.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _transform/logs-by-clientip/_start
--------------------------------------------------
// TEST[skip:setup kibana sample data]
====
--

. Create a {dfanalytics-job} to detect outliers in the new entity-centric index.
+
--
In the wizard on the *Machine Learning* > *Data Frame Analytics* page in {kib},
select your new {data-source} then use the default values for {oldetection}. For
example:

[role="screenshot"]
image::images/weblog-outlier-job-1.jpg["Create a {dfanalytics-job} in {kib}"]

The wizard includes a scatterplot matrix, which enables you to explore the 
relationships between the fields. You can use that information to help you
decide which fields to include or exclude from the analysis.

[role="screenshot"]
image::images/weblog-outlier-scatterplot.jpg["A scatterplot matrix for three fields in {kib}"]

If you want these charts to represent data from a larger sample size or from a
randomized selection of documents, you can change the default behavior. However, 
a larger sample size might slow down the performance of the matrix and a
randomized selection might put more load on the cluster due to the more
intensive query.

Alternatively, you can use the
{ref}/put-dfanalytics.html[create {dfanalytics-jobs} API].

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
PUT _ml/data_frame/analytics/weblog-outliers
{
  "source": {
    "index": "weblog-clientip"
  },
  "dest": {
    "index": "weblog-outliers"
  },
  "analysis": {
    "outlier_detection": {
    }
  },
  "analyzed_fields" : {
    "includes" : ["@timestamp.value_count","bytes.max","bytes.sum","request.value_count"]
  }
}
--------------------------------------------------
// TEST[skip:setup kibana sample data]
====

After you configured your job, the configuration details are automatically 
validated. If the checks are successful, you can proceed and start the job. A 
warning message is shown if the configuration is invalid. The message contains a 
suggestion to improve the configuration to be validated.
--

. Start the {dfanalytics-job}.
+
--
You can start, stop, and manage {dfanalytics-jobs} on the *Machine Learning* > 
*Data Frame Analytics* page. Alternatively, you can use the
{ref}/start-dfanalytics.html[start {dfanalytics-jobs}] and
{ref}/stop-dfanalytics.html[stop {dfanalytics-jobs}] APIs.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _ml/data_frame/analytics/weblog-outliers/_start
--------------------------------------------------
// TEST[skip:setup kibana sample data]
====
--

. View the results of the {oldetection} analysis.
+
--
The {dfanalytics-job} creates an index that contains the original data and
{olscores} for each document. The {olscore} indicates how different each entity
is from other entities.

In {kib}, you can view the results from the {dfanalytics-job} and sort them
on the outlier score:

[role="screenshot"]
image::images/outliers.jpg["View {oldetection} results in {kib}"]

The `ml.outlier` score is a value between 0 and 1. The larger the value, the
more likely they are to be an outlier. In {kib}, you can optionally enable
histogram charts to get a better understanding of the distribution of values for
each column in the result.

In addition to an overall outlier score, each document is annotated with feature
influence values for each field. These values add up to 1 and indicate which
fields are the most important in deciding whether an entity is an outlier or
inlier. For example, the dark shading on the `bytes.sum` field for the client IP 
`111.237.144.54` indicates that the sum of the exchanged bytes was the most
influential feature in determining that that client IP is an outlier.

If you want to see the exact feature influence values, you can retrieve them
from the index that is associated with your {dfanalytics-job}.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
GET weblog-outliers/_search?q="111.237.144.54"
--------------------------------------------------
// TEST[skip:setup kibana sample data]

The search results include the following {oldetection} scores:

[source,js]
--------------------------------------------------
...
  "ml" : {
    "outlier_score" : 0.9830020666122437,
    "feature_influence" : [
      {
        "feature_name" : "@timestamp.value_count",
        "influence" : 0.005870792083442211
      },
      {
        "feature_name" : "bytes.max",
        "influence" : 0.12034820765256882
      },
      {
        "feature_name" : "bytes.sum",
        "influence" : 0.8679102063179016
      },
     {
        "feature_name" : "request.value_count",
        "influence" : 0.005870792083442211
      } 
    ]
  }
...
--------------------------------------------------
// NOTCONSOLE
====

{kib} also provides a scatterplot matrix in the results. Outliers with a score 
that exceeds the threshold are highlighted in each chart:

[role="screenshot"]
image::images/outliers-scatterplot.jpg["View scatterplot in {oldetection} results"]

In addition to the sample size and random scoring options, there is a
*Dynamic size* option. If you enable this option, the size of each point is 
affected by its {olscore}; that is to say, the largest points have the
highest {olscores}. The goal of these charts and options is to help you 
visualize and explore the outliers within your data.

--

Now that you've found unusual behavior in the sample data set, consider how you
might apply these steps to other data sets. If you have data that is already
marked up with true outliers, you can determine how well the {oldetection}
algorithms perform by using the evaluate {dfanalytics} API. See
<<ml-outlier-detection-evaluate>>.

TIP: If you do not want to keep the {transform} and the {dfanalytics-job}, you
can delete them in {kib} or use the
{ref}/delete-data-frame-transform.html[delete {transform} API] and
{ref}/delete-dfanalytics.html[delete {dfanalytics-job} API]. When you delete
{transforms} and {dfanalytics-jobs} in {kib}, you have the option to also remove
the destination indices and {data-sources}.


[discrete]
[[outlier-detection-reading]]
== Further reading

* If you want to see another example of {oldetection} in a Jupyter notebook,
https://github.com/elastic/examples/tree/master/Machine%20Learning/Outlier%20Detection/Introduction[click here].

* https://www.elastic.co/blog/catching-malware-with-elastic-outlier-detection[This blog post]
shows you how to catch malware using {oldetection}.

* https://www.elastic.co/blog/benchmarking-outlier-detection-in-elastic-machine-learning[Benchmarking {oldetection} results in Elastic {ml}]
