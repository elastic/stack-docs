[role="xpack"]
[[ml-gs-results]]
=== View {anomaly-detect} results

After the {dfeeds} are started and the {anomaly-jobs} have processed some data,
you can view the results in {kib}.

TIP: Depending on the capacity of your machine, you might need to wait a few
seconds for the {ml} analysis to generate initial results.

[role="screenshot"]
image::images/ml-gs-web-results.jpg["Create jobs for the sample web logs"]

The {ml-features} analyze the input stream of data, model its behavior, and
perform analysis based on the detectors in each job. When an event occurs
outside of the model, that event is identified as an anomaly. You can
immediately see that all three jobs have found anomalies, which are indicated by
red blocks in the swim lanes for each job.

There are two tools for examining the results from {anomaly-jobs} in {kib}: the
**Anomaly Explorer** and the **Single Metric Viewer**.

[discrete]
[[ml-gs-results-smv]]
=== Single metric job results

One of the sample jobs (`low_request_rate`), is a _single metric {anomaly-job}_.
It has a single detector that uses the `low_count` function and limited job
properties. You might use a job like this if you want to determine when the
request rate on your web site drops significantly. 

Let's start by looking at this simple job in the
**Single Metric Viewer**:

[role="screenshot"]
image::images/ml-gs-job1-analysis.jpg["Single Metric Viewer for low_request_rate job"]

This view contains a chart that represents the actual and expected values over
time. It is available only if the job has `model_plot_config` enabled. It can
display only a single time series.

The blue line in the chart represents the actual data values. The shaded blue
area represents the bounds for the expected values. The area between the upper
and lower bounds are the most likely values for the model. If a value is outside
of this area then it can be said to be anomalous.

If you slide the time selector from the beginning to the end of the data, you
can see how the model improves as it processes more data. At the beginning, the
expected range of values is pretty broad and the model is not capturing the
periodicity in the data. But it quickly learns and begins to reflect the
patterns in your data.

.Anomaly scores
****
Any data points outside the range that was predicted by the model are marked
as anomalies. In order to provide a sensible view of the results, an
_anomaly score_ is calculated for each bucket time interval. The anomaly score
is a value from 0 to 100, which indicates the significance of the anomaly
compared to previously seen anomalies. The highly anomalous values are shown in
red and the low scored values are indicated in blue. An interval with a high
anomaly score is significant and requires investigation.
****

Slide the time selector to a section of the time series that contains a red
anomaly data point. If you hover over the point, you can see more information.

NOTE: You might notice a high spike in the time series. It's not highlighted as 
an anomaly, however, since this job looks for low counts only. 

For each anomaly, you can see key details such as the time, the actual and
expected ("typical") values, and their probability in the **Anomalies** section
of the viewer. For example:

[role="screenshot"]
image::images/ml-gs-job1-anomalies.jpg["Single Metric Viewer Anomalies for low_request_rate job"]

By default, the table contains all anomalies that have a severity of "warning"
or higher in the selected section of the timeline. If you are only interested in
critical anomalies, for example, you can change the severity threshold for this
table.

After you have identified anomalies, often the next step is to try to determine
the context of those situations. For example, are there other factors that are
contributing to the problem? Are the anomalies confined to particular
applications or servers? You can begin to troubleshoot these situations by
layering additional jobs or creating multi-metric jobs.

[discrete]
[[ml-gs-results-ae]]
=== Advanced or multi-metric job results

Conceptually, you can think of _multi-metric {anomaly-jobs}_ as running multiple
independent single metric jobs. By bundling them together in a multi-metric job,
however, you can see an overall score and shared influencers for all the metrics
and all the entities in the job. Multi-metric jobs therefore scale better than
having many independent single metric jobs. They also provide better results
when you have influencers that are shared across the detectors.

.Influencers
****
When you create an {anomaly-job}, you can identify fields as _influencers_.
These are fields that you think contain information about someone or something
that influences or contributes to anomalies. There are influencers in both the
`response_code_rates` and `url_scanning` jobs.

As a best practice, do not pick too many influencers. For example, you generally
do not need more than three. If you pick many influencers, the results can be
overwhelming and there is a small overhead to the analysis. For more details,
see <<ml-influencers>>.

****

You can also configure your {anomaly-jobs} to split a single time series into
multiple time series based on a categorical field. For example, the
`response_code_rates` job has a single detector that splits the data based on
the `response.keyword` and then uses the `count` function to determine when the
number of events is anomalous. You might use a job like this if you want to
look at both high and low request rates partitioned by response code. When you
view the results in the **Anomaly Explorer**, you can see separate swim lanes
for the `404` and `200` responses:

[role="screenshot"]
image::images/ml-gs-job2-explorer.jpg["Anomaly explorer for response_code_rates job"]

In this scenario, the analysis is segmented such that you have completely
different baselines for each distinct value of the _partition field_. By looking
at temporal patterns on a per entity basis, you might spot things that might
have otherwise been hidden in the lumped view.

On the left side of the **Anomaly Explorer**, there is a list of the top
influencers for all of the detected anomalies in that same time period. The list
includes maximum anomaly scores, which in this case are aggregated for each
influencer, for each bucket, across all detectors. There is also a total sum of
the anomaly scores for each influencer. You can use this list to help you narrow
down the contributing factors and focus on the most anomalous entities. 

Click on a section in the swim lanes to obtain more information about the
anomalies in that time period. For example, click on the red section in the
swim lane for the `response.keyword` value of `404`:

[role="screenshot"]
image::images/ml-gs-job2-explorer-anomaly.jpg["Anomaly charts for the response_code_rates job"]

You can see exact times when anomalies occurred. If there are multiple detectors
or metrics in the job, you can see which caught the anomaly. You can also switch
to viewing this time series in the **Single Metric Viewer**.

Below the charts, there is a table that provides more information, such as the
typical and actual values and the influencers that contributed to the anomaly.
For example:

[role="screenshot"]
image::images/ml-gs-job2-explorer-table.jpg["Anomaly tables for the response_code_rates job"]

If your job has multiple detectors, the table aggregates the anomalies to show
the highest severity anomaly per detector and entity, which is the field value
that is displayed in the **found for** column. To view all the anomalies without
any aggregation, set the **Interval** to `Show all`. 

In this sample data, the spike in the 404 response codes is influenced by a
specific client. Situations like this might indicate that the client is
accessing unusual pages or scanning your site to see if they can access
unusual URLs. This anomalous behavior merits further investigation.

TIP: The anomaly scores that you see in each section of the **Anomaly Explorer**
might differ slightly. This disparity occurs because for each job there are
bucket results, influencer results, and record results. Anomaly scores are
generated for each type of result. The anomaly timeline uses the bucket-level
anomaly scores. The list of top influencers uses the influencer-level anomaly
scores. The list of anomalies uses the record-level anomaly scores.

[discrete]
[[ml-gs-results-population]]
=== Population job results

The final sample job (`url_scanning`) is a _population {anomaly-job}_. As we
saw in the `response_code_rates` job results, there are some clients that seem
to be accessing unusually high numbers of URLs. The `url_scanning` sample job
provides another method for investigating that type of problem. It has a
single detector that uses the `high_distinct_count` function on the `url.keyword`
to detect unusually high numbers of distinct values in that field. It then
analyzes whether that behavior differs over the population of clients, as
defined by the `clientip` field.  

If you examine the results from the `url_scanning` {anomaly-job} in the
**Anomaly Explorer**, you'll notice its charts have a different format. For
example:

[role="screenshot"]
image::images/ml-gs-job3-explorer.jpg["Anomaly charts for the url_scanning job"]

In this case, the metrics for each client IP are analyzed relative to other
client IPs in each bucket and we can once again see that the
`30.156.16.164` client IP is behaving abnormally.

If you want to play with another example of a population {anomaly-job}, add the
sample eCommerce orders data set. Its `high_sum_total_sales` job determines
which customers have made unusual amounts of purchases relative to other
customers in each bucket of time. In this example, there is a low severity
anomalous event found for `Abd Burton`:

[role="screenshot"]
image::images/ml-gs-job4-explorer.jpg["Anomaly charts for the high_sum_total_sales job"]

For more information, see <<ml-configuring-pop>>.