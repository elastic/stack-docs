[[upgrading-elasticsearch]]
== Upgrade {es}

To upgrade from 7.16 or earlier to {version}, **you must first upgrade to {prev-major-last}**, 
even if you opt to do a full-cluster restart instead of a rolling upgrade. 
This enables you to use the **Upgrade Assistant** to <<prepare-to-upgrade, prepare to upgrade>>.

.FIPS Compliance and Java 17
[IMPORTANT]
--
{es} 8.0 requires Java 17 or later.  
There is not yet a FIPS-certified security module for Java 17 
that you can use when running {es} 8.0 in FIPS 140-2 mode.
If you run in FIPS 140-2 mode, you will either need to request
an exception from your security organization to upgrade to {es} 8.0, 
or remain on {es} 7.x until Java 17 is certified. 
ifeval::["{release-state}"=="released"]
Alternatively, consider using {ess} in the FedRAMP-certified GovCloud region.
endif::[]
--

An {es} cluster can be upgraded one node at
a time so upgrading does not interrupt service. Running multiple versions of
{es} in the same cluster beyond the duration of an upgrade is
not supported, as shards cannot be replicated from upgraded nodes to nodes
running the older version.

IMPORTANT: Upgrading from a pre-release to 8.0 GA is not supported.
Pre-releases should only be used for testing in a temporary environment.

When performing a <<rolling-upgrades, rolling upgrade>>:

. Upgrade nodes that are **NOT** {ref}/modules-node.html#master-node[master-eligible] first. 
You can retrieve a list of these nodes with `GET /_nodes/_all,master:false/_none` or by finding all the nodes configured with `node.master: false`.

. Upgrade nodes tier-by-tier, starting with the frozen tier.
Complete the upgrade for all nodes in each data tier before moving to the next. 
Upgrade the frozen tier, then the cold tier, then the warm tier, and upgrade the hot tier last. This ensures {ilm-init} can continue to move data through the tiers during the upgrade. You can get the list of nodes in a specific tier with a `GET /_nodes` request, 
for example:  `GET /_nodes/data_frozen:true/_none`.

. Upgrade the master-eligible nodes last. You can retrieve a list
of these nodes with `GET /_nodes/master:true`.

This order ensures that all nodes can join the cluster during the upgrade.
Upgraded nodes can join a cluster with an older master, but older nodes cannot
always join a cluster with a upgraded master. 

To upgrade a cluster:

. *Disable shard allocation*.
+
--
When you shut down a data node, the allocation process waits for
`index.unassigned.node_left.delayed_timeout` (by default, one minute) before
starting to replicate the shards on that node to other nodes in the cluster,
which can involve a lot of I/O. Since the node is shortly going to be
restarted, this I/O is unnecessary. You can avoid racing the clock by
{ref}/modules-cluster.html#cluster-routing-allocation-enable[disabling allocation] of replicas before
shutting down {ref}/modules-node.html#data-node[data nodes]:

[source,console]
--------------------------------------------------
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "primaries"
  }
}
--------------------------------------------------
--

. *Stop non-essential indexing and perform a flush.* (Optional)
+
--
While you can continue indexing during the upgrade, shard recovery
is much faster if you temporarily stop non-essential indexing and perform a
{ref}/indices-flush.html[flush].

[source,console]
--------------------------------------------------
POST /_flush
--------------------------------------------------
--

. *Temporarily stop the tasks associated with active {ml} jobs and {dfeeds}.* (Optional)
+
--
It is possible to leave your {ml} jobs running during the upgrade, 
but it puts increased load on the cluster. When you shut down a
{ml} node, its jobs automatically move to another node and restore the model
states. 

NOTE: Any {ml} indices created before {prev-major-version} must be reindexed
before upgrading, which you can initiate from the **Upgrade Assistant** in {prev-major-last}.

* Temporarily halt the tasks associated with your {ml} jobs and {dfeeds} and
prevent new jobs from opening by using the
{ref}/ml-set-upgrade-mode.html[set upgrade mode API]:
+
[source,console]
--------------------------------------------------
POST _ml/set_upgrade_mode?enabled=true
--------------------------------------------------
+
When you disable upgrade mode, the jobs resume using the last model
state that was automatically saved. This option avoids the overhead of managing
active jobs during the upgrade and is faster than explicitly stopping {dfeeds}
and closing jobs.

* {ml-docs}/ml-ad-run-jobs.html#ml-ad-close-job[Stop all {dfeeds} and close all jobs]. This option
saves the model state at the time of closure. When you reopen the jobs after the
upgrade, they use the exact same model. However, saving the latest model state
takes longer than using upgrade mode, especially if you have a lot of jobs or
jobs with large model states.
--

. [[upgrade-node]] *Shut down a single node*.
+
--
* If you are running {es} with `systemd`:
+
[source,sh]
--------------------------------------------------
sudo systemctl stop elasticsearch.service
--------------------------------------------------

* If you are running {es} with SysV `init`:
+
[source,sh]
--------------------------------------------------
sudo -i service elasticsearch stop
--------------------------------------------------

* If you are running {es} as a daemon:
+
[source,sh]
--------------------------------------------------
kill $(cat pid)
--------------------------------------------------
--

. *Upgrade the node you shut down.*
+
--
To upgrade using a {ref}/deb.html[Debian] or {ref}/rpm.html[RPM] package:

*   Use `rpm` or `dpkg` to install the new package. All files are
    installed in the appropriate location for the operating system
    and {es} config files are not overwritten.


To upgrade using a zip or compressed tarball:

.. Extract the zip or tarball to a **new** directory. This is critical if you
   are not using external `config` and `data` directories.

.. Set the `ES_PATH_CONF` environment variable to specify the location of
   your external `config` directory and `jvm.options` file. If you are not
   using an external `config` directory, copy your old configuration
   over to the new installation.

.. Set `path.data` in `config/elasticsearch.yml` to point to your external
   data directory. If you are not using an external `data` directory, copy
   your old data directory over to the new installation. +
+
IMPORTANT: If you use {monitor-features}, re-use the data directory when you upgrade
{es}. Monitoring identifies unique {es} nodes by using the persistent UUID, which
is stored in the data directory.


.. Set `path.logs` in `config/elasticsearch.yml` to point to the location
   where you want to store your logs. If you do not specify this setting,
   logs are stored in the directory you extracted the archive to.

[TIP]
================================================

When you extract the zip or tarball packages, the `elasticsearch-{bare_version}`
directory contains the {es} `config`, `data`, and `logs` directories.

We recommend moving these directories out of the {es} directory
so that there is no chance of deleting them when you upgrade {es}.
To specify the new locations, use the `ES_PATH_CONF` environment
variable and the `path.data` and `path.logs` settings. For more information,
see {ref}/important-settings.html[Important {es} configuration].

The Debian and RPM packages place these directories in the
appropriate place for each operating system. In production, we recommend
using the deb or rpm package.

================================================

[[rolling-upgrades-bootstrapping]]
Leave `cluster.initial_master_nodes` unset when performing a
rolling upgrade. Each upgraded node is joining an existing cluster so there is
no need for {ref}/modules-discovery-bootstrap-cluster.html[cluster bootstrapping]. 
You must configure {ref}/important-settings.html#discovery-settings[either `discovery.seed_hosts` or
`discovery.seed_providers`] on every node.
--

. *Upgrade any plugins.*
+
Use the `elasticsearch-plugin` script to install the upgraded version of each
installed {es} plugin. All plugins must be upgraded when you upgrade
a node.

. *Start the upgraded node.*
+
--

Start the newly-upgraded node and confirm that it joins the cluster by checking
the log file or by submitting a `_cat/nodes` request:

[source,console]
--------------------------------------------------
GET _cat/nodes
--------------------------------------------------
--

. *Reenable shard allocation.*
+
--

For data nodes, once the node has joined the cluster, remove the
`cluster.routing.allocation.enable` setting to enable shard allocation and start
using the node:

[source,console]
--------------------------------------------------
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": null
  }
}
--------------------------------------------------
--

. *Wait for the node to recover.*
+
--

Before upgrading the next node, wait for the cluster to finish shard allocation.
You can check progress by submitting a `_cat/health` request:

[source,console]
--------------------------------------------------
GET _cat/health?v=true
--------------------------------------------------

Wait for the `status` column to switch to `green`. Once the node is `green`, all
primary and replica shards have been allocated.

[IMPORTANT]
====================================================
During a rolling upgrade, primary shards assigned to a node running the new
version cannot have their replicas assigned to a node with the old
version. The new version might have a different data format that is
not understood by the old version.

If it is not possible to assign the replica shards to another node
(there is only one upgraded node in the cluster), the replica
shards remain unassigned and status stays `yellow`.

In this case, you can proceed once there are no initializing or relocating shards
(check the `init` and `relo` columns).

As soon as another node is upgraded, the replicas can be assigned and the
status will change to `green`.
====================================================

Shards that were not flushed might take longer to
recover. You can monitor the recovery status of individual shards by
submitting a `_cat/recovery` request:

[source,console]
--------------------------------------------------
GET _cat/recovery
--------------------------------------------------

If you stopped indexing, it is safe to resume indexing as soon as
recovery completes.
--

. *Repeat*.
+
--

When the node has recovered and the cluster is stable, repeat these steps
for each node that needs to be updated. You can monitor the health of the cluster
with a `_cat/health` request:

[source,console]
--------------------------------------------------
GET /_cat/health?v=true
--------------------------------------------------

And check which nodes have been upgraded with a `_cat/nodes` request:

[source,console]
--------------------------------------------------
GET /_cat/nodes?h=ip,name,version&v=true
--------------------------------------------------

--

. *Restart machine learning jobs.*
+
--
If you temporarily halted the tasks associated with your {ml} jobs,
use the set upgrade mode API to return them to active
states:

[source,console]
--------------------------------------------------
POST _ml/set_upgrade_mode?enabled=false
--------------------------------------------------

If you closed all {ml} jobs before the upgrade, open the jobs and start the
datafeeds from {kib} or with the {ref}/ml-open-job.html[open jobs] and
{ref}/ml-start-datafeed.html[start datafeed] APIs.
--

[discrete]
[[rolling-upgrades]]
=== Rolling upgrades

During a rolling upgrade, the cluster continues to operate normally. However,
any new functionality is disabled or operates in a backward compatible mode
until all nodes in the cluster are upgraded. New functionality becomes
operational once the upgrade is complete and all nodes are running the new
version. Once that has happened, there's no way to return to operating in a
backward compatible mode. Nodes running the previous version will not be
allowed to join the fully-updated cluster.

In the unlikely case of a network malfunction during the upgrade process that
isolates all remaining old nodes from the cluster, you must take the old nodes
offline and upgrade them to enable them to join the cluster.

If you stop half or more of the master-eligible nodes all at once during the
upgrade the cluster will become unavailable. You must upgrade and restart
all of the stopped master-eligible nodes to allow the cluster to re-form. 
It might also be necessary to upgrade all other nodes running the old version
to enable them to join the re-formed cluster.

Similarly, if you run a testing/development environment with a single master
node it should be upgraded last. Restarting a single master node
forces the cluster to be reformed. The new cluster will initially only have the
upgraded master node and will thus reject the older nodes when they re-join the
cluster. Nodes that have already been upgraded will successfully re-join the
upgraded master.


[discrete]
[[archived-settings]]
=== Archived settings

If you upgrade an {es} cluster that uses deprecated cluster or index settings
that are not used in the target version, they are archived. We
recommend you remove any archived settings after upgrading.
For more information, see 
{ref}/archived-settings.html[Archived settings]. 
