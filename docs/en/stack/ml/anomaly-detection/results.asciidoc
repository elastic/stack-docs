[role="xpack"]
[[ml-results]]
== Interpret {anomaly-detect} results

Each {anomaly-job} generates {ml} results for <<ml-results-buckets,buckets>>,
<<ml-results-influencers,influencers>>, <<ml-results-records,records>> and
<<ml-results-categories,categories>>. You can also query summarized bucket
results over multiple jobs; those results are called _overall buckets_.

There are {ml} results for each `bucket_span`. The timestamp for the results is
the start of the bucket time interval.

The results include _anomaly scores_, which are calculated for each anomaly
result type and each bucket interval. These scores are aggregated in order to
reduce noise, and normalized in order to identify and rank the most
mathematically significant anomalies.

[discrete]
[[ml-results-buckets]]
=== Bucket results

Bucket results provide the top level, overall view of the job and are ideal for
alerts. For example, the bucket results might indicate that at 16:05 the system
was unusual. This information is a summary of all the anomalies, pinpointing
when they occurred.

Each bucket has an anomaly score, which is a statistically aggregated and
normalized view of the combined anomalousness of all the record results within
the bucket.

One bucket result is written for each bucket span for each job, even if it is
not considered to be anomalous. If the bucket is not anomalous, it has an
anomaly score of zero.

When you identify an anomalous bucket, you can investigate further by examining
the pertinent records.

[discrete]
[[ml-results-influencers]]
=== Influencer results

Influencer results show which entities were anomalous and when. For example,
the influencer results might indicate that at 16:05 `user_name: Bob` was unusual.
This information is a summary of all the anomalies for each entity, so there
can be a lot of these results. Once you have identified a notable bucket time,
you can look to see which entities were significant.


[discrete]
[[ml-results-records]]
=== Record results

Record results provide details about what the individual anomaly was, when it
occurred and which entity was involved. For example, the record results might
indicate that at 16:05 Bob sent 837262434 bytes, when the typical value was
1067 bytes. Once you have identified a bucket time and perhaps a significant
entity too, you can drill through to the record results in order to investigate
the anomalous behavior.

[discrete]
[[ml-results-categories]]
=== Categorization results

Categorization results contain the definitions of _categories_ that have been
identified. These are only applicable for jobs that are configured to analyze
unstructured log data using categorization. These results do not contain a
timestamp or any calculated scores. For more information, see
<<ml-configuring-categories>>.

////
From GS tutorial:
Result records for each anomaly are stored in `.ml-anomalies-*` indices in {es}.
By default, the name of the index where {ml} results are stored is labelled
`shared`, which corresponds to the `.ml-anomalies-shared` index.

You can use the **Anomaly Explorer** or the **Single Metric Viewer** in {kib} to
view the analysis results.

Anomaly Explorer::
  This view contains swim lanes showing the maximum anomaly score over time.
  There is an overall swim lane that shows the overall score for the job, and
  also swim lanes for each influencer. By selecting a block in a swim lane, the
  anomaly details are displayed alongside the original source data (where
  applicable).

Single Metric Viewer::
  This view contains a chart that represents the actual and expected values over
  time. This is only available for jobs that analyze a single time series and
  where `model_plot_config` is enabled. As in the **Anomaly Explorer**, anomalous
  data points are shown in different colors depending on their score.
  
  Any data points outside the range that was predicted by the model are marked
  as anomalies. When you have high volumes of real-life data, many anomalies
  might be found. These vary in probability from very likely to highly unlikely,
  that is to say, from not particularly anomalous to highly anomalous. There
  can be none, one or two or tens, sometimes hundreds of anomalies found within
  each bucket. There can be many thousands found per job. In order to provide
  a sensible view of the results, an _anomaly score_ is calculated for each bucket
  time interval. The anomaly score is a value from 0 to 100, which indicates
  the significance of the observed anomaly compared to previously seen anomalies.
  The highly anomalous values are shown in red and the low scored values are
  indicated in blue. An interval with a high anomaly score is significant and
  requires investigation.
  
  ...
  For each anomaly you can see key details such as the time, the actual and
  expected ("typical") values, and their probability.


Multi-detector job:
...
On the left is a list of the top influencers for all of the detected anomalies
in that same time period. The list includes maximum anomaly scores, which in
this case are aggregated for each influencer, for each bucket, across all
detectors. There is also a total sum of the anomaly scores for each influencer.
You can use this list to help you narrow down the contributing factors and focus
on the most anomalous entities.

...NOTE: The anomaly scores that you see in each section of the **Anomaly Explorer**
might differ slightly. This disparity occurs because for each job we generate
bucket results, influencer results, and record results. Anomaly scores are
generated for each type of result. The anomaly timeline uses the bucket-level
anomaly scores. The list of top influencers uses the influencer-level anomaly
scores. The list of anomalies uses the record-level anomaly scores. For more
information about these different result types, see
{ref}/ml-results-resource.html[Results Resources].

////
