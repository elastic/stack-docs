[role="xpack"]
[[anomaly-detection-scale]]
= Using {anomaly-detect} at scale

Working with large {anomaly-jobs} requires careful planning as bigger jobs need 
more resources. This page contains several guidelines and considerations when 
using {anomaly-detect} at scale to get the best out of the feature.


[[infrastructure-config]]
== Infrastructure configuration guidelines


[[number-ml-nodes]]
=== Deciding the number of {ml} nodes 

By using the 
{ref}//ml-estimate-model-memory.html[Estimate {anomaly-jobs} model memory API], 
you can get approximately how much memory each job will require. Based on this 
information, decide how many {ml} nodes you need to add to the cluster to 
support those jobs. A single job runs on a single node. If you have a job that 
requires 10 GB of memory to run, you need a {ml} node that can provide enough 
resources to run the job.

You can control how much of the available native memory the node can use for 
{ml} processes (these processes are separate to {es} JVM) via the 
`xpack.ml.max_machine_memory_percent` setting. If the node is a dedicated {ml} 
node, increase this value from the default 30%. **If you have a cloud 
deployment, you don't need to manually increase this value, it is automatically 
set.**


[[job-config]]
== Job configuration guidelines


[[dedicated-results-index]]
=== Using a dedicated results index

For large jobs use a dedicated results index. This ensures that results from a 
single large job do not dominate the shared results index. It also ensures 
that the job and results (if `results_retention_days` is set) can be deleted 
more efficiently and improves renormalization performance.


[[disabling-model-plot]]
=== Disabling model plot

Model plot calculates and stores the model bounds for each analyzed entity which 
are used to display the shaded area in the single metric viewer charts. Model 
plot creates one result document per bucket per split field value. If you have 
high cardinality fields and/or a short bucket span, disabling model plot will 
reduce resource usage.


[[influencers-detectors]]
=== Choosing influencer fields and detectors

Optimize your {anomaly-job} by choosing only relevant influencer fields and 
detectors.


[[short-bucket-span]]
=== Avoiding short bucket span for large jobs

For machine data, bucket span is typically between 15m and 1h. The recommended 
value always depends on the data, the use case, and the latency required for 
alerting. A job with a longer bucket span uses less resources because fewer 
buckets require processing and fewer results are written. Bucket spans that are 
sensible dividers of an hour or day work best as most periodic patterns have a 
daily cycle.

If your data requires short bucket span, split the data to avoid having 
high cardinality. The combination of short bucket span and high cardinality is a 
resource-intensive situation that you may want to avoid.


[[model-memory-limit]]
=== Setting model memory limit properly

The `model_memory_limit` job variable sets the approximate maximum amount of 
memory resources required for analytical processing. If this variable is set too 
low for the job and the limit is approached, data pruning becomes more 
aggressive. Upon exceeding this limit, new entities are not modeled.

Use model memory estimation to have a better picture of the memory needs of the 
model. Model memory estimation happens automatically when you create the job in 
{kib} or you can call the {ref}//ml-estimate-model-memory.html[Estimate 
{anomaly-jobs} model memory API] manually. The estimation is based on the 
analysis configuration details for the job and cardinality estimates for the 
fields it references.


[[pre-aggregating-data]]
=== Pre-aggregating data

{anomaly-jobs-cap} always work on summary statistics calculated for the buckets. 
The statistics can be calculated in the job itself or via aggregations. It is 
more efficient to use an aggregation when it's possible, as in this case, the 
data node does the heavy-lifting instead of the {ml} node.

In certain cases, you cannot do aggregations to increase performance. For 
example, categorization jobs use the full log message to detect anomalies, so 
this data cannot be aggregated. If you have many influencer fields, it may not 
be beneficial to use an aggregation either, because only a few documents in each 
bucket may have the combination of all the different influencer fields.

Please consult <<ml-configuring-aggregation>> to learn more.


[[search-optimization]]
=== Optimizing search queries

There are different ways to write {es} queries and some of them are more 
efficient than others. If you are operating on a big scale, make sure that your 
{dfeed} query is as efficient as possible. Please consult 
{ref}/tune-for-search-speed.html[Tune for search speed] to learn more about {es} 
performance tuning.


[[results-retention]]
=== Setting result retention time

{anomaly-detect-cap} results are retained indefinitely by default. For big jobs, 
results build up over time, and your result index will be quite large. Consider 
how long you wish to retain the results and set `results_retention_days` 
accordingly – for example, to 30 or 60 days – to avoid unnecessarily large 
result indices. Deleting old results does not affect the model behavior.


[[renormalization]]
=== Configuring renormalization window days

When a new anomaly has a much higher score than any anomaly in the past, the 
anomaly scores are adjusted on a range from 0 to 100 based on the new data. This 
is called renormalization. It can mean rewriting a large number of documents in 
the results index. Renormalization happens for results from the last 30 days or 
100 bucket spans (depending on which is the longer) by default. When you are 
working at scale, set `renormalization_window_days` to a lower value, so the 
workload is reduced.


[[model-snapshots]]
=== Setting model snapshot retention time 

Model snapshots are taken periodically, to ensure resilience in the event of a 
system failure and to allow you to manually revert to a specific point in time. 
These are stored in a compressed format in an internal index and kept according 
to the configured retention policy. Load is placed on the cluster when indexing 
a model snapshot and index size is increased as multiple snapshots are retained.  

When working with large model sizes, consider how frequently you want to create 
model snapshots using `background_persist_interval`. The default is every 3 to 4 
hours. Increasing this interval will reduce the periodic indexing load on your 
cluster, but in the event of a system failure, you may be reverting to an older 
version of the model. 
 
Also consider how long you wish to retain snapshots for using 
`model_snapshot_retention_days` and `daily_model_snapshot_retention_after_days`. 
Retaining fewer snapshots substantially reduces index storage requirements for 
model state, but also reduces the granularity of model snapshots from which you 
can revert.

For more information, refer to <<ml-model-snapshots>>.


[[population-analysis]]
=== Using population analysis

Population analysis is more memory efficient than individual analysis of each 
series. It builds a profile of what a "typical" entity does over a specified 
time period and then identifies when one is behaving abnormally compared to the 
population. Use population analysis for analyzing high cardinality fields if you 
expect that the entities of the population generally behave in the same way.


[[forecasting]]
=== Understanding the cost of forecasting

There are two main factors to consider when you create a forecast: indexing load 
and memory usage.

Forecasting writes a new document to the result index for every forecasted 
element of the job. For jobs with high partition field cardinality, forecasting 
results in a high number of documents being written out to the result index, 
which may add a high load to your data nodes.

The memory usage of a forecast is restricted to 20 MB by default. From 7.9, you 
can extend this limit by setting `max_model_memory` to a higher value where the 
maximum is 40% of the memory limit of the {anomaly-job} or 500 MB. If the 
forecast needs more memory than the provided value, it spools to disk. Forecasts 
that would take more than 500 MB to run won't start because this is the maximum 
limit of disk space that a forecast is allowed to use. Jobs with high memory 
usage may have a significant impact on performance.

Check the cluster monitoring data to learn of the indexing rate and the memory 
usage of your cluster. To avoid performance issues, configure forecasting with a 
small window into the future (for example, a couple of hours), then take action 
if needed. If it is necessary, create another forecast with another small 
window.

Predicting days into the future is not only problematic because of the possible 
performance issues that it can cause. It is also possible that the analyzed 
behavior changes significantly over time, making the forecast irrelevant 
especially for jobs with a short bucket span. As the {anomaly-detect} model is 
updated constantly, forecasting should be considered as a dynamic process. 