[role="xpack"]
[[ml-dfa-limitations]]
== {dfanalytics-cap} limitations
[subs="attributes"]
++++
<titleabbrev>Limitations</titleabbrev>
++++

experimental[]

The following limitations and known problems apply to the {version} release of 
the Elastic {dfanalytics} feature:

[float]
[[dfa-ccs-limitations]]
=== {ccs-cap} is not supported

{ccs-cap} is not supported for {dfanalytics}.

[float]
[[dfa-deletion-limitations]]
=== Deleting a {dfanalytics-job} does not delete the destination index

The {ref}/delete-dfanalytics.html[delete {dfanalytics-job} API] does not delete
the destination index that contains the annotated data of the {dfanalytics}. 
That index must be deleted separately.

[float]
[[dfa-update-limitations]]
=== {dfanalytics-jobs-cap} cannot be updated

You cannot update {dfanalytics} configurations. Instead, delete the 
{dfanalytics-job} and create a new one.

[float]
[[dfa-dataframe-size-limitations]]
=== {dataframe-cap} memory limitation

{dfanalytics-cap} can analyze {transforms} that fit into the memory limit 
dedicated for {ml} processes. For general {ml} settings, see 
{ref}/ml-settings.html[{ml-cap} settings in {es}].

[float]
[[dfa-time-limitations]]
=== {dfanalytics-jobs-cap} runtime may vary

The runtime of {dfanalytics-jobs} depends on numerous factors, such as the
number of data points in the data set, the type of analytics, the number of 
fields that are included in the analysis, the supplied 
{ref}/put-dfanalytics.html#ml-hyperparam-optimization[hyperparameters], the 
type of analyzed fields, and so on. For this reason, a general runtime value that 
applies to all or most of the situations does not exist. The runtime of a 
{dfanalytics-job} may take from a couple of minutes up to 35 hours in extreme 
cases.

The runtime increases with an increasing number of analyzed fields in a nearly 
linear fashion. For data sets of more than 100,000 points, start with a low
training percent. Run a few {dfanalytics-jobs} to see how the runtime scales
with the increased number of data points and how the quality of results scales
with an increased training percentage.

[float]
[[dfa-missing-fields-limitations]]
=== Documents with missing values in analyzed fields are skipped

If there are missing values in feature fields (fields that are subjects of the 
{dfanalytics}), the document that contains these fields is skipped 
during the analysis.

[float]
[[dfa-od-field-type-docs-limitations]]
=== {oldetection-cap} field types

{oldetection-cap} requires numeric or boolean data to analyze. The algorithms 
don't support missing values (see also <<dfa-missing-fields-limitations>>), 
therefore fields that have data types other than numeric or boolean are ignored. 
Documents where included fields contain missing values, null values, or an array 
are also ignored. Therefore a destination index may contain documents that don't 
have an {olscore}. These documents are still reindexed from the source index to the 
destination index, but they are not included in the {oldetection} analysis and 
therefore no {olscore} is computed.

[float]
[[dfa-regression-field-type-docs-limitations]]
=== {regression-cap} field types

{regression-cap} supports fields that are numeric, boolean, text, keyword and 
ip. It is also tolerant of missing values. Fields that are supported are 
included in the analysis, other fields are ignored. Documents where included 
fields contain an array are also ignored. Documents in the destination index 
that don't contain a results field are not included in the {reganalysis}.

[float]
[[dfa-classification-field-type-docs-limitations]]
=== {classification-cap} field types

{classification-cap} supports fields that have numeric, boolean, text, keyword, 
or ip data types. It is also tolerant of missing values. Fields that are 
supported are included in the analysis, other fields are ignored. Documents 
where included fields contain an array are also ignored. Documents in the 
destination index that don't contain a results field are not included in the 
{classanalysis}.

[float]
[[dfa-classification-imbalanced-classes]]
=== Imbalanced class sizes affect {classification} performance

If your training data is very imbalanced, {classanalysis} may not provide 
good predictions. Try to avoid highly imbalanced situations. We recommend having 
at least 50 examples of each class and a ratio of no more than 10 to 1 for the 
majority to minority class labels in the training data. If your training dataset 
is very imbalanced, consider downsampling the majority class, upsampling the 
minority class or – if it's possible – gather more data. Consider investigating 
methods to change data that fit your use case the most.

[float]
[[dfa-inference-nested-limitation]]
=== Deeply nested objects affect {infer} performance

If the data that you run inference against contains documents that have a series 
of combinations of dot delimited and nested fields (for example: 
`{"a.b": "c", "a": {"b": "c"},...}`), the performance of the operation might be 
slightly slower. Consider using as simple mapping as possible for the best 
performance profile.

[float]
[[dfa-feature-importance-limitation]]
=== Analytics runtime performance may significantly slow down with feature importance computation

For complex models – those with many deep trees – the calculation of feature 
importance takes significant extra time. Feature importance is calculated at the 
end of the analysis and therefore the job may appear to be stuck at 99% for 
several hours.

If a reduction in runtime is important to you, try strategies such as disabling 
feature importance, using a smaller transform, setting 
{ref}/put-dfanalytics.html#ml-hyperparam-optimization[hyperparameter] values 
and/or only selecting fields for analysis that are relevant.
