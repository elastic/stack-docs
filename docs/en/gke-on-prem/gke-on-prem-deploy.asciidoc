:gettingstartedwithelasticstack: https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html
:filebeatautodiscoverdocs: https://www.elastic.co/guide/en/beats/filebeat/current/configuration-autodiscover.html
:metricbeatautodiscoverdocs: https://www.elastic.co/guide/en/beats/metricbeat/current/configuration-autodiscover.html
:filebeatmodules: https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-modules.html
:metricbeatmodules: https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-modules.html
:kube-state-metrics: https://github.com/kubernetes/kube-state-metrics
:filebeat-getting-started-view-the-sample-dashboards: https://www.elastic.co/guide/en/beats/filebeat/current/view-kibana-dashboards.html#view-kibana-dashboards
:k8s-rbac: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
:guestbook-app: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook
:k8s-metadata-labels: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/


[[gke-on-prem-deploy]]
== Prepare the Kubernetes environment and deploy a sample application

[[assign-kubernetes-roles]]
=== Assign Kubernetes roles

Logging and metrics tools like kube-state-metrics, Filebeat, Fluentd, Metricbeat, Prometheus, etc. get deployed in the kube-system namespace and have access to all namespaces. Create the cluster wide role binding to allow the deployment of kube-state-metrics and the Beats DaemonSets using the Role Based Access Control {k8s-rbac}[(RBAC) api]:

[source,sh]
----
kubectl create clusterrolebinding cluster-admin-binding  \
  --clusterrole=cluster-admin --user=$(gcloud config get-value account)
----

=== Deploy kube-state-metrics

{kube-state-metrics}[Kube-state-metrics] is a service that exposes metrics and events about the state of the nodes, pods, containers, etc.  Metricbeatâ€™s kubernetes module will connect to kube-state-metrics.
Check to see if kube-state-metrics is running

[source,sh]
----
kubectl get pods --namespace=kube-system | grep kube-state
----

and create it if needed (by default it will not be there)

[source,sh]
----
git clone https://github.com/kubernetes/kube-state-metrics.git
kubectl create -f kube-state-metrics/kubernetes
kubectl get pods --namespace=kube-system | grep kube-state
----

=== Clone the Elastic examples Github repo
[source,sh]
----
git clone https://github.com/elastic/examples.git
----

The remainder of the steps will refer to files from this repo, change directory into `examples/GKE-on-Prem-logging-and-metrics`.

=== Example application
If you are just getting started with GKE On-Prem and do not have anything running you can use a sample {guestbook-app}[guestbook application] from the Kubernetes engine documentation. The YAML has been concatenated into a single manifest, and some changes have been made to serve as an example for enabling Beats to autodiscover the components of the application.
Whether or not you deploy the example application, this documentation will refer to specific parts of the `guestbook.yaml` manifest file.

=== Network considerations
Before you deploy the sample application manifest have a look at the frontend service in `GKE-on-Prem-logging-and-metrics/guestbook.yaml`  You may need to edit this service so that the service is exposed to your internal network. The network topology of the lab where this example was developed has a load balancer in front of the GKE On-Prem environment, and so the service specifies an IP Address associated with the load balancer. Your configuration will likely be different.


[source,sh]
----
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: guestbook
    tier: frontend
  loadBalancerIP: 10.0.10.42 <1>
----

<1> Edit the file `guestbook.yaml` as appropriate to integrate with your network.

=== Label your application pods
The Beats autodiscover functionality is facilitated by Kubernetes metadata.  In the example manifest there are {k8s-metadata-labels}[metadata labels] assigned to the deployments, and the Filebeat and Metricbeat configurations are updated to expect this metadata.

These lines from the `guestbook.yaml` manifest file add the `app: redis` label to the Redis deployments:

[source,sh]
----
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis <1> <2>
----

<1> This label is added to the metadata for the k8s deployment and is applied to each pod in the deployment.
<2> You should create labels that are appropriate for your use case, `app: redis` is only an example.

These lines from the `filebeat-kubernetes.yaml` manifest file configure Filebeat to autodiscover Redis pods that have the appropriate label:

[source,sh]
----
   filebeat.autodiscover:
      providers:
        - type: kubernetes
          templates:
            - condition.contains: <1>
                kubernetes.labels.app: redis <2>
              config:
                - module: redis <3>
----

<1> Specifies that the condition is looking for a substring and not an exact match
<2> The label to inspect, and the substring to look for 
<3> The module to use when collecting, parsing, indexing, and visualizing logs from pods that meet the condition

If you are using the example application to get started with GKE On-Prem and the Elastic Stack, then deploy the sample application

[source,sh]
----
kubectl create -f guestbook.yaml
----

If you are ready to manage logs and metrics from your own application, then examine your pods for existing labels and update the Filebeat and Metricbeat autodiscover configuration within `filebeat-kubernetes.yaml` and `metricbeat-kubernetes.yaml` respectively.  See the documentation for configuring {filebeatautodiscoverdocs}[Filebeat autodiscover] and {metricbeatautodiscoverdocs}[Metricbeat autodiscover], you will also need the list of {filebeatmodules}[Filebeat modules] and {metricbeatmodules}[Metricbeat modules]. 

== Deploy Beats

TIP: If you do not have an Elasticsearch cluster with Kibana available, see {gettingstartedwithelasticstack}[Getting started with the Elastic Stack] and deploy Elasticsearch and Kibana, then come back to this page to deploy Beats.

=== Kubernetes secrets
Rather than putting the Elasticsearch and Kibana endpoints into the manifest files they are provided to the Filebeat pods as k8s secrets.  Edit the files elasticsearch-hosts-ports and kibana-host-port.  The files provided in the example contain details regarding the file format.  You should have two files resembling:

`elasticsearch-hosts-ports`:
[source,sh]
----
["http://10.1.1.4:9200", "http://10.1.1.5:9200"]
----

`kibana.host.port`:
[source,sh]
----
"http://10.1.1.6:5601"
----

=== Create the secret:

[source,sh]
----
kubectl create secret generic elastic-stack \
  --from-file=./elasticsearch-hosts-ports \
  --from-file=./kibana-host-port --namespace=kube-system
----


=== Deploy index patterns, visualizations, dashboards, and machine learning jobs
Filebeat and Metricbeat provide the configuration for things like web servers, caches, proxies, operating systems, container environments, databases, etc. These are referred to as Beats modules. By deploying these configurations you will be populating Elasticsearch and Kibana with index patterns, visualizations, dashboards, machine learning jobs, etc.

[source,sh]
----
kubectl create -f filebeat-setup.yaml
kubectl create -f metricbeat-setup.yaml
----

NOTE: These setup jobs are short lived, you will see them transition to the completed state in the output of `kubectl get pods -n kube-system`

=== Verify
kubectl get pods -n kube-system | grep beat
Verify that the setup pods complete Check the logs for the setup pods to ensure that they connected to Elasticsearch and Kibana (the setup pod connects to both)

=== Deploy the Beat DaemonSets

[source,sh]
----
kubectl create -f filebeat-kubernetes.yaml
kubectl create -f metricbeat-kubernetes.yaml
----

NOTE: Depending on your k8s node configuration, you may not need to deploy Journalbeat. If your Nodes use journald for logging, then deploy Journalbeat, otherwise Filebeat will get the logs.

[source,sh]
----
kubectl create -f journalbeat-kubernetes.yaml
----

=== Verify
Check for the running DaemonSets
Verify that there is one filebeat, metricbeat, and journalbeat pod per k8s Node running.

[source,sh]
----
kubectl get pods -n kube-system | grep beat
----

== View your logs and metrics in Kibana
You should be able to visualize your logs and metrics in the Kibana Discover app and in dashboards provided by the Beats modules that you are using.
See the {filebeat-getting-started-view-the-sample-dashboards}[getting started guide] for details.  If you deployed the sample Guestbook application, then you will have data in the Apache and Redis dashboards along with the Kubernetes and System dashboards.  If you are collecting logs and metrics from your own application, then see the dashboards for the modules related to your application.

image:images/redis-dashboard.png[]
Sample Filebeat Redis dashboard

