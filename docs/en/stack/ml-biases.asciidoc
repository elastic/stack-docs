[[ml-biases]]
=== Possible biases in {ml} processes

{ml-cap} is meant to improve our everyday life in a wide palette of activities 
across the globe. We use {ml} in science, healthcare, business, 
entertainment, and many other fields. We make decisions based on {ml} results.
It is therefore crucial to keep in mind the possible biases 
that can corrupt results, endanger fairness, and jeopardize the success of even 
the best-intended initiative. Some of these social biases can make their way into 
the {ml} analytics results more easily than we might think.

There are various possible biases that can negatively influence the result of a 
{ml} analysis. *These biases typically are not present in the {ml} algorithms 
that are used*, but rather exist in the datasets themselves or in the way the 
results are interpreted. We take a look at the two main types of biases: data 
related ones and interpretation related ones and recommend some basic practices 
to avoid them. However, it is still important to remember that the problems 
thematized in this piece are not solved yet. In fact, they are only a part of a 
greater question about the ethics behind technology.


[float]
==== Biases in datasets

Built-in biases are inserted in the data that we use to train the {ml} model. 
For example, it could be the result of manual labeling when the dataset has been 
annotated with the "ground truth" based on preconception or prejudice. The model 
will inherently learn the bias, therefore the analysis will be also infected, 
while technically the algorithm works exactly as intended.

It is impossible to avoid all the potential biases in the dataset. Eventually, 
data is the representation of reality and, like any other type of 
representation, it never perfectly concurs with its subject. It is possible, 
however, to carefully scrutinize a dataset and make sure that it's not selected 
in a predetermined way and it matches the real world as close as possible. To 
involve people with diverse backgrounds in the processes that require humans can 
also be a solution to minimize the chance of biases.


[float]
==== Biases in interpretation

People who interpret the data can also be biased, for example, when they would 
like to validate a hypothesis based on the results of an analysis. In these 
cases, the outcome of the analysis or the decision that is based on the analysis 
can be inaccurate even if the dataset itself is carefully designed.

It is safer to formulate the aim of the analysis in a way that does not contain 
the expected results, interpret the outcome of the analysis multiple times 
with different persons, and perform tests as much as possible.


[float]
==== Collective responsibility

The issue of biases that can affect {ml} related processes is far from solved. 
In fact, this is a field of an ongoing dispute and there are no universal 
answers for the emerged problems. Because of the various factors that can 
influence the {ml} processes, avoiding bias-influenced analysis is a collective 
responsibility of all the persons that participate in these processes; from the 
designer of the dataset to the person who interprets the outcome of the analysis 
to the ones who make decisions based on the data.
