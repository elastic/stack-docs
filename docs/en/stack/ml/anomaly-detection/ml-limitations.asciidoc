[role="xpack"]
[[ml-limitations]]
= {ml-cap} {anomaly-detect} limitations
[subs="attributes"]
++++
<titleabbrev>Limitations</titleabbrev>
++++

The following limitations and known problems apply to the {version} release of 
the Elastic {ml-features}. The limitations are grouped into four categories:

* <<ad-platform-limitations>> are related to the platform that hosts the {ml} 
  feature of the {stack}.
* <<ad-config-limitations>> apply to the configuration process of the 
  {anomaly-jobs}.
* <<ad-operational-limitations>> affect the behavior of the {anomaly-jobs} that 
  are running.
* <<ad-ui-limitations>> only apply to {anomaly-jobs} managed via the user 
  interface.


[discrete]
[[ad-platform-limitations]]
== Platform limitations

[discrete]
[[ml-limitations-sse]]
=== CPUs must support SSE4.2

{ml-cap} uses Streaming SIMD Extensions (SSE) 4.2 instructions, so it works only
on machines whose CPUs 
https://en.wikipedia.org/wiki/SSE4#Supporting_CPUs[support] SSE4.2. If you run 
{es} on older hardware you must disable {ml} by setting `xpack.ml.enabled` to 
`false`. See {ref}/ml-settings.html[{ml-cap} settings in {es}].


[discrete]
[[ml-scheduling-priority]]
=== CPU scheduling improvements apply to Linux and MacOS only

When there are many {ml} jobs running at the same time and there are 
insufficient CPU resources, the JVM performance must be prioritized so search 
and indexing latency remain acceptable. To that end, when CPU is constrained on 
Linux and MacOS environments, the CPU scheduling priority of native analysis 
processes is reduced to favor the {es} JVM. This improvement does not apply to 
Windows environments.


[discrete]
[[ad-config-limitations]]
== Configuration limitations


[discrete]
=== Terms aggregation size affects data analysis
//See x-pack-elasticsearch/#601

By default, the `terms` aggregation returns the buckets for the top ten terms.
You can change this default behavior by setting the `size` parameter.

If you send pre-aggregated data to a job for analysis, you must ensure that the 
`size` is configured correctly. Otherwise, some data might not be analyzed.


[discrete]
=== Fields named "by", "count", or "over" cannot be used to split data
//See x-pack-elasticsearch/#858

You cannot use the following field names in the `by_field_name` or
`over_field_name` properties in a job: `by`; `count`; `over`. This limitation
also applies to those properties when you create advanced jobs in {kib}.


[discrete]
[[ml-arrays-limitations]]
=== Arrays in analyzed fields are turned into comma-separated strings

If an {anomaly-job} is configured to analyze an aggregatable field (a field that 
is part of the index mapping definition), and this field contains an array, then 
the array is turned into a comma-separated concatenated string. The items in the 
array are sorted alphabetically and the duplicated items are removed. For 
example, the array `["zebra", "dog", "cat", "alligator", "cat"]` becomes 
`alligator,cat,dog,zebra`. The Anomaly Explorer charts don't display any results 
for the job as the string does not exist in the source data. The Single Metric 
Viewer displays results if the model plot is enabled.

If an array field is not aggregatable and is retrieved from `_source`, the array 
is also turned into a comma-separated, concatenated list. However, the list 
items are not sorted alphabetically, nor are they deduplicated. Taking the 
example above, the comma-separated list, in this case, would be
`zebra,dog,cat,alligator,cat`.

Analyzing large arrays results in long strings which may require more system 
resources. Consider using a query in the {dfeed} that filters on the relevant 
items of the array.





[discrete]
[[ml-forecast-config-limitations]]
=== Unsupported forecast configurations

There are some limitations that affect your ability to create a forecast:

* You can generate only three forecasts per {anomaly-job} concurrently. There is 
no limit to the number of forecasts that you retain. Existing forecasts are not 
overwritten when you create new forecasts. Rather, they are automatically 
deleted when they expire.
* If you use an `over_field_name` property in your {anomaly-job} (that is to 
say, it's a _population job_), you cannot create a forecast.
* If you use any of the following analytical functions in your {anomaly-job},
you cannot create a forecast:
** `lat_long`
** `rare` and `freq_rare`
** `time_of_day` and `time_of_week`
+
--
For more information about any of these functions, see <<ml-functions>>.
--


[discrete]
[[ml-limitations-runtime]]
=== {anomaly-detect-cap} performs better on indexed fields

{anomaly-jobs-cap} sort all data by a user-defined time field, which is 
frequently accessed. If the time field is a {ref}/runtime.html[runtime field], 
the performance impact of calculating field values at query time can 
significantly slow the job. Use an indexed field as a time field when running 
{anomaly-jobs}.

[discrete]
[[ml-limitations-painless-script]]
=== Deprecation warnings for Painless scripts in {dfeeds}

If a {dfeed} contains Painless scripts that use deprecated syntax, deprecation 
warnings are displayed when the {dfeed} is previewed or started. However, it is 
not possible to check for deprecation warnings across all {dfeeds} as a bulk 
action because running the required queries might be a resource intensive 
process. Therefore any deprecation warnings due to deprecated Painless syntax are 
not available in the Upgrade assistant.


[discrete]
[[ad-operational-limitations]]
== Operational limitations


[discrete]
=== Categorization uses English dictionary words
//See x-pack-elasticsearch/#3021
Categorization identifies static parts of unstructured logs and groups similar
messages together. The default categorization tokenizer assumes English language
log messages. For other languages you must define a different
`categorization_analyzer` for your job. For more information, see
<<ml-configuring-categories>>.

Additionally, a dictionary used to influence the categorization process contains
only English words. This means categorization might work better in English than
in other languages. The ability to customize the dictionary will be added in a
future release.


[discrete]
=== Misleading high missing field counts
//See x-pack-elasticsearch/#684

One of the counts associated with a {ml} job is `missing_field_count`,
which indicates the number of records that are missing a configured field.
//This information is most useful when your job analyzes CSV data. In this case,
//missing fields indicate data is not being analyzed and you might receive poor results.

Since jobs analyze JSON data, the `missing_field_count` might be misleading.
Missing fields might be expected due to the structure of the data and therefore
do not generate poor results.

For more information about `missing_field_count`,
see the {ref}/ml-get-job-stats.html[get {anomaly-job} statistics API].


[discrete]
=== Security integration

When the {es} {security-features} are enabled, a {dfeed} stores the roles of the 
user who created or updated the {dfeed} **at that time**. This means that if the 
roles the user has are changed after they create or update a {dfeed} then the 
{dfeed} continues to run without change. However, if instead the permissions 
associated with the roles that are stored with the {dfeed} are changed then this 
affects the {dfeed}. For more information, see <<ml-ad-datafeeds>>.


[discrete]
[[ml-result-size-limitations]]
=== Job and {dfeed} APIs have a maximum search size
//https://github.com/elastic/elasticsearch/issues/34864

In 6.6 and later releases, the {ref}/ml-get-job.html[get jobs API] and the
{ref}/ml-get-job-stats.html[get job statistics API] return a maximum of 10,000
jobs. Likewise, the {ref}/ml-get-datafeed.html[get {dfeeds} API] and the
{ref}/ml-get-datafeed-stats.html[get {dfeed} statistics API] return a maximum of
10,000 {dfeeds}.


[discrete]
[[ml-forecast-limitations]]
=== Forecast operational limitations

There are some factors that may be considered when you run forecasts:

* Forecasts run concurrently with real-time {ml} analysis. That is to say, {ml}
analysis does not stop while forecasts are generated. Forecasts can have an
impact on {anomaly-jobs}, however, especially in terms of memory usage. For this
reason, forecasts run only if the model memory status is acceptable.
* The {anomaly-job} must be open when you create a forecast. Otherwise, an error
occurs.
* If there is insufficient data to generate any meaningful predictions, an
error occurs. In general, forecasts that are created early in the learning phase
of the data analysis are less accurate.


[discrete]
[[ad-ui-limitations]]
== Limitations in {kib}


[discrete]
=== Pop-ups must be enabled in browsers
//See x-pack-elasticsearch/#844

The {ml-features} in {kib} use pop-ups. You must configure your web browser so 
that it does not block pop-up windows or create an exception for your {kib} URL.


[discrete]
=== Anomaly Explorer and Single Metric Viewer omissions and limitations
//See x-pack-elasticsearch/#844 and x-pack-kibana/#1461

In {kib}, **Anomaly Explorer** and **Single Metric Viewer** charts are not 
displayed for anomalies that were due to categorization (if model plot is not 
enabled), `time_of_day` functions, `time_of_week` functions, or `lat_long` 
geographic functions.

If model plot is not enabled, the charts are not displayed for detectors that 
use script fields either (except for scripts that define metric fields). In that 
case, the original source data cannot be easily searched because it has been 
transformed by the script.

If your <<aggs-dfeeds,{dfeed} uses aggregations with nested `terms` aggs>> and 
model plot is not enabled for the {anomaly-job}, neither the **Anomaly 
Explorer** nor the **Single Metric Viewer** can plot and display an anomaly 
chart for the job. In these cases, the charts are not visible and an explanatory 
message is shown.

The charts can also look odd in circumstances where there is very little data to 
plot. For example, if there is only one data point, it is represented as a 
single dot. If there are only two data points, they are joined by a line.


[discrete]
=== Jobs created in {kib} must use {dfeeds}

If you create jobs in {kib}, you must use {dfeeds}. If the data that you want to
analyze is not stored in {es}, you cannot use {dfeeds} and therefore you cannot
create your jobs in {kib}. You can, however, use the {ml} APIs to create jobs. For more information, see
<<ml-ad-datafeeds>> and <<ml-api-quickref>>.


[discrete]
=== Jobs created in {kib} use model plot config and pre-aggregated data
//See x-pack-elasticsearch/#844

If you create single or multi-metric jobs in {kib}, it might enable some
options under the covers that you'd want to reconsider for large or
long-running jobs.

For example, when you create a single metric job in {kib}, it generally
enables the `model_plot_config` advanced configuration option. That 
configuration option causes model information to be stored along with the 
results and provides a more detailed view into {anomaly-detect}. It is 
specifically used by the **Single Metric Viewer** in {kib}. When this option is 
enabled, however, it can add considerable overhead to the performance of the 
system. If you have jobs with many entities, for example data from tens of 
thousands of servers, storing this additional model information for every bucket 
might be problematic. If you are not certain that you need this option or if you 
experience performance issues, edit your job configuration to disable this 
option.

Likewise, when you create a single or multi-metric job in {kib}, in some cases
it uses aggregations on the data that it retrieves from {es}. One of the
benefits of summarizing data this way is that {es} automatically distributes
these calculations across your cluster. This summarized data is then fed into
{ml} instead of raw results, which reduces the volume of data that must
be considered while detecting anomalies. However, if you have two jobs, one of
which uses pre-aggregated data and another that does not, their results might
differ. This difference is due to the difference in precision of the input data.
The {ml} analytics are designed to be aggregation-aware and the likely increase
in performance that is gained by pre-aggregating the data makes the potentially
poorer precision worthwhile. If you want to view or change the aggregations
that are used in your job, refer to the `aggregations` property in your {dfeed}. 

When the aggregation interval of the {dfeed} and the bucket span of the job 
don't match, the values of the chart plotted in both the **Single Metric 
Viewer** and the **Anomaly Explorer** differ from the actual values of the job. 
To avoid this behavior, make sure that the aggregation interval in the {dfeed} 
configuration and the bucket span in the {anomaly-job} configuration have the 
same values.


[discrete]
[[ml-space-limitations]]
=== Calendars and filters are visible in all {kib} spaces

{kibana-ref}/xpack-spaces.html[Spaces] enable you to organize your
{anomaly-jobs} in {kib} and to see only the jobs and other saved objects
that belong to your space. However, this limited scope does not apply to 
<<ml-calendars,calendars>> and <<ml-rules,filters>>; they are visible in all
spaces.


[discrete]
[[ml-rollup-limitations]]
=== Rollup indices are not supported in {kib}

Rollup indices and {data-sources} with rolled up indices cannot be used in 
{anomaly-jobs} or {dfeeds} in {kib}. If you try to analyze data that exists in 
an index that uses the experimental {ref}/xpack-rollup.html[{rollup-features}], 
the {anomaly-job} creation wizards fail. If you use APIs to create 
{anomaly-jobs} that use {rollup-features}, the job results might not display 
properly in the *Single Metric Viewer* or *Anomaly Explorer* in {kib}.