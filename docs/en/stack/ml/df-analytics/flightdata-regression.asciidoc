[role="xpack"]
[testenv="platinum"]
[[flightdata-regression]]
=== Predicting flight delays with {reganalysis}

Let's try to predict flight delays by using the 
{kibana-ref}/add-sample-data.html[sample flight data]. We use this data to learn 
a {regression} model that allows us to predict flight delays based on 
information such as weather and location of the destination and origin, flight 
distance and carrier. For example, we can predict the number of minutes delayed for 
each flight. As it is a continuous numeric variable, we'll use {reganalysis} to 
make the prediction.

This dataset consists of sample data that has been manually created and it 
contains inconsistencies. For example, a flight can be both delayed and 
canceled. 
demonstrate {reganalysis} because it is easy to set up in Kibana, the use case 
is relevant and it is a good example of how the quality of input data will 
affect the quality of results.

We will create an {dfanalytics-job} to predict the minutes that flights are 
delayed by. Each document in the dataset contains details for a single flight, 
so this data is ready for analysis as it is already in a two-dimensional 
entity-based data structure that we call a {dataframe}. Depending on your source 
data, it may be required to first {ref}/transforms.html[transform] your source 
data into an entity-centric index before you could setup and start the analysis.

This is an example source document from the dataset:

```
{
  "_index": "kibana_sample_data_flights",
  "_type": "_doc",
  "_id": "S-JS1W0BJ7wufFIaPAHe",
  "_version": 1,
  "_seq_no": 3356,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "FlightNum": "N32FE9T",
    "DestCountry": "JP",
    "OriginWeather": "Thunder & Lightning",
    "OriginCityName": "Adelaide",
    "AvgTicketPrice": 499.08518599798685,
    "DistanceMiles": 4802.864932998549,
    "FlightDelay": false,
    "DestWeather": "Sunny",
    "Dest": "Chubu Centrair International Airport",
    "FlightDelayType": "No Delay",
    "OriginCountry": "AU",
    "dayOfWeek": 3,
    "DistanceKilometers": 7729.461862731618,
    "timestamp": "2019-10-17T11:12:29",
    "DestLocation": {
      "lat": "34.85839844",
      "lon": "136.8049927"
    },
    "DestAirportID": "NGO",
    "Carrier": "ES-Air",
    "Cancelled": false,
    "FlightTimeMin": 454.6742272195069,
    "Origin": "Adelaide International Airport",
    "OriginLocation": {
      "lat": "-34.945",
      "lon": "138.531006"
    },
    "DestRegion": "SE-BD",
    "OriginAirportID": "ADL",
    "OriginRegion": "SE-BD",
    "DestCityName": "Tokoname",
    "FlightTimeHour": 7.577903786991782,
    "FlightDelayMin": 0
  }
}
```


{regression-cap} is a supervised machine learning analysis and therefore needs 
to train on data that contains the ground truth for the `dependent_variable` 
that we want to predict. In this example, the ground truth is available in each 
document as the actual value of `FlightDelayMins`.

If your source data consisted of documents that contains a `dependent_variable` 
and some that does not contain a `dependent_variable`, then training is 
performed on the `training_percent` of the documents that contain ground truth 
and predictions will be made against all of the data. The current implementation 
of {reganalysis} supports a single batch analysis for both training and 
predictions. Future versions will allow a learned model to be incorporated at 
index time.

. Create a {dfanalytics-job}.
+
--
Use the {ref}/put-dfanalytics.html[create {dfanalytics-jobs}] API as you can see 
in the following example:

[source,console]
--------------------------------------------------
PUT _ml/data_frame/analytics/model-flight-delays
{
  "source": {
    "index": [
      "kibana_sample_data_flights" <1>
    ],
    "query": { <2>
      "range": {
        "DistanceKilometers": { 
          "gt": 0
        }
      }
    }
  },
  "dest": {
    "index": "df-flight-delays"  <3>
  },
  "analysis": {
    "regression": {
      "dependent_variable": "FlightDelayMin",  <4>
      "training_percent": 90  <5>
    }
  },
  "analyzed_fields": {
    "includes": [],
    "excludes": [    <6>
      "Cancelled",
      "FlightDelay",
      "FlightDelayType"
    ]
  },
  "model_memory_limit": "100mb" <7>
}
--------------------------------------------------
// TEST[skip:setup kibana sample data]

<1> The source index to analyze.
<2> This query removes erroneous data from the analysis to improve its quality.
<3> The index that will contain the results of the analysis; it will consist of 
a copy of the source index data where each document is annotated with the 
results.
<4> Specifies the continuous variable we want to predict with the {reganalysis}.
<5> Specifies the proportion of data that is used for training the model. In 
this example we use 90% of the source data. The training data is randomly 
selected and provide predictions from the remaining 10%. These percentages are 
approximate.
<6> Specifies fields to be excluded from the analysis. It is recommended to 
exclude fields that either contain erroneous data or describe the 
`dependent_variable`.
<7> Specifies a memory limit for the job. If the job requires more than this 
amount of memory, it fails to start. This makes it possible to prevent job 
execution if the available memory on the node is limited.


The API returns the following response:

[source,console-result]
--------------------------------------------------  
{
  "id" : "model-flight-delays",
  "source" : {
    "index" : [
      "kibana_sample_data_flights"
    ],
    "query" : {
      "range" : {
        "DistanceKilometers" : {
          "gt" : 0
        }
      }
    }
  },
  "dest" : {
    "index" : "df-flight-delays",
    "results_field" : "ml"
  },
  "analysis" : {
    "regression" : {
      "dependent_variable" : "FlightDelayMin",
      "training_percent" : 90.0
    }
  },
  "analyzed_fields" : {
    "includes" : [ ],
    "excludes" : [
      "Cancelled",
      "FlightDelay",
      "FlightDelayType"
    ]
  },
  "model_memory_limit" : "100mb",
  "create_time" : 1571918329132,
  "version" : "7.5.0",
  "allow_lazy_start" : false
}
--------------------------------------------------
--

. Start the job.
+
--
Use the {ref}/start-dfanalytics.html[start {dfanalytics-jobs}] API to start the 
job. It will stop automatically when the analysis is complete, you don't need to 
stop it manually.

[source,console]
--------------------------------------------------
POST _ml/data_frame/analytics/model-flight-delays/_start
--------------------------------------------------
// TEST[skip:TBD]


The job takes a few minutes to run. Runtime depends on the local hardware and 
also on the number of documents and fields that analyzed. The more fields and 
documents, the longer the job to run.
--

. Check the job stats to follow the progress by using the 
{ref}/get-dfanalytics-stats.html[get {dfanalytics-jobs} statistics API].
+
--


[source,console]
--------------------------------------------------
GET _ml/data_frame/analytics/model-flight-delays/_stats
--------------------------------------------------
// TEST[skip:TBD]


The API call returns the following response: 

[source,console-result]
----  
{
  "count" : 1,
  "data_frame_analytics" : [
    {
      "id" : "model-flight-delays",
      "state" : "stopped",
      "progress" : [
        {
          "phase" : "reindexing",
          "progress_percent" : 100
        },
        {
          "phase" : "loading_data",
          "progress_percent" : 100
        },
        {
          "phase" : "analyzing",
          "progress_percent" : 100
        },
        {
          "phase" : "writing_results",
          "progress_percent" : 100
        }
      ]
    }
  ]
}
----  


The job has four phases. When all the phases have completed, the job 
state becomes `stopped` and the results are ready to view and evaluate.
--


[[flightdata-regression-results]]
==== Viewing results

. Use the standard {es} search command to view the results in the destination 
index:
+
--

[source,console]
--------------------------------------------------
GET df-flight-delays/_search
--------------------------------------------------
// TEST[skip:TBD]



The snippet below shows a part of a document with the annotated results:

[source,console-result]
----  
          ...
          "DestRegion" : "UK",
          "OriginAirportID" : "LHR",
          "DestCityName" : "London",
          "FlightDelayMin" : 66,      <1>
          "ml" : {
            "FlightDelayMin_prediction" : 62.527,   <2>
            "is_training" : false   <3>
          }
          ...
----

<1> The `dependent_variable` with the ground truth value. This is what we are 
trying to predict with the {reganalysis}.
<2> The prediction. The field name is suffixed with `_prediction`.
<3> Indicates that this document was not used in the training set.


If a document doesn't contain a prediction field, then it is excluded from the 
analysis. In order to be analyzed, a document must contain at least one field 
with a supported data type (`numeric`, `boolean`, `text`, `keyword` or `ip`) and 
must not contain arrays with more than one item.
--


[[flightdata-regression-evaluate]]
==== Evaluating results

The results can be evaluated for documents which contain both the ground truth 
field and the prediction. In the example below, `FlightDelayMins` contains the 
ground truth and the prediction is stored as `ml.FlightDelayMin_prediction`.

. Use the {dfanalytics} evaluate API to evaluate the results.
+
--
First, we want to know the training error that represents how well the model 
performed on the training dataset:

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
 "index": "df-flight-delays",   <1>
  "query": {
      "bool": {
        "filter": [{ "term":  { "ml.is_training": true } }]  <2>
      }
    },
 "evaluation": {
   "regression": {
     "actual_field": "FlightDelayMin",   <3>
     "predicted_field": "ml.FlightDelayMin_prediction", <4>
     "metrics": {  
       "r_squared": {},
       "mean_squared_error": {}                            
     }
   }
 }
}
--------------------------------------------------
// TEST[skip:TBD]

<1> The destination index which is the output of the analysis job.
<2> We calculate the training error by only evaluating the training data.
<3> The ground truth label.
<4> Predicted value.

Next, we calculate the generalization error that represents how well the model 
performed on previously unseen data:

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
 "index": "df-flight-delays",
  "query": {
      "bool": {
        "filter": [{ "term":  { "ml.is_training": false } }] <1>
      }
    },
 "evaluation": {
   "regression": {
     "actual_field": "FlightDelayMin",
     "predicted_field": "ml.FlightDelayMin_prediction",
     "metrics": {  
       "r_squared": {},
       "mean_squared_error": {}                            
     }
   }
 }
}
--------------------------------------------------
// TEST[skip:TBD]
<1> By only evaluating the data that was not used in training, we can 
calculate the generalization error which shows the algorithm accuracy in making 
predictions for previously unseen data.


The evaluate {dfanalytics} API returns the following response:

[source,console-result]
----  
{
  "regression" : {
    "mean_squared_error" : {
      "error" : 3759.7242253334207
    },
    "r_squared" : {
      "value" : 0.5853159777330623
    }
  }
}
----

For more information about the evaluation metrics, see 
<<dfa-regression-evaluation>>.

If you don't want to keep the {dfanalytics-job}, you can delete it by using the 
{ref}/delete-dfanalytics.html[delete {dfanalytics-job} API]. When you delete 
{dfanalytics-jobs}, the destination indices remain intact.
--
