[role="xpack"]
[[ml-gs-results]]
=== View {anomaly-detect} results

After the {dfeeds} are started and the {anomaly-jobs} have processed some data,
you can view the results in {kib}.

TIP: Depending on the capacity of your machine, you might need to wait a few
seconds for the {ml} analysis to generate initial results.

[role="screenshot"]
image::images/ml-gs-web-results.jpg["Create jobs for the sample web logs"]

The {ml-features} analyze the input stream of data, model its behavior, and
perform analysis based on the detectors in each job. When an event occurs
outside of the model, that event is identified as an anomaly. You can
immediately see that all three jobs have found anomalies, which are indicated by
red blocks in the swim lanes for each job.

There are two tools for examining the results from {anomaly-jobs} in {kib}: the
**Anomaly Explorer** and the **Single Metric Viewer**.

[discrete]
[[ml-gs-results-smv]]
=== Single metric job results

One of the sample jobs (`low_request_rate`), is a _single metric {anomaly-job}_.
It has a single detector that uses the `low_count` function and limited job
properties. Let's start by looking at this simple job in the
**Single Metric Viewer**:

[role="screenshot"]
image::images/ml-gs-job1-analysis.jpg["Single Metric Viewer for low_request_rate job"]

This view contains a chart that represents the actual and expected values over
time. It is available only if the job has `model_plot_config` enabled. It can
display only a single time series.

The blue line in the chart represents the actual data values. The shaded blue
area represents the bounds for the expected values. The area between the upper
and lower bounds are the most likely values for the model. If a value is outside
of this area then it can be said to be anomalous.

If you slide the time selector from the beginning to the end of the data, you
can see how the model improves as it processes more data. At the beginning, the
expected range of values is pretty broad and the model is not capturing the
periodicity in the data. But it quickly learns and begins to reflect the
patterns in your data.

.Anomaly scores
****
Any data points outside the range that was predicted by the model are marked
as anomalies. In order to provide a sensible view of the results, an
_anomaly score_ is calculated for each bucket time interval. The anomaly score
is a value from 0 to 100, which indicates the significance of the anomaly
compared to previously seen anomalies. The highly anomalous values are shown in
red and the low scored values are indicated in blue. An interval with a high
anomaly score is significant and requires investigation.
****

Slide the time selector to a section of the time series that contains a red
anomaly data point. If you hover over the point, you can see more information.
You can also see details in the **Anomalies** section of the viewer. For example:

[role="screenshot"]
image::images/ml-gs-job1-anomalies.jpg["Single Metric Viewer Anomalies for low_request_rate job"]

For each anomaly you can see key details such as the time, the actual and
expected ("typical") values, and their probability.

By default, the table contains all anomalies that have a severity of "warning"
or higher in the selected section of the timeline. If you are only interested in
critical anomalies, for example, you can change the severity threshold for this
table.

The anomalies table also automatically calculates an interval for the data in
the table. If the time difference between the earliest and latest records in the
table is less than two days, the data is aggregated by hour to show the details
of the highest severity anomaly for each detector. Otherwise, it is aggregated
by day. You can change the interval for the table, for example, to show all
anomalies.

After you have identified anomalies, often the next step is to try to determine
the context of those situations. For example, are there other factors that are
contributing to the problem? Are the anomalies confined to particular
applications or servers? You can begin to troubleshoot these situations by
layering additional jobs or creating multi-metric jobs.

[discrete]
[[ml-gs-results-ae]]
=== Advanced or multi-metric job results

Conceptually, you can think of _multi-metric {anomaly-jobs}_ as running multiple
independent single metric jobs. By bundling them together in a multi-metric job,
however, you can see an overall score and shared influencers for all the metrics
and all the entities in the job. Multi-metric jobs therefore scale better than
having many independent single metric jobs. They also provide better results
when you have influencers that are shared across the detectors.

.Influencers
****
When you create an {anomaly-job}, you can identify fields as _influencers_.
These are fields that you think contain information about someone or something
that influences or contributes to anomalies. There are influencers in both the
`response_code_rates` and `url_scanning` jobs.

As a best practice, do not pick too many influencers. For example, you generally
do not need more than three. If you pick many influencers, the results can be
overwhelming and there is a small overhead to the analysis. For more details,
see <<ml-influencers>>.

****

You can also configure your {anomaly-jobs} to split a single time series into
multiple time series based on a categorical field. For example, the
`response_code_rates` job splits the data based on the `response.keyword`. When
you view the results in the **Anomaly Explorer**, you can see separate swim
lanes for the `404` and `200` responses:

[role="screenshot"]
image::images/ml-gs-job2-explorer.jpg["Anomaly explorer for response_code_rates job"]

In this scenario, the analysis is segmented such that you have completely
different baselines for each distinct value of the _partition field_. By looking
at temporal patterns on a per entity basis, you might spot things that might
have otherwise been hidden in the lumped view.

On the left side of the **Anomaly Explorer**, there is a list of the top
influencers for all of the detected anomalies in that same time period. The list
includes maximum anomaly scores, which in this case are aggregated for each
influencer, for each bucket, across all detectors. There is also a total sum of
the anomaly scores for each influencer. You can use this list to help you narrow
down the contributing factors and focus on the most anomalous entities.

Click on a section in the swim lanes to obtain more information about the
anomalies in that time period. For example, click on the red section in the
swim lane for the `response.keyword` value of `200`:

[role="screenshot"]
image::images/ml-gs-job2-explorer-anomaly.jpg["Anomaly charts for the response_code_rates job"]

You can see exact times when anomalies occurred. If there are multiple detectors
or metrics in the job, you can see which caught the anomaly. You can also switch
to viewing this time series in the **Single Metric Viewer**.

Below the charts, there is a table that provides more information, such as the
typical and actual values and the influencers that contributed to the anomaly.
For example:

[role="screenshot"]
image::images/ml-gs-job2-explorer-table.jpg["Anomaly tables for the response_code_rates job"]

If your job has multiple detectors, the table aggregates the anomalies to show
the highest severity anomaly per detector and entity, which is the field value
that is displayed in the **found for** column. To view all the anomalies without
any aggregation, set the **Interval** to `Show all`.

TIP: The anomaly scores that you see in each section of the **Anomaly Explorer**
might differ slightly. This disparity occurs because for each job there are
bucket results, influencer results, and record results. Anomaly scores are
generated for each type of result. The anomaly timeline uses the bucket-level
anomaly scores. The list of top influencers uses the influencer-level anomaly
scores. The list of anomalies uses the record-level anomaly scores.

[discrete]
[[ml-gs-results-population]]
=== Population job results

If you examine the results from the `url_scanning` {anomaly-job} in the
**Anomaly Explorer**, you'll notice its charts have a different format. For
example:

[role="screenshot"]
image::images/ml-gs-job3-explorer.jpg["Anomaly charts for the url_scanning job"]

_Population {anomaly-jobs}_ detect activity that is unusual compared to the
behavior of a specified population, which is defined by the _over field_. In
this case, the metrics for each client IP are analyzed relative to other client
IPs in each bucket and the `30.156.16.164` client IP is behaving abnormally.

If you add the sample eCommerce orders data set, it contains another example of
a population job. The metrics for each customer are analyzed relative to other
customers in each bucket. In this example, `Abd Burton` made anomalous purchases:

[role="screenshot"]
image::images/ml-gs-job4-explorer.jpg["Anomaly charts for the high_sum_total_sales job"]

For more information, see <<ml-configuring-pop>>.