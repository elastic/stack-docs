[[ml-getting-started]]
= Getting started with {ml}
:keywords: {ml-init}, {stack}, {anomaly-detect}, tutorial
:description: This tutorial shows you how to create {anomaly-jobs}, \
interpret the results, and forecast future behavior in {kib}. 

{ml-cap} features analyze your data and generate models for its patterns of
behavior. The type of analysis that you choose depends on the questions or
problems you want to address and the type of data you have available.

[discrete]
[[get-started-unsupervised]]
== Unsupervised {ml}

There are two types of analysis that can deduce the patterns and relationships
within your data without training or intervention: _{anomaly-detect}_ and
_{oldetection}_.

<<ml-ad-overview,{anomaly-detect-cap}>> requires time series data. It constructs a
probability model and can run continuously to identify unusual events as they
occur. The model evolves over time; you can use its insights to forecast future
behavior.

<<ml-dfa-finding-outliers,{oldetection-cap}>> does not require time series data; 
it identifies unusual points in a data set by analyzing how close each data 
point is to others and the density of the cluster of points around it. It does 
not run continuously; it generates a copy of your data set where each data point 
is annotated with an {olscore}. The score indicates the extent to which a data 
point is an outlier compared to other data points.

[discrete]
[[get-started-supervised]]
== Supervised {ml}

There are two types of analysis that require training data sets:
_{classification}_ and _{regression}_.

In both cases, the result is a copy of your data set where each data point is
annotated with predictions and a trained model, which you can deploy to make
predictions for new data. For more information, refer to
<<ml-supervised-workflow>>.

<<ml-dfa-classification,{classification-cap}>> learns relationships between your
data points in order to predict discrete categorical values, such as whether a
DNS request originates from a malicious or benign domain.

<<ml-dfa-regression,{regression-cap}>> learns relationships between your data
points in order to predict continuous numerical values, such as the response
time for a web request.

[discrete]
[[get-started-prereqs]]
== Try it out

Ready to take {ml} for a test drive? Follow this tutorial to:

* Try out the **{data-viz}**
* Create {anomaly-jobs} for the {kib} sample data
* Use the results to identify possible anomalies in the data

At the end of this tutorial, you should have a good idea of what {ml} is and
will hopefully be inspired to use it to detect anomalies in your own data.

The following video provides a quick overview of some of the tasks we'll perform in this tutorial:

++++
<script type="text/javascript" async src="https://play.vidyard.com/embed/v4.js"></script>
<img
  style="width: 100%; margin: auto; display: block;"
  class="vidyard-player-embed"
  src="https://play.vidyard.com/eVQoHfHNgGxBeuAZ5rCtXq.jpg"
  data-uuid="qAiTxhZSXKQVQF5aFMp1s3"
  data-v="4"
  data-type="inline"
/>
</br>
++++

Need more context? Check out the
{ref}/elasticsearch-intro.html[{es} introduction] to learn the lingo and
understand the basics of how {es} works.

. Before you can play with the {ml-features}, you must install {es} and {kib}.
{es} stores the data and the analysis results. {kib} provides a helpful user 
interface for creating and viewing jobs.
+
--
[TIP]
==========
You can run {es} and {kib} on your own hardware, or use our
https://www.elastic.co/cloud/elasticsearch-service[hosted {ess}] on {ecloud}.
The {ess} is available on both AWS and GCP.
https://www.elastic.co/cloud/elasticsearch-service/signup[Try out the {ess} for free].
==========
--

. Verify that your environment is set up properly to use the {ml-features}. If
the {es} {security-features} are enabled, to complete this tutorial you need a
user that has authority to manage {anomaly-jobs}. See <<setup>>.

. {kibana-ref}/get-started.html#gs-get-data-into-kibana[Add the sample data sets that ship with {kib}]. 

.. From the {kib} home page, click *Add data*, then select *Sample data*.

.. Pick a data set. In this tutorial, you'll use the *Sample web logs*. While
you're here, feel free to click *Add data* on all of the available sample data sets.

These data sets are now ready be analyzed in {ml} jobs in {kib}.
