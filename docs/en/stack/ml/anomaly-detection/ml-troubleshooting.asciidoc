[role="xpack"]
[[ml-troubleshooting]]
= Troubleshooting {anomaly-detect}
++++
<titleabbrev>Troubleshooting</titleabbrev>
++++

Use the information in this section to troubleshoot common problems and known
issues.

[discrete]
[[ml-avoid-upgrade-closures]]
== Unintended {anomaly-job} closures on upgrade

When you perform a {ref}/rolling-upgrades.html[rolling upgrade] to _or_ from 
versions 7.14.0 or 7.14.1 you may find that {anomaly-jobs} that were `opened`
during the upgrade incorrectly end up `closed` after the upgrade.

*Symptoms:*

* Some (but not necessarily all) {anomaly-jobs} that were in the `opened` state
before the upgrade are `closed` after the upgrade. The response from the
{ref}/ml-get-job.html[get {anomaly-jobs} API] for these jobs contains a
`blocked` property with `revert` as its reason.
* The {dfeed} associated with a `closed` {anomaly-job} is in the `started` state;
this combination should be impossible.

*Resolution:*

To avoid this problem, enable {ml} upgrade mode before you start the rolling
upgrade and disable it after the rolling upgrade is complete. Do not enable and
disable {ml} upgrade mode more than once; enable it before upgrading the first
node of the rolling upgrade and disable it after upgrading the last node. It is
only safe to enable {ml} upgrade mode again after all {anomaly-jobs} that were
`opened` have been assigned to nodes and fully recovered; this may take 30
minutes in large environments.

To remediate the problem if you experience it:

1. Force-stop the `started` {dfeed} associated with the `closed` {anomaly-job}
   by calling the {ref}/ml-stop-datafeed.html[stop {dfeeds} API] with `force`
   set to `true`.
2. Complete the `revert` operation that the {anomaly-job} is blocked on by
   calling the {ref}/ml-revert-snapshot.html[revert model snapshots API] with
   `delete_intervening_results` set to `true`. To find the appropriate model
   snapshot to revert to, look in the "Job Messages" tab for the {anomaly-job}
   in {kib}, for the model snapshot reversion that started during your rolling
   upgrade.
3. Open the incorrectly `closed` {anomaly-job}.
4. Start the associated {dfeed}.

Steps 3 and 4 can be done by clicking the start button for the job in {kib}.

[discrete]
[[ml-troubleshooting-mappings]]
== Incorrect mappings in 7.9.0 or higher

This problem occurs when you upgrade to 7.9.0 and incorrect mappings are
added to the {ml} annotations index or the {ml} config index.

It is also possible for this problem to occur for the {ml} config index when
you upgrade to 7.9.1 or higher after previously upgrading to several prior 7.x
versions. If you skip version 7.9.0 and upgrade directly to version 7.9.1 or
higher then the mappings on the {ml} annotations index will be correct.
However, if you upgraded to version 7.9.0 and suffered incorrect mappings then
upgrading to 7.9.1 will not fix these; you will need to follow the procedure
detailed below.

*Symptoms:*

* Some pages in the {ml-app} UI do not display correctly. For example, the
*Anomaly Explorer* fails to load.
* The following error occurs in {kib} when you try to view annotations for
{anomaly-jobs}: `Error loading the list of annotations for this job`
* Cannot create or update any {ml} jobs. The error messages in this case are
illegal argument exceptions like `mapper [model_plot_config.annotations_enabled]
cannot be changed from type [keyword] to [boolean]`. This problem is most likely
to occur if after upgrading you open an existing {anomaly-job} in 7.9.0 before
you create or update a job. 

*Resolution:*

To avoid this problem, manually update the mappings on the {ml} annotations and
config indices in your old {es} version before you upgrade to 7.9.0. For example:

[source,console]
--------------------------------------------------
PUT .ml-annotations-6/_mapping
{
  "properties": {
    "event" : {
      "type" : "keyword"
    },
    "detector_index" : {
      "type" : "integer"
    },
    "partition_field_name" : {
      "type" : "keyword"
    },
    "partition_field_value" : {
      "type" : "keyword"
    },
    "over_field_name" : {
      "type" : "keyword"
    },
    "over_field_value" : {
      "type" : "keyword"
    },
    "by_field_name" : {
      "type" : "keyword"
    },
    "by_field_value" : {
      "type" : "keyword"
    }
  }
}

PUT .ml-config/_mapping
{
  "properties": {
    "analysis_config": {
      "properties": {
        "per_partition_categorization" : {
          "properties" : {
            "enabled" : {
              "type" : "boolean"
            },
            "stop_on_warn" : {
              "type" : "boolean"
            }
          }
        }
      }
    },
    "max_num_threads" : {
      "type" : "integer"
    },
    "model_plot_config" : {
      "properties" : {
        "annotations_enabled" : {
          "type" : "boolean"
        }
      }
    }
  }
}
--------------------------------------------------
// TEST[skip:TBD]

NOTE: If {security-features} are enabled, you must have the
{ref}/built-in-roles.html[`superuser` role] to alter the `.ml-config` index.

If you did not manually update the mappings before the upgrade, you can
nonetheless try to do it after the upgrade. If either update fails, you must
reindex that index. For example, follow these steps:

. To reindex the {ml} annotations index:
.. Enable upgrade mode:
+
--
[source,console]
--------------------------------------------------
POST _ml/set_upgrade_mode?enabled=true&timeout=10m
--------------------------------------------------
// TEST[skip:TBD]
--
.. Create a temporary index:
+
--
[source,console]
--------------------------------------------------
PUT temp_ml_annotations
--------------------------------------------------
// TEST[skip:TBD]
--
.. Reindex the `.ml-annotations-6` index into the temporary index:
+
--
[source,console]
--------------------------------------------------
POST _reindex
{
  "source": { "index": ".ml-annotations-6" }, 
  "dest": { "index": "temp_ml_annotations" }
}
--------------------------------------------------
// TEST[skip:TBD]
--
.. Delete the .ml-annotations-6 index:
+
--
[source,console]
--------------------------------------------------
DELETE .ml-annotations-6
--------------------------------------------------
// TEST[skip:TBD]
--
.. Disable upgrade mode:
+
--
[source,console]
--------------------------------------------------
POST _ml/set_upgrade_mode?enabled=false&timeout=10m
--------------------------------------------------
// TEST[skip:TBD]
--
.. Wait for .ml-annotations-6 to be recreated.
.. Reindex the temporary index into the .ml-annotations-6 index:
+
--
[source,console]
--------------------------------------------------
POST _reindex
{
  "source": { "index": "temp_ml_annotations" }, 
  "dest": { "index": ".ml-annotations-6" }
}
--------------------------------------------------
// TEST[skip:TBD]
--
.. Delete the temporary index:
+
--
[source,console]
--------------------------------------------------
DELETE temp_ml_annotations
--------------------------------------------------
// TEST[skip:TBD]
--

. To reindex the {ml} config index, follow these steps:
.. Enable upgrade mode:
+
--
[source,console]
--------------------------------------------------
POST _ml/set_upgrade_mode?enabled=true&timeout=10m
--------------------------------------------------
// TEST[skip:TBD]
--
.. Create a temporary index:
+
--
[source,console]
--------------------------------------------------
PUT temp_ml_config
--------------------------------------------------
// TEST[skip:TBD]
--
.. Reindex the .ml-config index into the temporary index:
+
--
[source,console]
--------------------------------------------------
POST _reindex
{
  "source": { "index": ".ml-config" }, 
  "dest": { "index": "temp_ml_config" }
}
--------------------------------------------------
// TEST[skip:TBD]
--
.. Delete the .ml-config index:
+
--
[source,console]
--------------------------------------------------
DELETE .ml-config
--------------------------------------------------
// TEST[skip:TBD]
--
.. Create the .ml-config index:
+
--
[source,console]
--------------------------------------------------
PUT .ml-config
{
  "settings": { "auto_expand_replicas": "0-1"}
}
--------------------------------------------------
// TEST[skip:TBD]
--
.. Reindex the temporary index into the .ml-config index:
+
--
[source,console]
--------------------------------------------------
POST _reindex
{
  "source": { "index": "temp_ml_config" }, 
  "dest": { "index": ".ml-config" }
}
--------------------------------------------------
// TEST[skip:TBD]
--
.. Disable upgrade mode:
+
--
[source,console]
--------------------------------------------------
POST _ml/set_upgrade_mode?enabled=false&timeout=10m
--------------------------------------------------
// TEST[skip:TBD]
--
.. Delete the temporary index:
+
--
[source,console]
--------------------------------------------------
DELETE temp_ml_config
--------------------------------------------------
// TEST[skip:TBD]
--

[discrete]
[[ml-debian8-memory]]
== Suboptimal job assignment on Debian 8

Where possible, {ml} jobs are assigned to nodes based on the memory requirement
of the job and the memory available on the node. However, in certain cases, the
amount of memory on a node cannot be accurately determined and jobs are assigned
by balancing the number of jobs per {ml} node. It may lead to a situation where
all the jobs with high memory requirements are on one node and the less
memory-intensive jobs on another.

One particular case of this problem is that {es} fails to determine the amount
of memory on a machine that is running Debian 8 with the default Cgroups setup
and certain updates of Java versions earlier than Java 15. For example, Java
8u271 is known to be affected while Java 8u272 is not. Java 15 was fixed from
its initial release.

If you are running {es} on Debian 8 with an old version of Java and have not
already modified the Cgroups setup then it is recommended to do one of the
following:

* Upgrade Java to version 15.
* Upgrade to the latest Java update for the version of Java you are running.
* Enable the "memory" Cgroup by editing `/etc/default/grub` and adding:
+
--
[source,sh]
--------------------------------------------------
GRUB_CMDLINE_LINUX_DEFAULT="quiet cgroup_enable=memory swapaccount=1"
--------------------------------------------------

Update your GRUB configuration by running `sudo update-grub`, then reboot the
machine.

--
