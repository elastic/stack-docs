[[ml-nlp-model-ref]]
= Third party NLP models

The {stack-ml-features} support transformer models that conform to the standard
BERT model interface and use the WordPiece tokenization algorithm.

The current list of supported architectures is:

* BERT
* DPR bi-encoders
* DistilBERT
* ELECTRA
* MobileBERT
* RetriBERT
* MPNet
* SentenceTransformers bi-encoders with the above transformer architectures

In general, any trained model that has a supported architecture is deployable in
{es} by using eland. However, it is not possible to test every third party
model. The following lists are therefore provided for informational purposes
only and may not be current. Elastic makes no warranty or assurance that the
{ml-features} will continue to interoperate with these third party models in the
way described, or at all.

These models are listed by NLP task; for more information about those tasks,
refer to <<ml-nlp-overview>>.

[discrete]
[[ml-nlp-model-ref-mask]]
== Third party fill-mask models

* https://huggingface.co/bert-base-uncased
* https://huggingface.co/microsoft/mpnet-base

[discrete]
[[ml-nlp-model-ref-ner]]
== Third party named entity recognition models

* https://huggingface.co/dslim/bert-base-NER
* https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english
* https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english

[discrete]
[[ml-nlp-model-ref-text-embedding]]
== Third party text embedding models

Text Embedding models are designed to work with specific scoring functions
for calculating the similarity between the embeddings they produce. 
Examples of typical scoring functions are: `cosine`, `dot product` and 
`euclidean distance` (also known as `l2_norm`).

The embeddings produced by these models should be indexed in {es} using the
{ref}/dense-vector.html[dense vector field type]
with an appropriate {ref}/dense-vector.html#dense-vector-params[similarity function]
chosen for the model. 

To find similar embeddings in {es} use the efficient 
{ref}/knn-search.html#approximate-knn[Approximate k-nearest neighbor (kNN)]
search API with a text embedding as the query vector. Approximate 
kNN search uses the similarity function defined in 
the dense vector field mapping is used to calculate the relevance.
For the best results the function must be one of 
the suitable similarity functions for the model.


Using `SentenceTransformerWrapper`:

<<<<<<< HEAD
* https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2
* https://huggingface.co/sentence-transformers/LaBSE
* https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b 
* https://huggingface.co/sentence-transformers/msmarco-MiniLM-L-12-v3
* https://huggingface.co/sentence-transformers/nli-bert-base-cls-pooling
* https://huggingface.co/sentence-transformers/bert-base-nli-cls-token
* https://huggingface.co/sentence-transformers/facebook-dpr-ctx_encoder-multiset-base
* https://huggingface.co/sentence-transformers/facebook-dpr-question_encoder-single-nq-base
* https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2
=======
* https://huggingface.co/sentence-transformers/all-distilroberta-v1[All DistilRoBERTa v1]
Suitable similarity functions:	`dot_product`, `cosine`, `l2_norm`
* https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2[All MiniLM L12 v2]
Suitable similarity functions:	`dot_product`, `cosine`, `l2_norm`
* https://huggingface.co/sentence-transformers/all-mpnet-base-v2[All MPNet base v2]
Suitable similarity functions:	`dot_product`, `cosine`, `l2_norm`
* https://huggingface.co/sentence-transformers/facebook-dpr-ctx_encoder-multiset-base[Facebook dpr-ctx_encoder multiset base]
Suitable similarity functions:	`dot_product`
* https://huggingface.co/sentence-transformers/facebook-dpr-question_encoder-single-nq-base[Facebook dpr-question_encoder single nq base]
Suitable similarity functions:	`dot_product`
* https://huggingface.co/sentence-transformers/LaBSE[LaBSE]
Suitable similarity functions:	`cosine`
* https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b[msmarco DistilBERT base tas b]
Suitable similarity functions:	`dot_product`
* https://huggingface.co/sentence-transformers/msmarco-MiniLM-L12-cos-v5[msmarco MiniLM L12 v5]
Suitable similarity functions:	`dot_product`, `cosine`, `l2_norm`
* https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2[paraphrase mpnet base v2]
Suitable similarity functions:	`cosine`
>>>>>>> 72bd1e3 ([ML] Add suitable scoring functions for Text embedding models (#2183))

Using `DPREncoderWrapper`:

* https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base
* https://huggingface.co/facebook/dpr-question_encoder-single-nq-base
* https://huggingface.co/facebook/dpr-ctx_encoder-multiset-base
* https://huggingface.co/facebook/dpr-question_encoder-multiset-base
* https://huggingface.co/castorini/ance-dpr-context-multi
* https://huggingface.co/castorini/ance-dpr-question-multi
* https://huggingface.co/castorini/bpr-nq-ctx-encoder
* https://huggingface.co/castorini/bpr-nq-question-encoder

[discrete]
[[ml-nlp-model-ref-text-classification]]
=== Third party text classification models

* https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english
* https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion
* https://huggingface.co/Hate-speech-CNERG/dehatebert-mono-english
* https://huggingface.co/ProsusAI/finbert
* https://huggingface.co/nateraw/bert-base-uncased-emotion

[discrete]
[[ml-nlp-model-ref-zero-shot]]
== Third party zero-shot text classification models

* https://huggingface.co/typeform/distilbert-base-uncased-mnli
* https://huggingface.co/typeform/mobilebert-uncased-mnli
* https://huggingface.co/typeform/squeezebert-mnli