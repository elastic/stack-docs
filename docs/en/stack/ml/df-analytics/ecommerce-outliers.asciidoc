[role="xpack"]
[testenv="platinum"]
[[ecommerce-outliers]]
=== Finding outliers in the eCommerce sample data

beta[]

The goal of <<dfa-outlier-detection,{oldetection}>> is to find the most unusual
documents in an index. Let's try to detect unusual customer behavior in the 
{kibana-ref}/add-sample-data.html[eCommerce sample data set]. 

. Verify that your environment is set up properly to use {ml-features}. 
If the {es} {security-features} are enabled, you need a user that has authority
to create and manage {dfanalytics-jobs}. See <<setup>>.
+
--
Since we'll be creating {transforms}, you also need
`manage_data_frame_transforms` cluster privileges.
--

. Create a {transform} that generates an entity-centric index with numeric or
boolean data to analyze.
+
--
In this example, we'll use the eCommerce orders sample data and pivot the data
such that we get a new index that contains a sales summary for each customer.

In particular, create a {transform} that calculates the sum of the products
(`products.quantity`) and the sum of prices (`products.taxful_price`) in all of
the orders, grouped by customer (`customer_full_name`). Also include a value
count aggregation, so that we know how many orders (`order_id`) exist for each
customer.

You can preview the {transform} before you create it in {kib}:

[role="screenshot"]
image::images/ecommerce-transform-preview.jpg["Creating a {transform} in {kib}"]

Alternatively, you can use the
{ref}/preview-transform.html[preview {transform} API] and the
{ref}/put-transform.html[create {transform} API].

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _data_frame/transforms/_preview
{
  "source": {
    "index": [
      "kibana_sample_data_ecommerce"
    ]
  },
  "pivot": {
    "group_by": {
      "customer_full_name.keyword": {
        "terms": {
          "field": "customer_full_name.keyword"
        }
      }
    },
    "aggregations": {
      "products.quantity.sum": {
        "sum": {
          "field": "products.quantity"
        }
      },
      "products.taxful_price.sum": {
        "sum": {
          "field": "products.taxful_price"
        }
      },
      "order_id.value_count": {
        "value_count": {
          "field": "order_id"
        }
      }
    }
  }
}

PUT _data_frame/transforms/ecommerce-customer-sales
{
  "source": {
    "index": [
      "kibana_sample_data_ecommerce"
    ]
  },
  "pivot": {
    "group_by": {
      "customer_full_name.keyword": {
        "terms": {
          "field": "customer_full_name.keyword"
        }
      }
    },
    "aggregations": {
      "products.quantity.sum": {
        "sum": {
          "field": "products.quantity"
        }
      },
      "products.taxful_price.sum": {
        "sum": {
          "field": "products.taxful_price"
        }
      },
      "order_id.value_count": {
        "value_count": {
          "field": "order_id"
        }
      }
    }
  },
  "description": "E-commerce sales by customer",
  "dest": {
    "index": "ecommerce-customer-sales"
  }
}
--------------------------------------------------
// TEST[skip:set up sample data]
====

For more details about creating {transforms}, see
{ref}/ecommerce-transforms.html[Transforming the eCommerce sample data].
--

. Start the {transform}.
+
--

TIP: Even though resource utilization is automatically adjusted based on the
cluster load, a {transform} increases search and indexing load on your
cluster while it runs. If you're experiencing an excessive load, however, you
can stop it.

You can start, stop, and manage {transforms} in {kib}. Alternatively, you can
use the {ref}/start-data-frame-transform.html[start {transforms}] API.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _data_frame/transforms/ecommerce-customer-sales/_start
--------------------------------------------------
// TEST[skip:setup kibana sample data]
====
--

. Create a {dfanalytics-job} to detect outliers in the new entity-centric index.
+
--
There is a wizard for creating {dfanalytics-jobs} on the
*Machine Learning* > *Data Frame Analytics* page in {kib}:

[role="screenshot"]
image::images/ecommerce-outlier-job.jpg["Create a {dfanalytics-job} in {kib}"]

Alternatively, you can use the
{ref}/put-dfanalytics.html[create {dfanalytics-jobs} API].

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
PUT _ml/data_frame/analytics/ecommerce
{
  "source": {
    "index": "ecommerce-customer-sales"
  },
  "dest": {
    "index": "ecommerce-outliers"
  },
  "analysis": {
    "outlier_detection": {
    }
  },
  "analyzed_fields" : {
    "includes" : ["products.quantity.sum","products.taxful_price.sum","order_id.value_count"]
  }
}
--------------------------------------------------
// TEST[skip:setup kibana sample data]
====
--

. Start the {dfanalytics-job}.
+
--
You can start, stop, and manage {dfanalytics-jobs} on the
*Machine Learning* > *Data Frame Analytics* page in {kib}. Alternatively, you
can use the {ref}/start-dfanalytics.html[start {dfanalytics-jobs}] and
{ref}/stop-dfanalytics.html[stop {dfanalytics-jobs}] APIs.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _ml/data_frame/analytics/ecommerce/_start
--------------------------------------------------
// TEST[skip:setup kibana sample data]
====
--

. View the results of the {oldetection} analysis.
+
--
The {dfanalytics-job} creates an index that contains the original data and
{olscores} for each document. The {olscore} indicates how different each entity
is from other entities.

In {kib}, you can view the results from the {dfanalytics-job} and sort them
on the outlier score:

[role="screenshot"]
image::images/outliers.jpg["View {oldetection} results in {kib}"]

The `ml.outlier` score is a value between 0 and 1. The larger the value, the
more likely they are to be an outlier.

In addition to an overall outlier score, each document is annotated with feature
influence values for each field. These values add up to 1 and indicate which
fields are the most important in deciding whether an entity is an outlier or
inlier. For example, the dark shading on the `products.taxful_price.sum` field
for Wagdi Shaw indicates that the sum of the product prices was the most
influential feature in determining that Wagdi is an outlier.

If you want to see the exact feature influence values, you can retrieve them
from the index that is associated with your {dfanalytics-job}.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
GET ecommerce-outliers/_search?q="Wagdi Shaw"
--------------------------------------------------
// TEST[skip:setup kibana sample data]

The search results include the following {oldetection} scores:

[source,js]
--------------------------------------------------
...
"ml" :{
  "outlier_score" : 0.9653657078742981,
  "feature_influence.products.quantity.sum" : 0.00592468399554491,
  "feature_influence.order_id.value_count" : 0.01975759118795395,
  "feature_influence.products.taxful_price.sum" : 0.974317729473114
}
...
--------------------------------------------------
// NOTCONSOLE
====
--

Now that you've found unusual behavior in the sample data set, consider how you
might apply these steps to other data sets. If you have data that is already
marked up with true outliers, you can determine how well the {oldetection}
algorithms perform by using the evaluate {dfanalytics} API. See
<<ml-dfanalytics-evaluate>>.

TIP: If you do not want to keep the {transform} and the {dfanalytics-job}, you
can delete them in {kib} or use the
{ref}/delete-data-frame-transform.html[delete {transform} API] and
{ref}/delete-dfanalytics.html[delete {dfanalytics-job} API]. When
you delete {transforms} and {dfanalytics-jobs}, the destination indices and
{kib} index patterns remain.

If you want to see another example of {oldetection} in a Jupyter notebook,
https://github.com/elastic/examples/tree/master/Machine%20Learning/Outlier%20Detection/Introduction[click here].
