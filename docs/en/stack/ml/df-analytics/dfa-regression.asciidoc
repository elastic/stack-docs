[role="xpack"]
[[dfa-regression]]
=== {regression-cap}

experimental[]

{reganalysis-cap} is a {ml} process for estimating the relationships among 
different fields in your data, then making further predictions based on these 
relationships.

For example, suppose we are interested in finding the relationship between 
apartment size and monthly rent in a city. Our imaginary data set consists of 
three data points:

|===
| Size (m2) | Monthly rent 
| 44        | 1600
| 24        | 1055
| 63        | 2300
|===

After the model determines the relationship between the apartment size and the
rent, it can make predictions such as the monthly rent of a hundred square
meter-size apartment.

This is a simple example. Usually {regression} problems are multi-dimensional, 
so the relationships that {reganalysis} tries to find are between multiple 
fields. To extend our example, a more complex {reganalysis} could take into
account additional factors such as the location of the apartment in the city, on
which floor it is, and whether the apartment has a riverside view or not, and so
on. All of these factors can be considered _features_; they are measurable
properties or characteristics of the phenomenon we're studying.


[discrete]
[[dfa-regression-features]]
==== {feature-vars-cap}

When you perform {reganalysis}, you must identify a subset of fields that you 
want to use to create a model for predicting other fields. We refer to these 
fields as _feature variables_ and _dependent variables_, respectively.
{feature-vars-cap} are the values that the {depvar} value depends on. If one or 
more of the {feature-vars} changes, the {depvar} value also changes. There are 
three different types of {feature-vars} that you can use with our {regression} 
algorithm:

* Numerical. In our example, the size of the apartment was a 
  numerical {feature-var}.
* Categorical. A variable that can have one value from a set of values. The 
  value set has a fixed and limited number of possible items. In the example, 
  the location of the apartment in the city (borough) is a categorical variable.
* Boolean. The riverside view in the example is a boolean value because an 
  apartment either has a riverside view or doesn't have one.
Arrays are not supported.


[discrete]
[[dfa-regression-supervised]]
==== Training the {regression} model

{regression-cap} is a supervised {ml} method, which means that you need to 
supply a labeled training data set that has some {feature-vars} and a {depvar}. 
The {regression} algorithm identifies the relationships between the
{feature-vars} and the {depvar}. Once you've trained the model on your training
data set, you can reuse the knowledge that the model has learned to make
inferences about new data.

The relationships between the {feature-vars} and the {depvar} are described as a 
mathematical function. {reganalysis-cap} tries to find the best prediction for 
the {depvar} by combining the predictions from multiple base learners â€“ 
algorithms that generalize from the data set. The performance of an ensemble is 
usually better than the performance of each individual base learner because the 
individual learners will make different errors. These average out when their 
predictions are combined.

{regression-cap} works as a batch analysis. If new data comes into your index, 
you must restart the {dfanalytics-job}.


[discrete]
[[dfa-regression-algorithm]]
===== {regression-cap} algorithms

//tag::regression-algorithms[]
The ensemble learning technique that we use in the {stack} is a type of boosting 
called extreme gradient boost (XGboost) which combines decision trees with 
gradient boosting methodologies.
//end::regression-algorithms[]


[discrete]
[[dfa-regression-feature-importance]]
==== Feature importance

{feat-imp-cap} provides further information about the results of an analysis and 
helps to interpret the results in a more subtle way. If you want to learn more 
about {feat-imp}, <<ml-feature-importance,click here>>.


[discrete]
[[dfa-regression-evaluation]]
==== Measuring model performance

You can measure how well the model has performed on your training data set by 
using the `regression` evaluation type of the 
{ref}/evaluate-dfanalytics.html[evaluate {dfanalytics} API]. The mean squared 
error (MSE) value that the evaluation provides you on the training data set is 
the _training error_. Training the {regression} model means finding the 
combination of model parameters that produces the lowest possible training 
error.

Another crucial measurement is how well your model performs on unseen 
data points. To assess how well the trained model will perform on data it has 
never seen before, you must set aside a proportion of the training data set for 
testing. This split of the data set is the testing data set. Once the model has 
been trained, you can let the model 
predict the value of the data points it has never seen before and compare the 
prediction to the actual value. This test provides an estimate of a quantity 
known as the _model generalization error_.

Two concepts describe how well the {regression} algorithm was able to learn the 
relationship between the {feature-vars} and the {depvar}. _Underfitting_ is when 
the model cannot capture the complexity of the data set. _Overfitting_ is when 
the model is too specific to the training data set and is capturing details 
which do not generalize to new data. A model that overfits the data has a 
low MSE value on the training data set and a high MSE value on the testing 
data set. For more information about the evaluation metrics, see 
<<ml-dfanalytics-regression-evaluation>>.

==== Further readings

https://github.com/elastic/examples/tree/master/Machine%20Learning/Feature%20Importance[Feature importance for {dfanalytics} (Jupyter notebook)]
