[[ml-biases]]
=== Considering biases in {ml} processes

{ml-cap} is meant to improve our everyday life in a wide palette of activities 
across the globe. We use {ml} in science, healthcare, business, entertainment, 
and many other fields. We make decisions based on {ml} results. It is therefore 
crucial to keep in mind the possible biases that can corrupt results, endanger 
fairness, and jeopardize the success of even the best-intended initiative. Some 
of these social biases can make their way into the {ml} results more easily than 
we might think.

There are various possible biases that can negatively influence the result of a 
{ml} analysis. In this piece, we'll talk about statistical and social biases. 
They can exist in the datasets used to train {ml} models or in the way the 
predictions of these models are interpreted. These issues are part of a greater 
question about the ethics behind technology and there's no one solution for 
everyone. However, there are some basic practices you can use to reduce these 
biases.


[float]
==== Biases in datasets

Biases exist in the data that we use to train {ml} models. For example, our 
choice of which data to collect or analyze reflect our ideas about what is 
important or worthy of attention. But the bias could be also the result of 
manual labeling when the dataset has been annotated with the "ground truth", 
which is another opportunity to unwittingly introduce preconceptions or 
prejudice. The model will inherently learn the bias, therefore the analysis will 
be also compromised, while technically the algorithm works exactly as intended. 

It is impossible to avoid all the potential biases in the dataset. Eventually, 
data is a representation of reality and, like any other type of representation, 
it never perfectly captures all possible variables affecting a given phenomenon. 
However, you can and should carefully scrutinize your dataset, ensure it's not 
selected in a predetermined way and it matches the real world as close as 
possible. Involve people with diverse backgrounds in the processes that require 
humans, it is also a solution to minimize the chance of biases.


[float]
==== Biases in interpretation

When we interpret data, we can also be biased. For example, when we might want 
to validate a hypothesis based on the results of an analysis. In these cases, 
the outcome of the analysis or the decision that is based on the analysis can be 
inaccurate even if the dataset itself is carefully designed.

It is safer to formulate the aim of the analysis in a way that does not contain 
the expected results, interpret the outcome of the analysis multiple times 
with different persons, and perform tests as much as possible.

Avoid "blind interpretation"; {ml} algorithms shouldn't be applied without a 
thorough understand on the data and its shortcomings arrived at through an 
analysis.


[float]
==== Collective responsibility

The issue of biases that can affect {ml} related processes is far from solved. 
In fact, this is a field of ongoing disputes and there are no universal answers 
for the emerging problems. Because of the various factors that can influence the 
{ml} processes, avoiding bias-influenced analysis is a collective responsibility 
of all the persons that participate in these processes; from the designers of 
the dataset to the people who interpret the outcome of the analysis to the ones 
who make decisions based on the data and the companies that offer the platforms 
to make these decisions. Talk to your team about whether or not {ml} solutions 
are appropriate for your specific problem, then talk about the real possibility 
of introducing and reinforcing biases at each step in the process. Make plans, 
share techniques, and be open to reconsidering results that seem too good to be 
true.
