[role="xpack"]
[testenv="platinum"]
[[ml-dfa-regression]]
= Predicting numerical values with {regression}

:keywords: {ml-init}, {stack}, {dfanalytics}, {regression}
:description: An introduction to {ml} {regression}, which enables you to  \
predict numerical values in a data set.

This page contains a conceptual overview of {classification} in the {stack} and 
a simple <<performing-regression,example>> of how to use it.

[discrete]
[[dfa-regression]]
== {regression-cap}


{reganalysis-cap} is a {ml} process for estimating the relationships among 
different fields in your data, then making further predictions based on these 
relationships.

For example, suppose we are interested in finding the relationship between 
apartment size and monthly rent in a city. Our imaginary data set consists of 
three data points:

|===
| Size (m2) | Monthly rent 
| 44        | 1600
| 24        | 1055
| 63        | 2300
|===

After the model determines the relationship between the apartment size and the
rent, it can make predictions such as the monthly rent of a hundred square
meter-size apartment.

This is a simple example. Usually {regression} problems are multi-dimensional, 
so the relationships that {reganalysis} tries to find are between multiple 
fields. To extend our example, a more complex {reganalysis} could take into
account additional factors such as the location of the apartment in the city, on
which floor it is, and whether the apartment has a riverside view or not, and so
on. All of these factors can be considered _features_; they are measurable
properties or characteristics of the phenomenon we're studying.

[discrete]
[[dfa-regression-features]]
=== {feature-vars-cap}

When you perform {reganalysis}, you must identify a subset of fields that you 
want to use to create a model for predicting other fields. We refer to these 
fields as _feature variables_ and _dependent variables_, respectively.
{feature-vars-cap} are the values that the {depvar} value depends on. If one or 
more of the {feature-vars} changes, the {depvar} value also changes. There are 
three different types of {feature-vars} that you can use with our {regression} 
algorithm:

* Numerical. In our example, the size of the apartment was a 
  numerical {feature-var}.
* Categorical. A variable that can have one value from a set of values. The 
  value set has a fixed and limited number of possible items. In the example, 
  the location of the apartment in the city (borough) is a categorical variable.
* Boolean. The riverside view in the example is a boolean value because an 
  apartment either has a riverside view or doesn't have one.
Arrays are not supported.

[discrete]
[[dfa-regression-supervised]]
=== Training the {regression} model

{regression-cap} is a supervised {ml} method, which means that you need to 
supply a labeled training data set that has some {feature-vars} and a {depvar}. 
The {regression} algorithm identifies the relationships between the
{feature-vars} and the {depvar}. Once you've trained the model on your training
data set, you can reuse the knowledge that the model has learned to make
inferences about new data.

The relationships between the {feature-vars} and the {depvar} are described as a 
mathematical function. {reganalysis-cap} tries to find the best prediction for 
the {depvar} by combining the predictions from multiple base learners – 
algorithms that generalize from the data set. The performance of an ensemble is 
usually better than the performance of each individual base learner because the 
individual learners will make different errors. These average out when their 
predictions are combined.

{regression-cap} works as a batch analysis. If new data comes into your index, 
you must restart the {dfanalytics-job}.

[discrete]
[[dfa-regression-algorithm]]
==== {regression-cap} algorithms

//tag::regression-algorithms[]
The ensemble learning technique that we use in the {stack} is a type of boosting 
called extreme gradient boost (XGboost) which combines decision trees with 
gradient boosting methodologies.
//end::regression-algorithms[]

[discrete]
[[dfa-regression-lossfunction]]
==== Loss functions for {regression} analyses

A loss function measures how well a given {ml} model fits the specific data set. 
It boils down all the different under- and overestimations of the model to a 
single number, known as the prediction error. The bigger the difference between 
the prediction and the ground truth, the higher the value of the loss function. 
Loss functions are used automatically in the background during 
<<hyperparameters,hyperparameter optimization>> and when training the decision 
trees to compare the performance of various iterations of the model.

In the {stack}, there are three different types of loss function:

* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]: 
It is the default choice when no additional information about the data set is 
available.
* mean squared logarithmic error (`msle`; a variation of `mse`): It is for 
cases where the target values are all positive with a long tail distribution 
(for example, prices or population).
* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]:
Use it when you want to prevent the model trying to fit the outliers instead of 
regular data.

The various types of loss function calculate the prediction error differently. 
The appropriate loss function for your use case depends on the target 
distribution in your data set, the problem that you want to model, the number of 
outliers in the data, and so on.

You can specify the loss function to be used during {reganalysis} when you 
create the {dfanalytics-job}. The default is mean squared error (`mse`). If you 
choose `msle` or `huber`, you can also set up a parameter for the loss function. 
With the parameter, you can further refine the behavior of the chosen functions.

Consult 
https://github.com/elastic/examples/tree/master/Machine%20Learning/Regression%20Loss%20Functions[the Jupyter notebook on regression loss functions] 
to learn more.

TIP: The default loss function parameter values work fine for most of the cases. 
It is highly recommended to use the default values, unless you fully understand 
the impact of the different loss function parameters.

[discrete]
[[dfa-regression-deploy]]
==== Deploying the model

The model that you created is stored as {es} documents in internal indices. In 
other words, the characteristics of your trained model are saved and ready to be 
deployed and used as functions. The <<ml-inference-reg,{infer}>> feature enables 
you to use your model in a preprocessor of an ingest pipeline or in a pipeline 
aggregation of a search query to make predictions about your data.

[discrete]
[[dfa-regression-feature-importance]]
=== {feat-imp-cap}

{feat-imp-cap} provides further information about the results of an analysis and 
helps to interpret the results in a more subtle way. If you want to learn more 
about {feat-imp}, <<ml-feature-importance,click here>>.

[discrete]
[[dfa-regression-evaluation]]
=== Measuring model performance

You can measure how well the model has performed on your training data set by 
using the `regression` evaluation type of the 
{ref}/evaluate-dfanalytics.html[evaluate {dfanalytics} API]. The mean squared 
error (MSE) value that the evaluation provides you on the training data set is 
the _training error_. Training the {regression} model means finding the 
combination of model parameters that produces the lowest possible training 
error.

Another crucial measurement is how well your model performs on unseen 
data points. To assess how well the trained model will perform on data it has 
never seen before, you must set aside a proportion of the training data set for 
testing. This split of the data set is the testing data set. Once the model has 
been trained, you can let the model 
predict the value of the data points it has never seen before and compare the 
prediction to the actual value. This test provides an estimate of a quantity 
known as the _model generalization error_.

Two concepts describe how well the {regression} algorithm was able to learn the 
relationship between the {feature-vars} and the {depvar}. _Underfitting_ is when 
the model cannot capture the complexity of the data set. _Overfitting_ is when 
the model is too specific to the training data set and is capturing details 
which do not generalize to new data. A model that overfits the data has a 
low MSE value on the training data set and a high MSE value on the testing 
data set. For more information about the evaluation metrics, see 
<<ml-dfanalytics-regression-evaluation>>.

[discrete]
[[dfa-regression-readings]]
=== Further readings

* https://github.com/elastic/examples/tree/master/Machine%20Learning/Feature%20Importance[Feature importance for {dfanalytics} (Jupyter notebook)]

* https://github.com/elastic/examples/tree/master/Machine%20Learning/Regression%20Loss%20Functions[Regression loss functions (Jupyter notebook)]


[discrete]
[[ml-dfanalytics-regression-evaluation]]
=== {regression-cap} evaluation

include::ml-dfa-shared.asciidoc[tag=dfa-evaluation-intro]

This evaluation type is suitable for evaluating {regression} models. The 
{regression} evaluation type offers the following metrics to evaluate the model 
performance:

* Mean squared error (MSE)
* R-squared (R^2^)
* Pseudo-Huber loss
* Mean squared logarithmic error (MSLE)

[discrete]
[[ml-dfanalytics-mse]]
==== Mean squared error

The API provides a MSE by computing the average squared sum of the difference 
between the true value and the value that the {regression} model predicted. 
(Avg (predicted value-actual value)^2^). You can use the MSE to measure how well 
the {reganalysis} model is performing.

[discrete]
[[ml-dfanalytics-r-sqared]]
==== R-squared

Another evaluation metric for {reganalysis} is R-squared (R^2^). It represents 
the goodness of fit and measures how much of the variation in the data the 
predictions are able to explain. The value of R^2^ are less than or equal to 1, 
where 1 indicates that the predictions and true values are equal. A value of 0 
is obtained when all the predictions are set to the mean of the true values. A 
value of 0.5 for R^2^ would indicate that, the predictions are 1 - 0.5^(1/2)^ 
(about 30%) closer to true values than their mean.

[discrete]
[[ml-dfanalytics-huber]]
==== Pseudo-Huber loss

https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss metric] 
behaves as mean absolute error (MAE) for errors larger than a predefined value 
(defaults to `1`) and as mean squared error (MSE) for errors smaller than the 
predefined value. This loss function uses the `delta` parameter to define the 
transition point between MAE and MSE. Consult the 
<<dfa-regression-lossfunction>> page to learn more about loss functions.

[discrete]
[[ml-dfanalytics-msle]]
==== Mean squared logarithmic error

This evaluation metric is a variation of mean squared error. It can be used for 
cases when the target values are positive and distributed with a long tail such 
as data on prices or population. Consult the <<dfa-regression-lossfunction>> 
page to learn more about loss functions.

[discrete]
[[performing-regression]]
== Performing {reganalysis}

Let's try to predict flight delays by using the 
{kibana-ref}/add-sample-data.html[sample flight data]. The data set contains
information such as weather conditions, flight destinations and origins, flight 
distances, carriers, and the number of minutes each flight was delayed. When you
create a {dfanalytics-job} for {reganalysis}, it learns the relationships
between the fields in your data in order to predict the value of a
_dependent variable_, which in this case is the numeric `FlightDelayMins` field.
For an overview of these concepts, see <<dfa-regression>> and
<<ml-supervised-workflow>>.

[discrete]
[[flightdata-regression-data]]
=== Preparing your data

Each document in the data set contains details for a single flight, so this data 
is ready for analysis; it is already in a two-dimensional entity-based data 
structure. In general, you often need to 
{ref}/transforms.html[transform] the data into an entity-centric index before 
you analyze the data.

In order to be analyzed, a document must contain at least one field with a
supported data type (`numeric`, `boolean`, `text`, `keyword` or `ip`) and must
not contain arrays with more than one item. If your source data consists of some
documents that contain the dependent variable and some that do not, the model is
trained on the subset of the documents that contain it.

.Example source document
[%collapsible]
====
```
{
  "_index": "kibana_sample_data_flights",
  "_type": "_doc",
  "_id": "S-JS1W0BJ7wufFIaPAHe",
  "_version": 1,
  "_seq_no": 3356,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "FlightNum": "N32FE9T",
    "DestCountry": "JP",
    "OriginWeather": "Thunder & Lightning",
    "OriginCityName": "Adelaide",
    "AvgTicketPrice": 499.08518599798685,
    "DistanceMiles": 4802.864932998549,
    "FlightDelay": false,
    "DestWeather": "Sunny",
    "Dest": "Chubu Centrair International Airport",
    "FlightDelayType": "No Delay",
    "OriginCountry": "AU",
    "dayOfWeek": 3,
    "DistanceKilometers": 7729.461862731618,
    "timestamp": "2019-10-17T11:12:29",
    "DestLocation": {
      "lat": "34.85839844",
      "lon": "136.8049927"
    },
    "DestAirportID": "NGO",
    "Carrier": "ES-Air",
    "Cancelled": false,
    "FlightTimeMin": 454.6742272195069,
    "Origin": "Adelaide International Airport",
    "OriginLocation": {
      "lat": "-34.945",
      "lon": "138.531006"
    },
    "DestRegion": "SE-BD",
    "OriginAirportID": "ADL",
    "OriginRegion": "SE-BD",
    "DestCityName": "Tokoname",
    "FlightTimeHour": 7.577903786991782,
    "FlightDelayMin": 0
  }
}
```
====

TIP: The sample flight data is used in this example because it is easily
accessible. However, the data has been manually created and contains some
inconsistencies. For example, a flight can be both delayed and canceled. This is
a good reminder that the quality of your input data affects the quality of your
results.

[discrete]
[[flightdata-regression-model]]
=== Creating a {regression} model

To predict the number of minutes delayed for each flight:

. Verify that your environment is set up properly to use {ml-features}. If the
{stack} {security-features} are enabled, you need a user that has authority
to create and manage {dfanalytics-jobs}. See <<setup>>.

. Create a {dfanalytics-job}.
+
--
You can use the wizard on the *{ml-app}* > *Data Frame Analytics* tab
in {kib} or the {ref}/put-dfanalytics.html[create {dfanalytics-jobs}] API.

[role="screenshot"]
image::images/flights-regression-job-1.jpg["Creating a {dfanalytics-job} in {kib}"]
--
.. Choose `kibana_sample_data_flights` as the source index.
.. Choose `regression` as the job type.
.. Optionally improve the quality of the analysis by adding a query that removes erroneous data. In this case, we omit flights with a distance of 0 kilometers or less.
.. Choose `FlightDelayMin` as the dependent variable, which is the field that we
want to predict with the {reganalysis}.
.. Add `Cancelled`, `FlightDelay`, and `FlightDelayType` to the list of excluded
fields. These fields will be excluded from the analysis. It is recommended to 
exclude fields that either contain erroneous data or describe the 
`dependent_variable`.
+
--
The wizard includes a scatterplot matrix, which enables you to explore the 
relationships between the numeric fields. The color of each point is affected by 
the value of the dependent variable for that document, as shown in the legend. 
You can use this matrix to help you decide which fields to include or exclude 
from the analysis.

[role="screenshot"]
image::images/flightdata-regression-scatterplot.png["A scatterplot matrix for three fields in {kib}"]

If you want these charts to represent data from a larger sample size or from a
randomized selection of documents, you can change the default behavior. However, 
a larger sample size might slow down the performance of the matrix and a
randomized selection might put more load on the cluster due to the more
intensive query.
--
.. Choose a training percent of `90` which means it randomly selects 90% of the
source data for training.
.. If you want to experiment with <<ml-feature-importance,{feat-imp}>>, specify
a value in the advanced configuration options. In this example, we choose to
return a maximum of 5 {feat-imp} values per document. This option affects the
speed of the analysis, so by default it is disabled.
.. Use a model memory limit of at least 50 MB. If the job requires more than
this amount of memory, it fails to start. If the available memory on the node is
limited, this setting makes it possible to prevent job execution.
.. Add a job ID (such as `model-flight-delay-regression`) and optionally a job description.
.. Add the name of the destination index that will contain the results of the 
analysis. In {kib}, the index name matches the job ID by default. It will
contain a copy of the source index data where each document is annotated with
the results. If the index does not exist, it will be created automatically.
+
--
.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
PUT _ml/data_frame/analytics/model-flight-delays-regression
{
  "source": {
    "index": [
      "kibana_sample_data_flights"
    ],
    "query": {
      "range": {
        "DistanceKilometers": { 
          "gt": 0
        }
      }
    }
  },
  "dest": {
    "index": "model-flight-delays-regression"
  },
  "analysis": {
    "regression": {
      "dependent_variable": "FlightDelayMin",
      "training_percent": 90
    }
  },
  "analyzed_fields": {
    "includes": [],
    "excludes": [
      "Cancelled",
      "FlightDelay",
      "FlightDelayType"
    ]
  }
}
--------------------------------------------------
// TEST[skip:setup kibana sample data]
====
--

. Start the job in {kib} or use the
{ref}/start-dfanalytics.html[start {dfanalytics-jobs}] API.
+
--
The job takes a few minutes to run. Runtime depends on the local hardware and 
also on the number of documents and fields that are analyzed. The more fields
and documents, the longer the job runs. It stops automatically when the analysis
is complete.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _ml/data_frame/analytics/model-flight-delays-regression/_start
--------------------------------------------------
// TEST[skip:TBD]
====
--

. Check the job stats to follow the progress in {kib} or use the 
{ref}/get-dfanalytics-stats.html[get {dfanalytics-jobs} statistics API].
+
--
[role="screenshot"]
image::images/flights-regression-details.jpg["Statistics for a {dfanalytics-job} in {kib}"]

When the job stops, the results are ready to view and evaluate. To learn more
about the job phases, see <<ml-dfa-phases>>.


.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
GET _ml/data_frame/analytics/model-flight-delays-regression/_stats
--------------------------------------------------
// TEST[skip:TBD]

The API call returns the following response: 

[source,console-result]
----  
{
  "count" : 1,
  "data_frame_analytics" : [
    {
      "id" : "model-flight-delays-regression",
      "state" : "stopped",
      "progress" : [
        {
          "phase" : "reindexing",
          "progress_percent" : 100
        },
        {
          "phase" : "loading_data",
          "progress_percent" : 100
        },
        {
          "phase" : "feature_selection",
          "progress_percent" : 100
        },
        {
          "phase" : "coarse_parameter_search",
          "progress_percent" : 100
        },
        {
          "phase" : "fine_tuning_parameters",
          "progress_percent" : 100
        },
        {
          "phase" : "final_training",
          "progress_percent" : 100
        },
        {
          "phase" : "writing_results",
          "progress_percent" : 100
        },
        {
          "phase" : "inference",
          "progress_percent" : 100
        }
      ],
      "data_counts" : {
        "training_docs_count" : 11210,
        "test_docs_count" : 1246,
        "skipped_docs_count" : 0
      },
      "memory_usage" : {
        "timestamp" : 1599773614155,
        "peak_usage_bytes" : 50156565,
        "status" : "ok"
      },
      "analysis_stats" : {
        "regression_stats" : {
          "timestamp" : 1599773614155,
          "iteration" : 18,
          "hyperparameters" : {
            "alpha" : 19042.721566629778,
            "downsample_factor" : 0.911884068909842,
            "eta" : 0.02331774683318904,
            "eta_growth_rate_per_tree" : 1.0143154178910303,
            "feature_bag_fraction" : 0.5504020748926737,
            "gamma" : 53.373570122718846,
            "lambda" : 2.94058933878574,
            "max_attempts_to_add_tree" : 3,
            "max_optimization_rounds_per_hyperparameter" : 2,
            "max_trees" : 894,
            "num_folds" : 4,
            "num_splits_per_feature" : 75,
            "soft_tree_depth_limit" : 2.945317520946171,
            "soft_tree_depth_tolerance" : 0.13448633124842999
          },
          "timing_stats" : {
            "elapsed_time" : 302959,
            "iteration_time" : 13075
          },
          "validation_loss" : {
            "loss_type" : "mse"
          }
        }
      }
    }
  ]
}
----
====
--

[discrete]
[[flightdata-regression-results]]
=== Viewing {regression} results

Now you have a new index that contains a copy of your source data with 
predictions for your dependent variable.

When you view the {regression} results in {kib}, it shows the contents of the
destination index in a tabular format. It also provides information about the 
analysis details, model evaluation metrics, total feature importance values, and
a scatterplot matrix. Let’s start by looking at the results table:

[role="screenshot"]
image::images/flights-regression-results.jpg["Results for a {dfanalytics-job} in {kib}"]

In this example, the table shows a column for the dependent variable
(`FlightDelayMin`), which contains the ground truth values that we are trying to
predict with the {reganalysis}. It also shows a column for the prediction values
(`ml.FlightDelayMin_prediction`) and a column that indicates whether the
document was used in the training set (`ml.is_training`). You can filter the
table to show only testing or training data and you can select which fields are
shown in the table. You can also enable histogram charts to get a better
understanding of the distribution of values in your data.

If you chose to calculate {feat-imp}, the destination index also contains
`ml.feature_importance` objects. Every field that is included in the
{reganalysis} (known as a _feature_ of the data point) is assigned a {feat-imp}
value. This value has both a magnitude and a direction (positive or negative),
which indicates how each field affects a particular prediction. Only the most
significant values (in this case, the top 5) are stored in the index. However,
the trained model metadata also contains the average magnitude of the {feat-imp}
values for each field across all the training data. You can view this
summarized information in {kib}:

[role="screenshot"]
image::images/flights-regression-total-importance.jpg["Total {feat-imp} values in {kib}"]

You can also see the {feat-imp} values for each individual prediction in the
form of a decision plot:

[role="screenshot"]
image::images/flights-regression-importance.png["A decision plot for {feat-imp} values in {kib}"]

The decision path starts at a baseline, which is the average of the predictions
for all the data points in the training data set. From there, the feature
importance values are added to the decision path until it arrives at its final
prediction. The features with the most significant positive or negative impact
appear at the top. Thus in this example, the features related to the flight
distance had the most significant influence on this particular predicted flight
delay. This type of information can help you to understand how models arrive at  
their predictions. It can also indicate which aspects of your data set are most
influential or least useful when you are training and tuning your model.

If you do not use {kib}, you can see summarized {feat-imp} values by using the
{ref}/get-inference.html[get trained model API] and the individual values by
searching the destination index.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
GET _ml/inference/model-flight-delays-regression*?include=total_feature_importance,feature_importance_baseline
--------------------------------------------------
// TEST[skip:TBD]

The snippet below shows an example of the total feature importance details in
the trained model metadata:

[source,console-result]
----
{
  "count" : 1,
  "trained_model_configs" : [
    {
      "model_id" : "model-flight-delays-regression-1601312043770",
      ...
      "metadata" : {
        ...
        "feature_importance_baseline" : {
          "baseline" : 47.43643652716527 <1>
        },
        "total_feature_importance" : [
          {
            "feature_name" : "dayOfWeek",
            "importance" : {
              "mean_magnitude" : 0.38674590521018903, <2>
              "min" : -9.42823116446923, <3>
              "max" : 8.707461689065173 <4>
            }
          },
          {
            "feature_name" : "OriginWeather",
            "importance" : {
            "mean_magnitude" : 0.18548393012368913,
            "min" : -9.079576266629092,
            "max" : 5.142479101907649
          }
          ...
----
<1> This value is the baseline for the {feat-imp} decision path. It is the
average of the prediction values across all the training data.
<2> This value is the average of the absolute {feat-imp} values for the
`dayOfWeek` field across all the training data.
<3> This value is the minimum {feat-imp} value across all the training data for
this field.
<4> This value is the maximum {feat-imp} value across all the training data for
this field.

To see the top {feat-imp} values for each prediction, search the destination
index. For example:

[source,console]
--------------------------------------------------
GET model-flight-delays-regression/_search
--------------------------------------------------
// TEST[skip:TBD]

The snippet below shows a part of a document with the annotated results:

[source,console-result]
----  
          ...
          "DestCountry" : "CH",
          "DestRegion" : "CH-ZH",
          "OriginAirportID" : "VIE",
          "DestCityName" : "Zurich",
          "ml": {
            "FlightDelayMin_prediction": 277.5392150878906,
            "feature_importance": [
            {
              "feature_name": "DestCityName",
              "importance": 0.6285966753441136
            },
            {
              "feature_name": "DistanceKilometers",
              "importance": 84.4982943868267
            },
            {
              "feature_name": "DistanceMiles",
              "importance": 103.90011847132116
            },
            {
              "feature_name": "FlightTimeHour",
              "importance": 3.7119156097309345
            },
            {
              "feature_name": "FlightTimeMin",
              "importance": 38.700587425831365
            }
            ],
            "is_training": true
          }
          ...
----
====

Lastly, {kib} provides a scatterplot matrix in the results. It has the same
functionality as the matrix that you saw in the job wizard. Its purpose is to
likewise help you visualize and explore the relationships between the numeric
fields and the dependent variable in your data.

[discrete]
[[flightdata-regression-evaluate]]
=== Evaluating {regression} results

Though you can look at individual results and compare the predicted value
(`ml.FlightDelayMin_prediction`) to the actual value (`FlightDelayMins`), you
typically need to evaluate the success of the {regression} model as a whole.

{kib} provides _training error_ metrics, which represent how well the model
performed on the training data set. It also provides _generalization error_
metrics, which represent how well the model performed on testing data.

[role="screenshot"]
image::images/flights-regression-evaluation.jpg["Evaluating {reganalysis} results in {kib}"]

A mean squared error (MSE) of zero means that the models predicts the dependent 
variable with perfect accuracy. This is the ideal, but is typically not possible. 
Likewise, an R-squared value of 1 indicates that all of the variance in the 
dependent variable can be explained by the feature variables. Typically, you 
compare the MSE and R-squared values from multiple {regression} models to find
the best balance or fit for your data.

For more information about the interpreting the evaluation metrics, see
<<ml-dfanalytics-regression-evaluation>>.

You can alternatively generate these metrics with the
{ref}/evaluate-dfanalytics.html[{dfanalytics} evaluate API].

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
 "index": "model-flight-delays-regression",
  "query": {
      "bool": {
        "filter": [{ "term":  { "ml.is_training": true } }]  <1>
      }
    },
 "evaluation": {
   "regression": {
     "actual_field": "FlightDelayMin",   <2>
     "predicted_field": "ml.FlightDelayMin_prediction", <3>
     "metrics": {
       "r_squared": {},
       "mse": {},
       "msle": {},
       "huber": {}                           
     }
   }
 }
}
--------------------------------------------------
// TEST[skip:TBD]

<1> Calculate the training error by evaluating only the training data.
<2> The field that contains the actual (ground truth) value.
<3> The field that contains the predicted value.

The API returns a response like this:

[source,console-result]
----  
{
  "regression" : {
    "huber" : {
      "value" : 30.216037330465102
    },
    "mse" : {
      "value" : 2847.2211476422967
    },
    "msle" : {
      "value" : "NaN"
    },
    "r_squared" : {
      "value" : 0.6956530017255125
    }
  }
}
----

Next, we calculate the generalization error:

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
 "index": "model-flight-delays-regression",
  "query": {
      "bool": {
        "filter": [{ "term":  { "ml.is_training": false } }] <1>
      }
    },
 "evaluation": {
   "regression": {
     "actual_field": "FlightDelayMin",
     "predicted_field": "ml.FlightDelayMin_prediction",
     "metrics": {  
       "r_squared": {},
       "mse": {},
       "msle": {},
       "huber": {}                           
     }
   }
 }
}
--------------------------------------------------
// TEST[skip:TBD]
<1> Evaluate only the documents that are not part of the training data.
====

When you have trained a satisfactory model, you can deploy it to make predictions
about new data.

If you don't want to keep the {dfanalytics-job}, you can delete it. For example,
use {kib} or the {ref}/delete-dfanalytics.html[delete {dfanalytics-job} API].
When you delete {dfanalytics-jobs} in {kib}, you have the option to also remove
the destination indices and index patterns.

[discrete]
[[ml-inference-reg]]
== {infer-cap}

include::ml-dfa-shared.asciidoc[tag=dfa-inference]

[discrete]
[[ml-inference-processor-reg]]
=== {infer-cap} processor

include::ml-dfa-shared.asciidoc[tag=dfa-inference-processor]

[discrete]
[[ml-inference-aggregation-reg]]
=== {infer-cap} aggregation

include::ml-dfa-shared.asciidoc[tag=dfa-inference-aggregation]