tag::ml-nlp-adaptive-allocations[]
The numbers of threads and allocations you can set manually for a model remain constant even when not all the available resources are fully used or when the load on the model requires more resources.
Adaptive allocations, however, provide full control and flexibility over the model allocations, and helps you to manage performance and cost more easily.
When adaptive allocations is enabled, the number of allocations of the model is set automatically based on the current load.
When the load is high, a new model allocation is automatically created.
When the load is low, a model allocation is automatically removed.

You can enable adaptive allocations by using:

* the Create inference endpoint API for {ref}/infer-service-elser.html[ELSER], {ref}/infer-service-elasticsearch.html[E5 and models uploaded through Eland] that are used as {infer} services.
* the {ref}/start-trained-model-deployment.html[start trained model deployment] or {ref}/update-trained-model-deployment.html[update trained model deployment] APIs for trained models that are uploaded to and managed on {ml} nodes.

If the new allocations fit on the current {ml} nodes, they are immediately started.
If more resource capacity is needed for creating new model allocations, then your {ml} node will be scaled up if {ml} autoscaling is enabled to provide enough resources for the new allocation.
The number of model allocations cannot be scaled down to less than 1.
And they cannot be scaled up to more than 32 allocations, unless you explicitly set the maximum number of allocations to more.
Adaptive allocations must set up independently for each deployment and {infer} endpoint.
end::ml-nlp-adaptive-allocations[]

tag::nlp-eland-clone-docker-build[]
You can use the {eland-docs}[Eland client] to install the {nlp} model. Use the prebuilt  
Docker image to run the Eland install model commands. Pull the latest image with:

[source,shell]
--------------------------------------------------
docker pull docker.elastic.co/eland/eland
--------------------------------------------------

After the pull completes, your Eland Docker client is ready to use.
end::nlp-eland-clone-docker-build[]

tag::nlp-requirements[]
To follow along the process on this page, you must have:

* an {es} Cloud cluster that is set up properly to use the {ml-features}. Refer 
to <<setup>>.

* The {subscriptions}[appropriate subscription] level or the free trial period 
activated.

* https://docs.docker.com/get-docker/[Docker] installed.
end::nlp-requirements[]

tag::nlp-start[]
Since the `--start` option is used at the end of the Eland import command, {es} 
deploys the model ready to use. If you have multiple models and want to select 
which model to deploy, you can use the **{ml-app} > Model Management** user 
interface in {kib} to manage the starting and stopping of models.
end::nlp-start[]

tag::nlp-sync[]
Go to the **{ml-app} > Trained Models** page and synchronize your trained 
models. A warning message is displayed at the top of the page that says 
_"ML job and trained model synchronization required"_. Follow the link to 
_"Synchronize your jobs and trained models."_ Then click **Synchronize**. You 
can also wait for the automatic synchronization that occurs in every hour, or 
use the {kibana-ref}/ml-sync.html[sync {ml} objects API].
end::nlp-sync[]