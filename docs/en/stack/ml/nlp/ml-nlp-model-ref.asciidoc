[[ml-nlp-model-ref]]
= Third party NLP models

The {stack-ml-features} support transformer models that conform to the standard
BERT model interface and use the WordPiece tokenization algorithm.

The current list of supported architectures is:

* BERT
* BART
* DPR bi-encoders
* DistilBERT
* ELECTRA
* MobileBERT
* RoBERTa
* RetriBERT
* MPNet
* SentenceTransformers bi-encoders with the above transformer architectures

In general, any trained model that has a supported architecture is deployable in
{es} by using eland. However, it is not possible to test every third party
model. The following lists are therefore provided for informational purposes
only and may not be current. Elastic makes no warranty or assurance that the
{ml-features} will continue to interoperate with these third party models in the
way described, or at all.

These models are listed by NLP task; for more information about those tasks,
refer to <<ml-nlp-overview>>.

[discrete]
[[ml-nlp-model-ref-mask]]
== Third party fill-mask models

* https://huggingface.co/bert-base-uncased
* https://huggingface.co/distilroberta-base
* https://huggingface.co/roberta-large
* https://huggingface.co/microsoft/mpnet-base

[discrete]
=== Fill mask expected model output

Fill mask is a specific kind of token classification; it is the base training task of many transformer models.

For the Elastic stack's fill mask NLP task to understand the model output, it must have a specific format. It needs to
be a float tensor with `shape(<number of sequences>, <number of tokens>, <vocab size>)`. 

Here is an example with a single sequence `"The capital of [MASK] is Paris"` and with vocabulary
`["The", "capital", "of", "is", "Paris", "France", "[MASK]"]`.

Should output:

[source]
----
 [
   [
     [ 0, 0, 0, 0, 0, 0, 0 ], // The
     [ 0, 0, 0, 0, 0, 0, 0 ], // capital
     [ 0, 0, 0, 0, 0, 0, 0 ], // of
     [ 0.01, 0.01, 0.3, 0.01, 0.2, 1.2, 0.1 ], // [MASK]
     [ 0, 0, 0, 0, 0, 0, 0 ], // is
     [ 0, 0, 0, 0, 0, 0, 0 ] // Paris
   ] 
]
----

The predicted value here for `[MASK]` is obviously `"France"` with a score of 1.2.

[discrete]
[[ml-nlp-model-ref-ner]]
== Third party named entity recognition models

* https://huggingface.co/dslim/bert-base-NER
* https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english
* https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english
* https://huggingface.co/philschmid/distilroberta-base-ner-conll2003

[discrete]
=== Named entity recognition expected model output

Named entity recognition is a specific token classification task. Each token in the sequence is scored related to 
a specific set of classification labels. For the Elastic Stack, we use Inside-Outside-Beginning (IOB) tagging. Additionally,
 only the following classification labels are supported: "O", "B_MISC", "I_MISC", "B_PER", "I_PER", "B_ORG", "I_ORG", "B_LOC", "I_LOC".

The `"O"` entity label indicates that the current token is outside any entity.
`"I"` indicates that the token is inside an entity.
`"B"` indicates the beginning of an entity.
`"MISC"` is a miscellaneous entity.
`"LOC"` is a location.
`"PER"` is a person.
`"ORG"` is an organization.

The response format must be a float tensor with `shape(<number of sequences>, <number of tokens>, <num classification labels>)`.

Here is an example with a single sequence `"Waldo is in Paris"`:

[source]
----
 [
   [
//    "O", "B_MISC", "I_MISC", "B_PER", "I_PER", "B_ORG", "I_ORG", "B_LOC", "I_LOC"
     [ 0,  0,         0,       0.4,     0.5,     0,       0.1,     0,       0 ], // Waldo 
     [ 1,  0,         0,       0,       0,       0,       0,       0,       0 ], // is
     [ 1,  0,         0,       0,       0,       0,       0,       0,       0 ], // in
     [ 0,  0,         0,       0,       0,       0,       0,       0,       1.0 ] // Paris
   ] 
]
----

[discrete]
[[ml-nlp-model-ref-text-embedding]]
== Third party text embedding models

Using `SentenceTransformerWrapper`:

* https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2
* https://huggingface.co/sentence-transformers/all-mpnet-base-v2
* https://huggingface.co/sentence-transformers/LaBSE
* https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b 
* https://huggingface.co/sentence-transformers/msmarco-MiniLM-L-12-v3
* https://huggingface.co/sentence-transformers/nli-bert-base-cls-pooling
* https://huggingface.co/sentence-transformers/bert-base-nli-cls-token
* https://huggingface.co/sentence-transformers/facebook-dpr-ctx_encoder-multiset-base
* https://huggingface.co/sentence-transformers/facebook-dpr-question_encoder-single-nq-base
* https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2
* https://huggingface.co/sentence-transformers/all-distilroberta-v1

Using `DPREncoderWrapper`:

* https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base
* https://huggingface.co/facebook/dpr-question_encoder-single-nq-base
* https://huggingface.co/facebook/dpr-ctx_encoder-multiset-base
* https://huggingface.co/facebook/dpr-question_encoder-multiset-base
* https://huggingface.co/castorini/ance-dpr-context-multi
* https://huggingface.co/castorini/ance-dpr-question-multi
* https://huggingface.co/castorini/bpr-nq-ctx-encoder
* https://huggingface.co/castorini/bpr-nq-question-encoder

[discrete]
=== Text embedding expected model output

Text embedding allows for semantic embedding of text for dense information retrieval. 
The output of the model must be the specific embedding directly without any additional pooling. 

Eland does this wrapping for the aforementioned models. But if supplying your own, the model must output the embedding for
each inferred sequence.

[discrete]
[[ml-nlp-model-ref-text-classification]]
=== Third party text classification models

* https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english
* https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion
* https://huggingface.co/Hate-speech-CNERG/dehatebert-mono-english
* https://huggingface.co/ProsusAI/finbert
* https://huggingface.co/nateraw/bert-base-uncased-emotion
* https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment

[discrete]
=== Text classification expected model output

With text classification (for example, in tasks like sentiment analysis), the entire sequence is classified. The output of
the model must be a float tensor with `shape(<number of sequences>, <num classification labels>)`.

Here is an example with two sequences for a binary classification model of "happy" and "sad":
[source]
----
 [
   [
//     happy, sad
     [ 0,     1], // first sequence 
     [ 1,     0] // second sequence
   ] 
]
----


[discrete]
[[ml-nlp-model-ref-zero-shot]]
== Third party zero-shot text classification models

* https://huggingface.co/typeform/distilbert-base-uncased-mnli
* https://huggingface.co/typeform/mobilebert-uncased-mnli
* https://huggingface.co/typeform/squeezebert-mnli
* https://huggingface.co/facebook/bart-large-mnli
* https://huggingface.co/valhalla/distilbart-mnli-12-6
* https://huggingface.co/cross-encoder/nli-distilroberta-base
* https://huggingface.co/cross-encoder/nli-roberta-base

[discrete]
=== Zero-shot text classification expected model output

Zero-shot text classification allows text to be classified for arbitrary labels not necessarily part of the original
training. Each sequence is combined with the label given some hypothesis template. The model then scores each of these
combinations according to `[entailment, neutral, contradiction]`. The output of the model must be a float tensor 
with `shape(<number of sequences>, <number of labels>, <3>)`.

Here is an example with a single sequence classified against 4 labels:

[source]
----
 [
   [
//     entailment, neutral, contradiction
     [ 0.5,        0.1,     0.4], // first label 
     [ 0,          0,       1], // second label 
     [ 1,          0,       0], // third label 
     [ 0.7,        0.2,     0.1] // fourth label
   ] 
]
----
