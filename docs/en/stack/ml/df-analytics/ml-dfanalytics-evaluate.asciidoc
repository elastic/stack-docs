[role="xpack"]
[[ml-dfanalytics-evaluate]]
= Evaluating {dfanalytics}


[[ml-dfanalytics-regression-evaluation]]
== {regression-cap} evaluation

This evaluation type is suitable for evaluating {regression} models. The 
{regression} evaluation type offers the following metrics to evaluate the model 
performance:

* Mean squared error (MSE)
* R-squared (R^2^)
* Pseudo-Huber loss
* Mean squared logarithmic error (MSLE)

[[ml-dfanalytics-mse]]
=== Mean squared error

The API provides a MSE by computing the average squared sum of the difference 
between the true value and the value that the {regression} model predicted. 
(Avg (predicted value-actual value)^2^). You can use the MSE to measure how well 
the {reganalysis} model is performing.

[[ml-dfanalytics-r-sqared]]
=== R-squared

Another evaluation metric for {reganalysis} is R-squared (R^2^). It represents 
the goodness of fit and measures how much of the variation in the data the 
predictions are able to explain. The value of R^2^ are less than or equal to 1, 
where 1 indicates that the predictions and true values are equal. A value of 0 
is obtained when all the predictions are set to the mean of the true values. A 
value of 0.5 for R^2^ would indicate that, the predictions are 1 - 0.5^(1/2)^ 
(about 30%) closer to true values than their mean.

[[ml-dfanalytics-huber]]
=== Pseudo-Huber loss

https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss metric] 
behaves as mean absolute error (MAE) for errors larger than a predefined value 
(defaults to `1`) and as mean squared error (MSE) for errors smaller than the 
predefined value. This loss function uses the `delta` parameter to define the 
transition point between MAE and MSE. Consult the 
<<dfa-regression-lossfunction>> page to learn more about loss functions.

[[ml-dfanalytics-msle]]
=== Mean squared logarithmic error

This evaluation metric is a variation of mean squared error. It can be used for 
cases when the target values are positive and distributed with a long tail such 
as data on prices or population. Consult the <<dfa-regression-lossfunction>> 
page to learn more about loss functions.


[[ml-dfanalytics-classification]]
== {classification-cap} evaluation

This evaluation type is suitable for evaluating {classification} models. The 
{classification} evaluation offers the following metrics to evaluate the model 
performance:

* Multiclass confusion matrix
* Area under the curve of receiver operating characteristic (AUC ROC) 

[[ml-dfanalytics-mccm]]
=== Multiclass confusion matrix

The multiclass confusion matrix provides a summary of the performance of the 
{classanalysis}. It contains the number of occurrences where the analysis
classified data points correctly with their actual class as well as the number
of occurrences where it misclassified them.

Let's see two examples of the confusion matrix. The first is a confusion matrix 
of a binary problem:

image::images/confusion-matrix-binary.jpg[alt="Confusion matrix of a binary problem",width="75%",align="center"]

It is a two by two matrix because there are only two classes (`true` and
`false`). The matrix shows the proportion of data points that is correctly
identified as members of a each class and the proportion that is 
misidentified.

As the number of classes in your {classanalysis} increases, the confusion
matrix also increases in complexity:

image::images/confusion-matrix-multiclass.jpg[alt="Confusion matrix of a multiclass problem",width="100%",align="center"]


The matrix contains the actual labels on the left side while the predicted 
labels are on the top. The proportion of correct and incorrect predictions is 
broken down for each class. This enables you to examine how the {classanalysis}
confused the different classes while it made its predictions.


[[ml-dfanalytics-class-aucroc]]
=== Area under the curve of receiver operating characteristic (AUC ROC)

The receiver operating characteristic (ROC) curve is a plot that represents the 
performance of the classification process at different predicted probability 
thresholds. It compares the true positive rate for a specific class against the 
rate of all the other classes combined ("one versus all" strategy) at the 
different threshold levels to create the curve.

Let's see an example. You have three classes: `A`, `B`, and `C`, you want to 
calculate AUC ROC for `A`. In this case, the number of correctly classified 
++A++s (true positives) are compared to the number of ++B++s and ++C++s that are 
misclassified as ++A++s (false positives).

From this plot, you can compute the area under the curve (AUC) value, which is a 
number between 0 and 1. The higher the AUC, the better the model is at 
predicting ++A++s as ++A++s, in this case.

NOTE: To use this evaluation method, you must set `num_top_classes` to `-1`
or a value greater than or equal to the total number of classes when you create
the {dfanalytics-job}.
