[[ml-nlp-e5]]
= E5 – EmbEddings from bidirEctional Encoder rEpresentations
++++
<titleabbrev>E5</titleabbrev>
++++

:frontmatter-description: E5 is a multilingual dense vector model.
:frontmatter-tags-products: [ml] 
:frontmatter-tags-content-type: [how-to] 
:frontmatter-tags-user-goals: [analyze]

EmbEddings from bidirEctional Encoder rEpresentations - or E5 -  is a {nlp} 
model that enables you to perform multi-lingual semantic search by using dense 
vector representations. This model is recommended for non-English language 
documents and queries. If you want to perform semantic search on English 
language documents, use the <<ml-nlp-elser>> model.

{ref}/semantic-search.html[Semantic search] provides you search results based on 
contextual meaning and user intent, rather than exact keyword matches.

E5 has two versions: one cross-platform version which runs on any hardware 
and one version which is optimized for Intel® silicon. The 
**Model Management** > **Trained Models** page shows you which version of E5 is 
recommended to deploy based on your cluster's hardware.

The supported model version of E5 is `multilingual-e5-small`, refer to 
<<ml-nlp-e5-limit, this page>> for more information.

Refer to the model cards of the 
https://huggingface.co/elastic/multilingual-e5-small[multilingual-e5-small] and 
the 
https://huggingface.co/elastic/multilingual-e5-small-optimized[multilingual-e5-small-optimized]
models on HuggingFace for further information including licensing.


[discrete]
[[e5-req]]
== Requirements

To use E5, you must have the {subscriptions}[appropriate subscription] level 
for semantic search or the trial period activated.


[discrete]
[[download-deploy-e5]]
== Download and deploy E5

You can download and deploy the E5 model either from 
**{ml-app}** > **Trained Models**, from **Search** > **Indices**, or by using 
the Dev Console.


[discrete]
[[trained-model-e5]]
=== Using the Trained Models page

1. In {kib}, navigate to **{ml-app}** > **Trained Models**. E5 can be found in 
the list of trained models. There are two versions available: one portable 
version which runs on any hardware and one version which is optimized for Intel® 
silicon. You can see which model is recommended to use based on your hardware 
configuration.
2. Click the **Add trained model** button. Select the E5 model version you want 
to use in the opening modal window. The model that is recommended for you based 
on your hardware configuration is highlighted. Click **Download**. You can check 
the download status on the **Notifications** page.
+
--
[role="screenshot"]
image::images/ml-nlp-e5-download.png[alt="Downloading E5",align="center"]
--
+
--
Alternatively, click the **Download model** button under **Actions** in the 
trained model list.
--
3. After the download is finished, start the deployment by clicking the 
**Start deployment** button.
4. Provide a deployment ID, select the priority, and set the number of 
allocations and threads per allocation values.
+
--
[role="screenshot"]
image::images/ml-nlp-deployment-id-e5.png[alt="Deploying ELSER",align="center"]
--
5. Click Start.


[discrete]
[[elasticsearch-e5]]
=== Using the search indices UI

Alternatively, you can download and deploy the E5 model to an {infer} pipeline 
using the search indices UI.

1. In {kib}, navigate to **Search** > **Indices**.
2. Select the index from the list that has an {infer} pipeline in which you want 
to use E5.
3. Navigate to the **Pipelines** tab.
4. Under **{ml-app} {infer-cap} Pipelines**, click the **Deploy** button in the 
**Improve your results with E5** section to begin downloading the E5 model. This 
may take a few minutes depending on your network. 
+
--
[role="screenshot"]
image::images/ml-nlp-deploy-e5-es.png[alt="Deploying E5 in Elasticsearch",align="center"]
--
5. Once the model is downloaded, click the **Start single-threaded** button to 
start the model with basic configuration or select the **Fine-tune performance** 
option to navigate to the **Trained Models** page where you can configure the 
model deployment.
+
--
[role="screenshot"]
image::images/ml-nlp-start-e5-es.png[alt="Start E5 in Elasticsearch",align="center"]
--

When your E5 model is deployed and started, it is ready to be used in a 
pipeline.


[discrete]
[[dev-console-e5]]
=== Using the Dev Console

1. In {kib}, navigate to the **Dev Console**.
2. Create the E5 model configuration by running the following API call:
+
--
[source,console]
----------------------------------
PUT _ml/trained_models/.multilingual-e5-small
{
  "input": {
	"field_names": ["text_field"]
  }
}
----------------------------------

The API call automatically initiates the model download if the model is not 
downloaded yet.
--
3. Deploy the model by using the 
{ref}/start-trained-model-deployment.html[start trained model deployment API] 
with a delpoyment ID:
+
--
[source,console]
----------------------------------
POST _ml/trained_models/.multilingual-e5-small/deployment/_start?deployment_id=for_search
----------------------------------
--

[discrete]
[[air-gapped-install-e5]]
== Deploy the E5 model in an air-gapped environment

You can install the E5 model in a restricted or closed network by pointing the 
`eland_import_hub_model` script to the model's local files.

For an offline install, the model first needs to be cloned locally, Git and 
https://git-lfs.com/[Git Large File Storage] are required to be installed in 
your system.

1. Clone the E5 model from Hugging Face by using the model URL. 
+
--
Use this command for `multilingual-e5-small`:
[source,bash]
----
git clone https://huggingface.co/elastic/multilingual-e5-small
----

Use this command for `multilingual-e5-small-optimized`:
[source,bash]
----
git clone https://huggingface.co/elastic/multilingual-e5-small-optimized
----
Both commands result in a local copy of the respective model in the directory 
named after the model: `multilingual-e5-small` or 
`multilingual-e5-small-optimized`.
--

2. Use the `eland_import_hub_model` script with the `--hub-model-id` set to the 
directory of the cloned model to install it:
+
--
For `multilingual-e5-small`:
[source,bash]
----
eland_import_hub_model \
      --url 'XXXX' \
      --hub-model-id /PATH/TO/MODEL \
      --task-type text_embedding \
      --es-username elastic --es-password XXX \
      --es-model-id multilingual-e5-small
----

For `multilingual-e5-small-optimized`:
[source,bash]
----
eland_import_hub_model \
      --url 'XXXX' \
      --hub-model-id /PATH/TO/MODEL \
      --task-type text_embedding \
      --es-username elastic --es-password XXX \
      --es-model-id multilingual-e5-small-optimized
----

If you use the Docker image to run `eland_import_hub_model` you must bind mount 
the model directory, so the container can read the files.
For `multilingual-e5-small` run the following:
[source,bash]
----
docker run --mount type=bind,source=/PATH/TO/MODELS,destination=/models,readonly -it --rm docker.elastic.co/eland/eland \
    eland_import_hub_model \
      --url 'XXXX' \
      --hub-model-id /models/multilingual-e5-small \
      --task-type text_embedding \
      --es-username elastic --es-password XXX \
      --es-model-id multilingual-e5-small
----

For `multilingual-e5-small-optimized` run the following:
[source,bash]
----
docker run --mount type=bind,source=/PATH/TO/MODELS,destination=/models,readonly -it --rm docker.elastic.co/eland/eland \
    eland_import_hub_model \
      --url 'XXXX' \
      --hub-model-id /models/multilingual-e5-small-optimized \
      --task-type text_embedding \
      --es-username elastic --es-password XXX \
      --es-model-id multilingual-e5-small-optimized
----

Once it's uploaded to {es}, the model will have the ID specified by 
`--es-model-id`. If it is not set, the model ID is derived from 
`--hub-model-id`; spaces and path delimiters are converted to double 
underscores `__`.
--



[discrete]
[[terms-of-use-e5]]
== Disclaimer

Customers may add third party trained models for management in Elastic. These 
models are not owned by Elastic. Customers must contract separately with the 
third party model owner for the use of the model, and such use will be governed 
by the applicable terms and conditions. You understand and agree that Elastic 
has no control over, or liability for, the third party models.