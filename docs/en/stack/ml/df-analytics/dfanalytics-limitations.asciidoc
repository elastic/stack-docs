[role="xpack"]
[[ml-dfa-limitations]]
== {dfanalytics-cap} limitations
[subs="attributes"]
++++
<titleabbrev>Limitations</titleabbrev>
++++

experimental[]

The following limitations and known problems apply to the {version} release of 
the Elastic {dfanalytics} feature:

[float]
[[dfa-ccs-limitations]]
=== {ccs-cap} is not supported

{ccs-cap} is not supported for {dfanalytics}.

[float]
[[dfa-deletion-limitations]]
=== Deleting a {dfanalytics-job} does not delete the destination index

The {ref}/delete-dfanalytics.html[delete {dfanalytics-job} API] does not delete
the destination index that contains the annotated data of the {dfanalytics}. 
That index must be deleted separately.

[float]
[[dfa-update-limitations]]
=== {dfanalytics-jobs-cap} cannot be updated

You cannot update {dfanalytics-cap} configurations. Instead, delete the 
{dfanalytics-job} and create a new one.

[float]
[[dfa-dataframe-size-limitations]]
=== {dataframe-cap} memory limitation

{dfanalytics-cap} can analyze {dataframes} that fit into the memory limit 
dedicated for {ml} processes. For general {ml} settings, see 
{ref}/ml-settings.html[{ml-cap} settings in {es}].

[float]
[[dfa-time-limitations]]
=== {dfanalytics-jobs-cap} runtime may vary

The runtime of the {dfanalytics-jobs} depends on numerous factors, such as the 
number of data points in the dataset, the type of analytics, the number of 
fields that are included in the analysis, the supplied 
{ref}/put-dfanalytics.html#ml-hyperparam-optimization[hyperparameters], the 
type of analyzed fields and so on. For example, running an analysis on a dataset 
with many numerical fields will take longer than running an analysis on a 
dataset that contains mainly categorical fields. Hyperparameters specified by 
the user also lower the runtime. For this reason, a general runtime value that 
applies to all or most of the situations does not exist. The runtime of a 
{dfanalytics-job} may take from a couple of minutes up to 35 hours in extreme 
cases.

The runtime increases with the increasing number of analyzed fields in a nearly 
linear fashion. For datasets of more than 100 000 points, we recommend to start 
with a low training percent and run a few {dfanalytics-jobs} to see how the 
runtime scales with the increased number of data points and how the quality of 
results scales with increased training percentage.

[float]
[[dfa-missing-fields-limitations]]
=== Documents with missing values in analyzed fields are skipped

If there are missing values in feature fields (fields that are subjects of the 
{dfanalytics}), then the document that contains the fields with the missing 
values will be skipped during the analysis.

[float]
[[dfa-od-field-type-docs-limitations]]
=== {oldetection-cap} field types

{oldetection-cap} requires numeric or boolean data to analyze. The algorithms 
don't support missing values (see also <<dfa-missing-fields-limitations>>), 
therefore fields that have data types other than numeric or boolean are ignored. 
Documents where included fields contain missing values, null values, or an array 
are also ignored. Therefore a destination index may contain documents that don't 
have an {olscore}. These documents are still reindexed from the source index to 
the destination index, but they are not included in the {oldetection} analysis 
and therefore no {olscore} is computed.

[float]
[[dfa-regression-field-type-docs-limitations]]
=== {regression-cap} field types

{regression-cap} supports fields that are numeric, boolean, text, keyword and 
ip. It is also tolerant of missing values. Fields that are supported are 
included in the analysis, other fields are ignored. Documents where included 
fields contain an array are also ignored. Documents in the destination index 
that don't contain a results field are not included in the {reganalysis}.

[float]
[[dfa-classification-field-type-docs-limitations]]
=== {classification-cap} field types

{classification-cap} supports fields that have numeric, boolean, text, keyword, 
or ip data types. It is also tolerant of missing values. Fields that are 
supported are included in the analysis, other fields are ignored. Documents 
where included fields contain an array are also ignored. Documents in the 
destination index that don't contain a results field are not included in the 
{classanalysis}.

[float]
[[dfa-classification-imbalanced-classes]]
=== Imbalanced class sizes affect {classification} performance

If your training data is very imbalanced, then {classanalysis} may not provide 
good predictions. Training tries to mitigate the effects of imbalanced 
training data by maximizing the minimum recall of any class. For imbalanced training data, this means that it
will try to balance the proportion of values it correctly labels in the minority and majority class.
However, this process can result in a slight degradation of the overall 
accuracy. Try to avoid highly imbalanced situations. It is required at least 50 
examples for each class in order to get better results. If your training dataset 
is very imbalanced, fine-tune the data, for example, by downsampling the 
majority classes. Consider investigating methods to change data that fit your 
use case the most.

[float]
[[dfa-inference-nested-limitation]]
=== Deeply nested objects affect {infer} performance

If the data that you run inference against contains documents that have a series 
of combinations of dot delimited and nested fields (for example: 
`{"a.b": "c", "a": {"b": "c"},...}`), the performance of the operation might be 
slightly slower. Consider using as simple mapping as possible for the best 
performance profile.
