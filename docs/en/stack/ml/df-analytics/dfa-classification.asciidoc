[role="xpack"]
[[dfa-classification]]
=== {classification-cap}

experimental[]

{classification-cap} is a {ml} process for predicting the class or category of a 
given data point in a dataset. Typical examples of {classification} problems are 
predicting loan risk, classifying music, or detecting cancer in a DNA sequence. 
In the first case, for example, our dataset consists of data on loan applicants 
that covers investment history, employment status, debit status, and so on. 
Based on historical data, the {classification} analysis predicts whether it is 
safe or risky to lend money to a given loan applicant. In the second case, the 
data we have represents songs and the analysis – based on the features of the 
data points – classifies the songs as hip-hop, country, classical, or any 
other genres available in the set of categories we have. Therefore, 
{classification} is for predicting discrete, categorical values, unlike 
{reganalysis} which predicts continuous, numerical values.

From the perspective of the possible output, there are two types of 
{classification}: binary and multiclass {classification}. In binary 
{classification}, the variable you want to predict has only two potential 
values. The loan example above is a binary {classification} problem where the 
two potential outputs are `safe` or `risky`. The music classification problem is 
an example of multiclass {classification} where there are many different 
potential outputs; one for every possible music genre. In the {version} version 
of the {stack}, you can perform both binary and multiclass {classanalysis}. The 
maximum number of classes for multiclass {classification} is 30.


[discrete]
[[dfa-classification-features]]
==== {feature-vars-cap}

When you perform {classification}, you must identify a subset of fields that you 
want to use to create a model for predicting another field value. We refer to 
these fields as _{feature-vars}_ and _{depvar}_, respectively. 
{feature-vars-cap} are the values that the {depvar} value depends on. There are 
three different types of {feature-vars} that you can use with our 
{classification} algorithm: numerical, categorical, and boolean. Arrays are not 
supported in the {feature-var} fields.  


[discrete]
[[dfa-classification-supervised]]
==== Training the {classification} model

{classification-cap} – just like {regression} – is a supervised {ml} process. It 
means that you need to supply a labeled training dataset that has some 
{feature-vars} and a {depvar}. The {classification} algorithm learns the 
relationships between the features and the {depvar}. Once you’ve trained the 
model on your training dataset, you can reuse the knowledge that the model has 
learned about the relationships between the data points to classify new data. In 
the case of multiclass {classification}, an automated process called 
stratification makes sure that the proportions of each class in the training 
data are equal so that the training is consistent. 

Your training dataset should be approximately balanced. It means the number of 
data points belonging to the various classes should not be widely different. It 
is also necessary to have sufficient representation from all classes so that the 
model can learn about each class enough, otherwise, the {classanalysis} may not 
provide the best predictions.

The sufficient representation of the classes always depends on the complexity 
and the characteristics of the dataset. Complex decision boundaries – the rules 
that separate the different classes in a dataset – are harder to learn and 
require more data points per class. For this reason, it is not possible to 
declare a hard rule on how much data is enough per class that fits all use 
cases.

Read <<dfa-classification-imbalanced-classes>> to learn more.


[discrete]
[[dfa-classification-algorithm]]
===== {classification-cap} algorithms

//tag::classification-algorithms[]
The ensemble algorithm that we use in the {stack} for {classification} is a type 
of boosting called boosted tree regression model which combines multiple weak 
models into a composite one. We use decision trees to learn to predict the 
probability that a data point belongs to a certain class. A sequence of decision 
trees are trained and every decision tree learns from the mistakes of the 
previous one. Every tree is an iteration of the last one, hence it improves the 
decision made by the previous tree. Both binary and multiclass {classification} 
mostly use the same machinery since the latter is an extension of binary 
{classification}.
//end::classification-algorithms[]


[discrete]
[[dfa-classification-performance]]
==== {classification-cap} performance

As a rule of thumb, multiclass {classification} takes more time to run than 
binary. The runtime of the analysis scales approximately linearly with the 
number of involved documents below roughly 200.000 data points. Therefore, if 
you double the number of documents, then the runtime of the analysis doubles 
respectively. Above 200.000 data points, runtime increases faster.

The number of classes also affects the runtime of the analysis. The relationship 
between these two factors is linear. 
 

[discrete]
[[dfa-classification-interpret]]
==== Interpreting {classification} results

The following sections help you understand and interpret the results of a 
{classanalysis}.

[discrete]
[[dfa-classification-class-probability]]
===== `class_probability`

The value of `class_probability` shows how likely it is that a given datapoint 
belongs to a certain class. It is a value between 0 and 1. The higher the 
number, the higher the probability that the data point belongs to the named 
class. This information is stored in the `top_classes` array for each document 
in your destination index. See the
{ml-docs}/flightdata-classification.html#flightdata-classification-results[Viewing {classification} results]
section in the {classification} example.


[discrete]
[[dfa-classification-class-score]]
===== `class_score`

The value of `class_score` controls the probability at which a class label is 
assigned to a datapoint. In normal case – that you maximize the number of 
correct labels – a class label is assigned when its predicted probability is 
greater than 0.5. The `class_score` makes it possible to change this behavior, 
so it can be less than or greater than 0.5. For example, suppose our two classes 
are denoted `class 0` and `class 1`, then the value of `class_score` is always 
non-negative and its definition is:

```
class_score(class 0) = 0.5 / (1.0 - k) * probability(class 0)
class_score(class 1) = 0.5 / k * probability(class 1)
```

Here, `k` is a positive constant less than one. It represents the predicted 
probability of `class 1` for a datapoint at which to label it `class 1` and is 
chosen to maximise the minimum recall of any class. This is useful for example 
in case of highly imbalanced data. If `class 0` is much more frequent in the 
training data than `class 1`, then it can mean that you achieve the best 
accuracy by assigning `class 0` to every datapoint. This is equivalent to zero 
recall for `class 1`. Instead of this behavior, the default scheme of the 
{stack} {classanalysis} is to choose `k < 0.5` and accept a higher rate of 
actual `class 0` predicted `class 1` errors, or in other words, a slight 
degradation of the overall accuracy.


[discrete]
[[dfa-classification-feature-importance]]
===== Feature importance

include::{docdir}/shared-ml-concepts.asciidoc[tag=feature-importance]


[discrete]
[[dfa-classification-evaluation]]
==== Measuring model performance

You can measure how well the model has performed on your dataset by using the 
`classification` evaluation type of the 
{ref}/evaluate-dfanalytics.html[evaluate {dfanalytics} API]. The metric that the 
evaluation provides you is a confusion matrix both for binary {classification} 
and multiclass {classification}. The more classes you have, the more complex the 
confusion matrix is. The matrix tells you how many data points that belong to a 
given class were classified correctly and incorrectly. In other words, how many 
data points that belong to the X class were classified correctly and mistakenly 
classified as Y or Z or any other class in the dataset other than X.

Another crucial measurement is how well your model performs on unseen data 
points. To assess how well the trained model will perform on data it has never 
seen before, you must set aside a proportion of the training dataset for 
testing. This split of the dataset is the _testing dataset_. Once the model has 
been trained, you can let the model predict the value of the data points it has 
never seen before and compare the prediction to the actual value by using the 
evaluate {dfanalytics} API.
