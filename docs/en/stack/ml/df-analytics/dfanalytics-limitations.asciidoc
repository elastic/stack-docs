[role="xpack"]
[[ml-dfa-limitations]]
== {dfanalytics-cap} limitations
[subs="attributes"]
++++
<titleabbrev>Limitations</titleabbrev>
++++

experimental[]

The following limitations and known problems apply to the {version} release of 
the Elastic {dfanalytics} feature:

[float]
[[dfa-ccs-limitations]]
=== {ccs-cap} is not supported

{ccs-cap} is not supported for {dfanalytics}.

[float]
[[dfa-deletion-limitations]]
=== Deleting a {dfanalytics-job} does not delete the destination index

The {ref}/delete-dfanalytics.html[delete {dfanalytics-job} API] does not delete
the destination index that contains the annotated data of the {dfanalytics}. 
That index must be deleted separately.

[float]
[[dfa-update-limitations]]
=== {dfanalytics-jobs-cap} cannot be updated

You cannot update {dfanalytics} configurations. Instead, delete the 
{dfanalytics-job} and create a new one.

[float]
[[dfa-dataframe-size-limitations]]
=== {dfanalytics-cap} memory limitation

{dfanalytics-cap} can only perform analyses that fit into the memory available 
for {ml}. Overspill to disk is not currently possible. For general {ml} 
settings, see {ref}/ml-settings.html[{ml-cap} settings in {es}].

[float]
[[dfa-time-limitations]]
=== {dfanalytics-jobs-cap} runtime may vary

The runtime of {dfanalytics-jobs} depends on numerous factors, such as the
number of data points in the data set, the type of analytics, the number of 
fields that are included in the analysis, the supplied 
{ref}/put-dfanalytics.html#ml-hyperparam-optimization[hyperparameters], the 
type of analyzed fields, and so on. For this reason, a general runtime value that 
applies to all or most of the situations does not exist. The runtime of a 
{dfanalytics-job} may take from a couple of minutes up to 35 hours in extreme 
cases.

The runtime increases with an increasing number of analyzed fields in a nearly 
linear fashion. For data sets of more than 100,000 points, start with a low
training percent. Run a few {dfanalytics-jobs} to see how the runtime scales
with the increased number of data points and how the quality of results scales
with an increased training percentage.

[float]
[[dfa-missing-fields-limitations]]
=== Documents with missing values in analyzed fields are skipped

If there are missing values in feature fields (fields that are subjects of the 
{dfanalytics}), the document that contains these fields is skipped 
during the analysis.

[float]
[[dfa-od-field-type-docs-limitations]]
=== {oldetection-cap} field types

{oldetection-cap} requires numeric or boolean data to analyze. The algorithms 
don't support missing values (see also <<dfa-missing-fields-limitations>>), 
therefore fields that have data types other than numeric or boolean are ignored. 
Documents where included fields contain missing values, null values, or an array 
are also ignored. Therefore a destination index may contain documents that don't 
have an {olscore}. These documents are still reindexed from the source index to the 
destination index, but they are not included in the {oldetection} analysis and 
therefore no {olscore} is computed.

[float]
[[dfa-regression-field-type-docs-limitations]]
=== {regression-cap} field types

{regression-cap} supports fields that are numeric, boolean, text, keyword and 
ip. It is also tolerant of missing values. Fields that are supported are 
included in the analysis, other fields are ignored. Documents where included 
fields contain an array are also ignored. Documents in the destination index 
that don't contain a results field are not included in the {reganalysis}.

[float]
[[dfa-classification-field-type-docs-limitations]]
=== {classification-cap} field types

{classification-cap} supports fields that have numeric, boolean, text, keyword, 
or ip data types. It is also tolerant of missing values. Fields that are 
supported are included in the analysis, other fields are ignored. Documents 
where included fields contain an array are also ignored. Documents in the 
destination index that don't contain a results field are not included in the 
{classanalysis}.

[float]
[[dfa-classification-imbalanced-classes]]
=== Imbalanced class sizes affect {classification} performance

If your training data is very imbalanced, {classanalysis} may not provide 
good predictions. Try to avoid highly imbalanced situations. We recommend having 
at least 50 examples of each class and a ratio of no more than 10 to 1 for the 
majority to minority class labels in the training data. If your training data 
set is very imbalanced, consider downsampling the majority class, upsampling the 
minority class, or gathering more data.

[float]
[[dfa-inference-nested-limitation]]
=== Deeply nested objects affect {infer} performance

If the data that you run inference against contains documents that have a series 
of combinations of dot delimited and nested fields (for example: 
`{"a.b": "c", "a": {"b": "c"},...}`), the performance of the operation might be 
slightly slower. Consider using as simple mapping as possible for the best 
performance profile.

[float]
[[dfa-feature-importance-limitation]]
=== Analytics runtime performance may significantly slow down with feature importance computation

For complex models (such as those with many deep trees), the calculation of 
feature importance takes significantly more time. Feature importance is 
calculated at the end of the analysis and therefore the job may appear to be 
stuck at 99% for several hours.

If a reduction in runtime is important to you, try strategies such as disabling 
feature importance, using a smaller {transform}, setting 
{ref}/put-dfanalytics.html#ml-hyperparam-optimization[hyperparameter] values, or 
only selecting fields that are relevant for analysis.

[float]
[[dfa-inference-multi-field]]
=== Analytics training on multi-field values may affect {infer}

{dfanalytics-jobs-cap} dynamically select the best field when multi-field
values are included. For example, if a multi-field `foo` is included for training,
the `foo.keyword` is actually used. This poses a complication for {infer} with
the inference processor. Documents supplied to ingest pipelines are not mapped. Consequently,
only the field `foo` is present. This means that a model trained with the field `foo.keyword`
does not take the field `foo` into account.

You can work around this limitation by using the `field_mappings` parameter in the inference processor.

Example:
```
{
  "inference": {
    "model_id": "my_model_with_multi-fields",
    "field_mappings": {
      "foo": "foo.keyword"
    },
    "inference_config": { "regression": {} }
  }
}
```

[float]
[[dfa-inference-bwc]]
=== {infer-cap} trained models created in 7.8 are not backwards compatible

Inference models created in version 7.8.0 are not backwards compatible with 
older node versions. In a mixed cluster environment, all nodes must be at 
least 7.8.0 to use a model created on a 7.8.0 node.
