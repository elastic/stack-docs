[role="xpack"]
[[ml-dataframes]]
= {dataframe-transforms-cap}

[partintro]
--

beta[]

{es} aggregations are a powerful and flexible feature that enable you to
summarize and retrieve complex insights about your data. You can summarize
complex things like the number of web requests per day on a busy website, broken
down by geography and browser type. If you use the same data set to try to
calculate something as simple as a single number for the average duration of
visitor web sessions, however, you can quickly run out of memory. Why does this
occur? A web session duration is an example of a behavioral attribute not held
on any one log record; it has to be derived by finding the first and last
records for each session in our weblogs. This derivation requires some complex
query expressions and a lot of memory to connect all the data points. If you
have an ongoing background process that fuses related events from one index into
entity-centric summaries in another index, you get a more useful joined-up
picture--this is essentially what _{dataframes}_ are.

You might want to consider using {dataframes} instead of aggregations when:

* You need a complete _feature index_ rather than a top-N set of items.

** In {ml}, you often need a complete set of behavioral features rather just the
top-N. For example, if you are predicting customer churn, you might look at
features such as the number of website visits in the last week, the total number
of sales, or the number of emails sent. The {stack} {ml-features} create models
based on this multi-dimensional feature space, so they benefit from full feature
indices ({dataframes}).
** When you are trying to search across the results of an aggregation or
multiple aggregations. Aggregation results can be ordered or filtered, but there
are
{ref}/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-order[limitations to ordering]
and
{ref}/search-aggregations-pipeline-bucket-selector-aggregation.html[filtering by bucket selector]
is constrained by the maximum number of buckets returned. If you want to search
all aggregation results, you need to create the complete {dataframe}. If you
need to sort or filter the aggregation results by multiple fields, {dataframes}
are particularly useful.

* You need to sort aggregation results by a pipeline aggregation.
{ref}/search-aggregations-pipeline.html[Pipeline aggregations] cannot be used
for sorting. Technically, this is because pipeline aggregations are run during
the reduce phase after all other aggregations have already completed. If you
create a {dataframe}, you can effectively perform multiple passes over the data.

* You want to create summary tables to optimize queries. For example, if you
have a high level dashboard that is accessed by a large number of users and it
uses a complex aggregation over a large dataset, it may be more efficient to
create a {dataframe} to cache results. Thus, each user doesn't need to run the
aggregation query.

Though there are multiple ways to create {dataframes}, this content pertains
<<<<<<< HEAD
to one specific method: {dataframe-transforms}. {dataframe-transforms-cap}
enable you to define a pivot, which is a set of features that transform the
index into a different, more digestible format. Pivoting results in a summary of
your data, which is the {dataframe}.

To define a pivot, first you select one or more fields that you will use to
group your data. You can select categorical fields (terms) and numerical fields
for grouping. If you use numerical fields, the field values are bucketed using
an interval that you specify.

The second step is deciding how you want to aggregate the grouped data. When 
using aggregations, you practically ask questions about the index. There are 
different types of aggregations, each with its own purpose and output. To learn 
more about the supported aggregations and group-by fields, see 
{ref}/data-frame-transform-resource.html[{dataframe-transform-cap} resources].

As an optional step, you can also add a query to further limit the scope of the
aggregation.

The {dataframe-transform} performs a composite aggregation that 
paginates through all the data defined by the source index query. The output of
the aggregation is stored in a destination index. Each time the 
{dataframe-transform} queries the source index, it creates a _checkpoint_. You 
can decide whether you want the {dataframe-transform} to run once (batch 
{dataframe-transform}) or continuously ({cdataframe-transform}). A batch 
{dataframe-transform} is a single operation that has a single checkpoint. 
{cdataframe-transforms-cap} continually increment and process checkpoints as new 
source data is ingested.

.Example

Imagine that you run a webshop that sells clothes. Every order creates a document 
that contains a unique order ID, the name and the category of the ordered product, 
its price, the ordered quantity, the exact date of the order, and some customer 
information (name, gender, location, etc). Your dataset contains all the transactions 
from last year.

If you want to check the sales in the different categories in your last fiscal
year, define a {dataframe-transform} that groups the data by the product
categories (women's shoes, men's clothing, etc.) and the order date. Use the
last year as the interval for the order date. Then add a sum aggregation on the
ordered quantity. The result is a {dataframe} that shows the number of sold
items in every product category in the last year.

[role="screenshot"]
image::images/ml-dataframepivot.jpg["Example of a data frame pivot in {kib}"]

IMPORTANT: The {dataframe-transform-cap} leaves your source index intact. It
creates a new index that is dedicated to the {dataframe}.

--
=======
to one specific method: _{dataframe-transforms}_.
--
>>>>>>> [DOCS] Adds data frame transform overview
