tag::dfa-deploy-model[]
. To deploy {dfanalytics} model in a pipeline, navigate to  **Machine Learning** > 
**Model Management** > **Trained models** in the main menu, or use the {kibana-ref}/kibana-concepts-analysts.html#_finding_your_apps_and_objects[global search field] in {kib}.

. Find the model you want to deploy in the list and click **Deploy model** in 
the **Actions** menu.
+
--
[role="screenshot"]
image::images/ml-dfa-trained-models-ui.png["The trained models UI in {kib}"]
--

. Create an {infer} pipeline to be able to use the model against new data 
through the pipeline. Add a name and a description or use the default values.
+
--
[role="screenshot"]
image::images/ml-dfa-inference-pipeline.png["Creating an inference pipeline"]
--

. Configure the pipeline processors or use the default settings.
+
--
[role="screenshot"]
image::images/ml-dfa-inference-processor.png["Configuring an inference processor"]
--
. Configure to handle ingest failures or use the default settings. 

. (Optional) Test your pipeline by running a simulation of the pipeline to 
  confirm it produces the anticipated results.

. Review the settings and click **Create pipeline**.

The model is deployed and ready to use through the {infer} pipeline.
end::dfa-deploy-model[]


tag::dfa-evaluation-intro[]
Using the {dfanalytics} features to gain insights from a data set is an 
iterative process. After you defined the problem you want to solve, and chose 
the analytics type that can help you to do so, you need to produce a 
high-quality data set and create the appropriate {dfanalytics-job}. You might 
need to experiment with different configurations, parameters, and ways to 
transform data before you arrive at a result that satisfies your use case. A 
valuable companion to this process is the 
{ref}/evaluate-dfanalytics.html[{evaluatedf-api}], which enables you to evaluate 
the {dfanalytics} performance. It helps you understand error distributions and 
identifies the points where the {dfanalytics} model performs well or less 
trustworthily.

To evaluate the analysis with this API, you need to annotate your index that 
contains the results of the analysis with a field that marks each document with 
the ground truth. The {evaluatedf-api} evaluates the performance of the 
{dfanalytics} against this manually provided ground truth.
end::dfa-evaluation-intro[]

tag::dfa-inference[]
{infer-cap} enables you to use <<ml-trained-models,trained {ml} models>> against 
incoming data in a continuous fashion.

For instance, suppose you have an online service and you would like to predict 
whether a customer is likely to churn. You have an index with historical data – 
information on the customer behavior throughout the years in your business – and 
a {classification} model that is trained on this data. The new information comes 
into a destination index of a {ctransform}. With {infer}, you can perform the 
{classanalysis} against the new data with the same input fields that you've 
trained the model on, and get a prediction.
end::dfa-inference[]

tag::dfa-inference-processor[]
{infer-cap} can be used as a processor specified in an 
{ref}/ingest.html[ingest pipeline]. It uses a trained model to infer against
the data that is being ingested in the pipeline. The model is used on the ingest
node. {infer-cap} pre-processes the data by using the model and provides a
prediction. After the process, the pipeline continues executing (if there is any
other processor in the pipeline), finally the new data together with the results
are indexed into the destination index.

Check the {ref}/inference-processor.html[{infer} processor] and 
{ref}/ml-df-analytics-apis.html[the {ml} {dfanalytics} API documentation] to 
learn more.
end::dfa-inference-processor[]

tag::dfa-inference-aggregation[]
{infer-cap} can also be used as a pipeline aggregation. You can reference a 
trained model in the aggregation to infer on the result field of the parent
bucket aggregation. The {infer} aggregation uses the model on the results to
provide a prediction. This aggregation enables you to run {classification} or
{reganalysis} at search time. If you want to perform the analysis on a small set
of data, this aggregation enables you to generate predictions without the need
to set up a processor in the ingest pipeline.

Check the 
{ref}/search-aggregations-pipeline-inference-bucket-aggregation.html[{infer} bucket aggregation] 
and {ref}/ml-df-analytics-apis.html[the {ml} {dfanalytics} API documentation] to 
learn more.

NOTE: If you use trained model aliases to reference your trained model in an 
{infer} processor or {infer} aggregation, you can replace your trained model 
with a new one without the need of updating the processor or the aggregation. 
Reassign the alias you used to a new trained model ID by using the 
{ref}/put-trained-models-aliases.html[Create or update trained model aliases API].
The new trained model needs to use the same type of {dfanalytics} as the old 
one.
end::dfa-inference-aggregation[]

tag::dfa-model-jvm[]
NOTE: When you create a {dfanalytics-job}, the inference step of the process 
might fail if the model is too large to fit into JVM. For a workaround, refer 
to https://github.com/elastic/elasticsearch/issues/76093[this GitHub issue].
end::dfa-model-jvm[]

tag::dfa-phases[]
* `reindexing`: Documents are copied from the source index to the destination 
  index.
* `loading_data`: The job fetches the necessary data from the destination index.
* `feature_selection`: The process identifies the most relevant analyzed fields 
   for predicting the {depvar}.
* `coarse_parameter_search`: The process identifies initial values for undefined 
   hyperparameters.
* `fine_tuning_parameters`: The process identifies final values for undefined 
   hyperparameters. Refer to <<hyperparameters,hyperparameter optimization>>.
* `final_training`: The model training occurs.
* `writing_results`: The job matches the results with the data rows in the 
   destination index, merges them, and indexes them back to the destination 
   index.
* `inference`: The job validates the trained model against the test split of the 
   data set.
end::dfa-phases[]