[role="xpack"]
[testenv="platinum"]
[[ml-dfa-classification]]
= Predicting classes with {classification}

:frontmatter-description: An introduction to {ml} {classification}, which enables you to predict classes of data points in a data set.
:frontmatter-tags-products: [ml] 
:frontmatter-tags-content-type: [how-to] 
:frontmatter-tags-user-goals: [analyze]

{classification-cap} is a {ml} process that predicts the class or category of a 
data point in a data set. For a simple example, consider how the shapes in the 
following graph can be differentiated and classified as "circles" and 
"triangles":

image::images/classification-vis.png[alt="Classification process",align="center"]

In reality, {classification} problems are more complex, such as classifying 
malicious and benign domains to detect DGA activities for security reasons or 
predicting customer churn based on customer calling data. {classification-cap} 
is for predicting discrete, categorical values.

When you create a {classification} job, you must specify which field contains 
the classes that you want to predict. This field is known as the _{depvar}_. It
must contain no more than 30 classes. By default, all other
{ref}/put-dfanalytics.html#dfa-supported-fields[supported fields] are included
in the analysis and are known as _{feature-vars}_. You can optionally include or 
exclude fields. For more information about field selection, refer to the
{ref}/explain-dfanalytics.html[explain data frame analytics API].
  

[discrete]
[[dfa-classification-algorithm]]
== {classification-cap} algorithms

//tag::classification-algorithms[]
{classanalysis-cap} uses an ensemble algorithm that is a type of boosting called 
boosted tree regression model which combines multiple weak models into a 
composite one. It uses decision trees to learn to predict the probability that a 
data point belongs to a certain class. A sequence of decision trees are trained 
and every decision tree learns from the mistakes of the previous one. Every tree 
is an iteration of the last one, hence it improves the decision made by the 
previous tree.
//end::classification-algorithms[]

[discrete]
[[dfa-classification-problem]]
== 1. Define the problem

{classification-cap} can be useful in cases where discrete, categorical values 
needs to be predicted. If your use case requires predicting such values, then 
{classification} might be the suitable choice for you.

[discrete]
[[dfa-classification-environment]]
== 2. Set up the environment

Before you can use the {stack-ml-features}, there are some configuration
requirements (such as security privileges) that must be addressed. Refer to
<<setup>>.

[discrete]
[[dfa-classification-prepare-data]]
== 3. Prepare and transform data

{classification-cap} is a supervised {ml} method, which means you need to supply 
a labeled training data set. This data set must have values for the 
{feature-vars} and the {depvar} which are used to train the model. The training 
process uses this information to learn the releationships between the classes 
and the {feature-vars}. This labeled data set also plays a critical role in 
model evaluation.

If possible, prepare your input data such that it has less classes. A 
{classanalysis} with many classes takes more time to run than a binary 
{classification} job. The relationship between the number of classes and the 
runtime is roughly linear.

You might also need to {ref}/transforms.html[{transform}] your data to create a 
{dataframe} which can be used as the source for {classification}.

To learn more about how to prepeare your data, refer to 
<<prepare-transform-data, the relevant section>> of the supervised learning 
overview.

[discrete]
[[dfa-classification-create-job]]
== 4. Create a job

{dfanalytics-jobs-cap} contain the configuration information and metadata 
necessary to perform an analytics task. You can create {dfanalytics-jobs} via 
{kib} or using the {ref}/put-dfanalytics.html[create {dfanalytics-jobs} API]. 

Select {classification} as the analytics type, then select the field that you 
want to predict (the {depvar}). You can also include and exclude fields.

To improve performance, consider using a small `training_percent` value to train 
the model more quickly. It is a good strategy to make progress iteratively: run 
the analysis with a small training percentage, then evaluate the performance. 
Based on the results, you can decide if it is necessary to increase the 
`training_percent` value.

[discrete]
[[dfa-classification-start]]
== 5. Start the job

You can start the job via {kib} or using the 
{ref}/start-dfanalytics.html[start {dfanalytics-jobs}] API. A {classification} 
job has the following phases: 

include::ml-dfa-shared.asciidoc[tag=dfa-phases]

After the last phase is finished, the job stops and the results are ready for 
evaluation.

include::ml-dfa-shared.asciidoc[tag=dfa-model-jvm]

[discrete]
[[ml-dfanalytics-classification-evaluation]]
== 6. Evaluate and interpret the result

include::ml-dfa-shared.asciidoc[tag=dfa-evaluation-intro]

You can measure how well the model has performed on your training data set by 
using the `classification` evaluation type of the 
{ref}/evaluate-dfanalytics.html[evaluate {dfanalytics} API] or by viewing the 
job results in {kib}. The {classification} evaluation offers the following 
metrics to evaluate the model performance:

* Multiclass confusion matrix
* Area under the curve of receiver operating characteristic (AUC ROC) 

The following metrics helps you interpret the analysis results:

* {feat-imp}
* `class_probability`
* `class_score`


[discrete]
[[ml-dfanalytics-mccm]]
=== Multiclass confusion matrix

The multiclass confusion matrix provides a summary of the performance of the 
{classanalysis}. It contains the number of occurrences where the analysis
classified data points correctly with their actual class as well as the number
of occurrences where it misclassified them.

This is an example of a confusion matrix for a binary problem:

image::images/confusion-matrix-binary.jpg[alt="Confusion matrix of a binary problem",width="75%",align="center"]

It is a two by two matrix because there are only two classes (`true` and
`false`). It shows the proportion of data points that is correctly identified as 
members of a each class and the proportion that is misidentified.

As the number of classes increases, the confusion matrix becomes more complex:

image::images/confusion-matrix-multiclass.jpg[alt="Confusion matrix of a multiclass problem",width="100%",align="center"]

This matrix contains the actual labels on the left side while the predicted 
labels are on the top. The proportion of correct and incorrect predictions is 
broken down for each class. This enables you to examine how the {classanalysis}
confused the different classes while it made its predictions.


[discrete]
[[ml-dfanalytics-class-aucroc]]
=== Area under the curve of receiver operating characteristic (AUC ROC)

The receiver operating characteristic (ROC) curve is a plot that represents the 
performance of the {classification} process at different predicted probability 
thresholds. It compares the true positive rate for a specific class against the 
rate of all the other classes combined ("one versus all" strategy) at the 
different threshold levels to create the curve.

For example, there are three classes: `A`, `B`, and `C`, and AUC ROC is 
calculated for `A`. In this case, the number of correctly classified ++A++s 
(true positives) are compared to the number of ++B++s and ++C++s that are 
misclassified as ++A++s (false positives).

From this plot, you can compute the area under the curve (AUC) value, which is a 
number between 0 and 1. The higher the AUC, the better the model is at 
predicting ++A++s as ++A++s, in this case.

NOTE: To use this evaluation method, you must set `num_top_classes` to `-1`
or a value greater than or equal to the total number of classes when you create
the {dfanalytics-job}.


[discrete]
[[dfa-classification-feature-importance]]
=== {feat-imp-cap}

{feat-imp-cap} provides further information about the results of an analysis and 
helps to interpret the results in a more subtle way. If you want to learn more 
about {feat-imp}, refer to <<ml-feature-importance>>. 


[discrete]
[[dfa-classification-class-probability]]
=== `class_probability`

The `class_probability` is a value between 0 and 1, which indicates how likely
it is that a given data point belongs to a certain class. The higher the number,
the higher the probability that the data point belongs to the named class. This
information is stored in the `top_classes` array for each document in the 
destination index.


[discrete]
[[dfa-classification-class-score]]
=== `class_score`

The `class_score` is a function of the `class_probability` and has a value that
is greater than or equal to zero. It takes into consideration your objective (as
defined in the `class_assignment_objective` job configuration option):
_accuracy_ or _recall_.

If your objective is to maximize accuracy, the scores are weighted to maximize
the proportion of correct predictions in the training data set.

[role="screenshot"]
image::images/confusion-matrix-binary-accuracy.jpg[alt="A confusion matrix with the correct predictions highlighted",width="75%"]

TIP: If there is an imbalanced class distribution in your training data, 
focusing on accuracy can decrease your model's sensitivity to incorrect
predictions in the under-represented classes.

By default, {classanalysis} jobs accept a slight degradation of the overall
accuracy in return for greater sensitivity to classes that are predicted
incorrectly. That is to say, their objective is to maximize the minimum recall.
For example, in the context of a multi-class confusion matrix, the predictions
of interest are in each row:

[role="screenshot"]
image::images/confusion-matrix-multiclass-recall.jpg["A confusion matrix with a row highlighted"]

For each class, the recall is calculated as the number of correct predictions
divided by the sum of all the other predicted labels in that row. This value is 
represented as a percentage in each cell of the confusion matrix. The class 
scores are then weighted to favor predictions that result in the highest recall 
values across the training data. This objective typically performs better than 
accuracy when you have highly imbalanced data.

To learn more about choosing the class assignment objective that fits your goal, 
refer to this 
https://github.com/elastic/examples/blob/master/Machine%20Learning/Class%20Assigment%20Objectives/classification-class-assignment-objective.ipynb[Jupyter notebook].


[discrete]
[[dfa-classification-deploy]]
== 7. Deploy the model

The model that you created is stored as {es} documents in internal indices. In 
other words, the characteristics of your trained model are saved and ready to be 
deployed and used as functions.


[discrete]
[[ml-inference-class]]
=== {infer-cap}

include::ml-dfa-shared.asciidoc[tag=dfa-inference]


[discrete]
[[ml-inference-processor-class]]
==== {infer-cap} processor

include::ml-dfa-shared.asciidoc[tag=dfa-inference-processor]


[discrete]
[[ml-inference-aggregation-class]]
==== {infer-cap} aggregation

include::ml-dfa-shared.asciidoc[tag=dfa-inference-aggregation]


[discrete]
[[performing-classification]]
== Performing {classanalysis} in the sample flight data set

Let's try to predict whether a flight will be delayed or not by using the 
{kibana-ref}/get-started.html#gs-get-data-into-kibana[sample flight data]. The 
data set contains information such as weather conditions, carrier, flight 
distance, origin, destination, and whether or not the flight was delayed. The 
{classification} model learns the relationships between the fields in your data 
to predict the value of the _dependent variable_, which in this case is the 
boolean `FlightDelay` field.

TIP: If you want to view this example in a Jupyter notebook,
https://github.com/elastic/examples/tree/master/Machine%20Learning/Analytics%20Jupyter%20Notebooks[click here].


[discrete]
[[flightdata-classification-data]]
=== Preparing your data

Each document in the sample flight data set contains details for a single 
flight, so the data is ready for analysis; it is already in a two-dimensional
entity-based data structure. In general, you often need to
{ref}/transforms.html[transform] the data into an entity-centric index before
you can analyze it.

In order to be analyzed, a document must contain at least one field with a
supported data type (`numeric`, `boolean`, `text`, `keyword` or `ip`) and must
not contain arrays with more than one item. If your source data consists of some
documents that contain the dependent variable and some that do not, the model is
trained on the subset of documents that contain it.

.Example source document
[%collapsible]
====
```
{
  "_index": "kibana_sample_data_flights",
  "_type": "_doc",
  "_id": "S-JS1W0BJ7wufFIaPAHe",
  "_version": 1,
  "_seq_no": 3356,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "FlightNum": "N32FE9T",
    "DestCountry": "JP",
    "OriginWeather": "Thunder & Lightning",
    "OriginCityName": "Adelaide",
    "AvgTicketPrice": 499.08518599798685,
    "DistanceMiles": 4802.864932998549,
    "FlightDelay": false,
    "DestWeather": "Sunny",
    "Dest": "Chubu Centrair International Airport",
    "FlightDelayType": "No Delay",
    "OriginCountry": "AU",
    "dayOfWeek": 3,
    "DistanceKilometers": 7729.461862731618,
    "timestamp": "2019-10-17T11:12:29",
    "DestLocation": {
      "lat": "34.85839844",
      "lon": "136.8049927"
    },
    "DestAirportID": "NGO",
    "Carrier": "ES-Air",
    "Cancelled": false,
    "FlightTimeMin": 454.6742272195069,
    "Origin": "Adelaide International Airport",
    "OriginLocation": {
      "lat": "-34.945",
      "lon": "138.531006"
    },
    "DestRegion": "SE-BD",
    "OriginAirportID": "ADL",
    "OriginRegion": "SE-BD",
    "DestCityName": "Tokoname",
    "FlightTimeHour": 7.577903786991782,
    "FlightDelayMin": 0
  }
}
```
====

TIP: The sample flight data set is used in this example because it is easily
accessible. However, the data has been manually created and contains some
inconsistencies. For example, a flight can be both delayed and canceled. This is
a good reminder that the quality of your input data affects the quality of your
results.


[discrete]
[[flightdata-classification-model]]
=== Creating a {classification} model

To predict whether a specific flight is delayed:

. Create a {dfanalytics-job}.
+
--
You can use the wizard on the *{ml-app}* > *Data Frame Analytics* tab
in {kib} or the {ref}/put-dfanalytics.html[create {dfanalytics-jobs}] API.

[role="screenshot"]
image::images/flights-classification-job-1.jpg["Creating a {dfanalytics-job} in {kib}"]
--

.. Choose `kibana_sample_data_flights` as the source index.
.. Choose `classification` as the job type.
.. Choose `FlightDelay` as the dependent variable, which is the field that we
want to predict with the {classanalysis}.
.. Add `Cancelled`, `FlightDelayMin`, and `FlightDelayType` to the list of
excluded fields. It is recommended to exclude fields that either contain 
erroneous data or describe the `dependent_variable`.
+
--
The wizard includes a scatterplot matrix, which enables you to explore the 
relationships between the numeric fields. The color of each point is affected by
the value of the dependent variable for that document, as shown in the legend.
You can use this matrix to help you decide which fields to include or exclude.

[role="screenshot"]
image::images/flights-classification-scatterplot.png["A scatterplot matrix for three fields in {kib}"]

If you want these charts to represent data from a larger sample size or from a
randomized selection of documents, you can change the default behavior. However, 
a larger sample size might slow down the performance of the matrix and a
randomized selection might put more load on the cluster due to the more
intensive query.
--
.. Choose a training percent of `10` which means it randomly selects 10% of the
source data for training. While that value is low for this example, for many
large data sets using a small training sample greatly reduces runtime without 
impacting accuracy.
.. If you want to experiment with <<ml-feature-importance,{feat-imp}>>, specify
a value in the advanced configuration options. In this example, a maximum of 10 
{feat-imp} values per document will return. This option affects the speed of the 
analysis, so by default it is disabled. 
.. Use the default memory limit for the job. If the job requires more than this 
amount of memory, it fails to start. If the available memory on the node is
limited, this setting makes it possible to prevent job execution.
.. Add a job ID (such as `model-flight-delays-classification`) and optionally a
job description.
.. Add the name of the destination index that will contain the results. In 
{kib}, the index name matches the job ID by default. It will contain a copy of 
the source index data where each document is annotated with the results. If the 
index does not exist, it will be created automatically.
.. Use default values for all other options.
+
--
.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
PUT _ml/data_frame/analytics/model-flight-delays-classification
{
  "source": {
    "index": [
      "kibana_sample_data_flights"
    ]
  },
  "dest": {
    "index": "model-flight-delays-classification",
    "results_field": "ml" <1>
  },
  "analysis": {
    "classification": {
      "dependent_variable": "FlightDelay",
      "training_percent": 10,
      "num_top_feature_importance_values": 10 <2>
    }
  },
  "analyzed_fields": {
    "includes": [],
    "excludes": [
      "Cancelled",
      "FlightDelayMin",
      "FlightDelayType"
    ]
  }
}
--------------------------------------------------
// TEST[skip:setup kibana sample data]
<1> The field name in the `dest` index that contains the analysis results.
<2> To disable {feat-imp} calculations, omit this option. 
====
After you configured your job, the configuration details are automatically 
validated. If the checks are successful, you can start the job. A warning 
message is shown if the configuration is invalid. The message contains a 
suggestion to improve the configuration to be validated.
--

. Start the job in {kib} or use the
{ref}/start-dfanalytics.html[start {dfanalytics-jobs}] API.
+
--
The job takes a few minutes to run. Runtime depends on the local hardware and 
also on the number of documents and fields that are analyzed. The more fields 
and documents, the longer the job runs. It stops automatically when the analysis 
is complete.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
POST _ml/data_frame/analytics/model-flight-delays-classification/_start
--------------------------------------------------
// TEST[skip:TBD]
====
--

. Check the job stats to follow the progress in {kib} or use the 
{ref}/get-dfanalytics-stats.html[get {dfanalytics-jobs} statistics API].
+
--
[role="screenshot"]
image::images/flights-classification-details.png["Statistics for a {dfanalytics-job} in {kib}"]

When the job stops, the results are ready to view and evaluate. To learn more
about the job phases, see <<ml-dfa-phases>>.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
GET _ml/data_frame/analytics/model-flight-delays-classification/_stats
--------------------------------------------------
// TEST[skip:TBD]


The API call returns the following response: 

[source,console-result]
--------------------------------------------------  
{
  "count" : 1,
  "data_frame_analytics" : [
    {
      "id" : "model-flight-delays-classification",
      "state" : "stopped",
      "progress" : [
        {
          "phase" : "reindexing",
          "progress_percent" : 100
        },
        {
          "phase" : "loading_data",
          "progress_percent" : 100
        },
        {
          "phase" : "feature_selection",
          "progress_percent" : 100
        },
        {
          "phase" : "coarse_parameter_search",
          "progress_percent" : 100
        },
        {
          "phase" : "fine_tuning_parameters",
          "progress_percent" : 100
        },
        {
          "phase" : "final_training",
          "progress_percent" : 100
        },
        {
          "phase" : "writing_results",
          "progress_percent" : 100
        },
        {
          "phase" : "inference",
          "progress_percent" : 100
        }
      ],
      "data_counts" : {
        "training_docs_count" : 1305,
        "test_docs_count" : 11754,
        "skipped_docs_count" : 0
      },
      "memory_usage" : {
        "timestamp" : 1597182490577,
        "peak_usage_bytes" : 316613,
        "status" : "ok"
      },
      "analysis_stats" : {
        "classification_stats" : {
          "timestamp" : 1601405047110,
          "iteration" : 18,
          "hyperparameters" : {
            "class_assignment_objective" : "maximize_minimum_recall",
            "alpha" : 0.7633136599817167,
            "downsample_factor" : 0.9473152348018332,
            "eta" : 0.02331774683318904,
            "eta_growth_rate_per_tree" : 1.0143154178910303,
            "feature_bag_fraction" : 0.5504020748926737,
            "gamma" : 0.26389161802240446,
            "lambda" : 0.6309726978583623,
            "max_attempts_to_add_tree" : 3,
            "max_optimization_rounds_per_hyperparameter" : 2,
            "max_trees" : 894,
            "num_folds" : 5,
            "num_splits_per_feature" : 75,
            "soft_tree_depth_limit" : 4.672705943455812,
            "soft_tree_depth_tolerance" : 0.13448633124842999
            },
            "timing_stats" : {
              "elapsed_time" : 76459,
              "iteration_time" : 1861
            },
            "validation_loss" : {
              "loss_type" : "binomial_logistic"
            }
          }
      }
    }
  ]
}
--------------------------------------------------
====
--


[discrete]
[[flightdata-classification-results]]
=== Viewing {classification} results

Now you have a new index that contains a copy of your source data with 
predictions for your {depvar}.

When you view the {classification} results in {kib}, it shows the contents of
the destination index in a tabular format. It also provides information about
the analysis details, model evaluation metrics, total {feat-imp} values, and a
scatterplot matrix.

[role="screenshot"]
image::images/flights-classification-results.jpg["Destination index table for a classification job in {kib}"]

The table shows a column for the {depvar} (`FlightDelay`), which contains the 
ground truth values that you are trying to predict. It also shows a column for 
the predicted values (`ml.FlightDelay_prediction`), which were generated by the 
{classanalysis}. The `ml.is_training` column indicates whether the document was 
used in the training or testing data set. You can use the *Training* and 
*Testing* filter options to refine the contents of the results table. You can 
also enable histogram charts to get a better understanding of the distribution 
of values.

If you want to understand how certain the model is about each prediction, you
can examine its probability and score (`ml.prediction_probability` and
`ml.prediction_score`). The higher these values are, the more confident the
model is that the data point belongs to the named class. If you examine the
destination index more closely in the *Discover* app in {kib} or use the
standard {es} search command, you can see that the analysis predicts the
probability of all possible classes for the {depvar}. The `top_classes` object 
contains the predicted classes with the highest scores.

TIP: If you have a large number of classes, your destination index contains a
large number of predicted probabilities for each document. When you create the
{classification} job, you can use the `num_top_classes` option to modify this
behavior.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
GET model-flight-delays-classification/_search
--------------------------------------------------
// TEST[skip:TBD]

The snippet below shows the probability and score details for a document in the
destination index:

[source,console-result]
--------------------------------------------------  
          ...
          "FlightDelay" : false,
          ...
          "ml" : {
            "FlightDelay_prediction" : false,
            "top_classes" : [ <1>
              {
                "class_name" : false,
                "class_probability" : 0.9427605087816684,
                "class_score" : 0.3462468700158476
              },
              {
                "class_name" : true,
                "class_probability" : 0.057239491218331606,
                "class_score" : 0.057239491218331606
              }
            ],
            "prediction_probability" : 0.9427605087816684,
            "prediction_score" : 0.3462468700158476,
            ...
--------------------------------------------------
<1> An array of values specifying the probability of the prediction and the 
score for each class.

The class with the highest score is the prediction. In this example, `false` has
a `class_score` of 0.35 while `true` has only 0.06, so the prediction will be
`false`. For more details about these values, see
<<dfa-classification-class-score>>.

====

If you chose to calculate {feat-imp}, the destination index also contains
`ml.feature_importance` objects. Every field that is included in the
analysis (known as a _feature_ of the data point) is assigned a {feat-imp} 
value. It has both a magnitude and a direction (positive or negative), which 
indicates how each field affects a particular prediction. Only the most
significant values (in this case, the top 10) are stored in the index. However,
the trained model metadata also contains the average magnitude of the {feat-imp}
values for each field across all the training data. You can view this
summarized information in {kib}:

[role="screenshot"]
image::images/flights-classification-total-importance.jpg["Total {feat-imp} values in {kib}"]

You can also see the {feat-imp} values for each individual prediction in the
form of a decision plot:

[role="screenshot"]
image::images/flights-classification-importance.png["A decision plot for {feat-imp} values in {kib}"]

In {kib}, the decision path shows the relative impact of each feature on the
probability of the prediction. The features with the most significant positive
or negative impact appear at the top of the decision plot. Thus in this example,
the features related to flight time and distance had the most significant
influence on the probability value for this prediction. This type of information
can help you to understand how models arrive at their predictions. It can also
indicate which aspects of your data set are most influential or least useful
when you are training and tuning your model.

If you do not use {kib}, you can see the summarized {feat-imp} values by using
the {ref}/get-inference.html[get trained model API] and the individual values by
searching the destination index.

.API example
[%collapsible]
====
[source,console]
--------------------------------------------------
GET _ml/trained_models/model-flight-delays-classification*?include=total_feature_importance
--------------------------------------------------
// TEST[skip:TBD]

The snippet below shows an example of the total {feat-imp} and the corresponding baseline
in the trained model metadata:

[source,console-result]
--------------------------------------------------
{
  "count" : 1,
  "trained_model_configs" : [
    {
      "model_id" : "model-flight-delays-classification-1601405047985",
      ...
      "metadata" : {
        ...
        "feature_importance_baseline" : { <1>
          "classes" : [
            {
              "class_name" : true,
              "baseline" : -1.5869016940485443
            },
            {
              "class_name" : false,
              "baseline" : 1.5869016940485443 
            }
          ]
        },
        "total_feature_importance" : [
          {
            "feature_name" : "dayOfWeek",
            "classes" : [
              {
                "class_name" : false,
                "importance" : {
                  "mean_magnitude" : 0.037513174351966404, <2>
                  "min" : -0.20132653028125566, <3>
                  "max" : 0.20132653028125566 <4>
                }
              },
              {
                "class_name" : true,
                "importance" : {
                  "mean_magnitude" : 0.037513174351966404,
                  "min" : -0.20132653028125566,
                  "max" : 0.20132653028125566
                }
              }
            ]
          },
          {
            "feature_name" : "OriginWeather",
            "classes" : [
              {
                "class_name" : false,
                "importance" : {
                  "mean_magnitude" : 0.05486662317369895,
                  "min" : -0.3337477336556598,
                  "max" : 0.3337477336556598
                }
              },
              {
                "class_name" : true,
                "importance" : {
                  "mean_magnitude" : 0.05486662317369895,
                  "min" : -0.3337477336556598,
                  "max" : 0.3337477336556598
                }
              }
            ]
          },
          ...
--------------------------------------------------
<1> This object contains the baselines that are used to calculate the {feat-imp}
decision paths in {kib}.
<2> This value is the average of the absolute {feat-imp} values for the
`dayOfWeek` field across all the training data when the predicted class is
`false`.
<3> This value is the minimum {feat-imp} value across all the training data for
this field when the predicted class is `false`.
<4> This value is the maximum {feat-imp} value across all the training data for
this field when the predicted class is `false`.

To see the top {feat-imp} values for each prediction, search the destination
index. For example:

[source,console]
--------------------------------------------------
GET model-flight-delays-classification/_search
--------------------------------------------------
// TEST[skip:TBD]

The snippet below shows an example of the {feat-imp} details for a document in
the search results:

[source,console-result]
--------------------------------------------------  
          ...
          "FlightDelay" : false,
          ...
          "ml" : {
            "FlightDelay_prediction" : false,
            ...
            "prediction_probability" : 0.9427605087816684,
            "prediction_score" : 0.3462468700158476,
            "feature_importance" : [
              {
                "feature_name" : "DistanceMiles",
                "classes" : [
                  {
                    "class_name" : false,
                    "importance" : -1.4766536146534828
                  },
                  {
                    "class_name" : true,
                    "importance" : 1.4766536146534828
                  }
                ]
              },
              {
                "feature_name" : "FlightTimeMin",
                "classes" : [
                  {
                    "class_name" : false,
                    "importance" : 1.0919201754729184
                  },
                  {
                    "class_name" : true,
                    "importance" : -1.0919201754729184
                  }
                ]
              },
              ...
--------------------------------------------------

The sum of the {feat-imp} values for each class in this data point approximates
the logarithm of its odds.

====

Lastly, {kib} provides a scatterplot matrix in the results. It has the same
functionality as the matrix that you saw in the job wizard. Its purpose is to
help you visualize and explore the relationships between the numeric fields and 
the {depvar}.


[discrete]
[[flightdata-classification-evaluate]]
=== Evaluating {classification} results

Though you can look at individual results and compare the predicted value
(`ml.FlightDelay_prediction`) to the actual value (`FlightDelay`), you
typically need to evaluate the success of your {classification} model as a
whole.

{kib} provides a _normalized confusion matrix_ that contains the percentage of
occurrences where the analysis classified data points correctly with their
actual class and the percentage of occurrences where it misclassified them.

[role="screenshot"]
image::images/flights-classification-evaluation.png["Evaluation of a classification job in {kib}"]

NOTE: As the sample data may change when it is loaded into {kib}, the results of 
the analysis can vary even if you use the same configuration as the example. 
Therefore, use this information as a guideline for interpreting your own 
results.

If you want to see the exact number of occurrences, select a quadrant in the
matrix. You can also use the *Training* and *Testing* filter options to refine
the contents of the matrix. Thus you can see how well the model performs on
previously unseen data. In this example, there are 2952 documents in the testing
data that have the `true` class. 794 of them are
predicted as `false`; this is called a _false negative_. 2158 are predicted
correctly as `true`; this is called a _true positive_. The confusion matrix 
shows us that 27% of the actual `true` values were correctly predicted and 73% 
were incorrectly predicted in the test data.

Likewise if you select other quadrants in the matrix, it shows the number of
documents that have the `false` class as their actual value in the testing data. 
In this example, the model labeled 7262 documents out of 8802 correctly as
`false`; this is called a _true negative_. 1540 documents are predicted
incorrectly as `true`; this is called a _false positive_. Thus 83% of the actual
`false` values were correctly predicted and 17% were incorrectly predicted in
the test data. When you perform {classanalysis} on your own data, it might take 
multiple iterations before you are satisfied with the results and ready to 
deploy the model.

{kib} also provides the _receiver operating characteristic (ROC) curve_ as part 
of the model evaluation. The plot compares the true positive rate (y-axis) to 
the false positive rate (x-axis) for each class; in this example, `true` and 
`false`. From this plot, the area under the curve (AUC) value is computed. It is 
a number between 0 and 1. The higher the AUC, the better the model is at 
predicting the classes correctly.

[role="screenshot"]
image::images/flights-classification-roc-curve.jpg["Evaluation of a classification job in {kib} – ROC curve"]

You can also generate these metrics with the
{ref}/evaluate-dfanalytics.html[{dfanalytics} evaluate API]. For more
information about interpreting the evaluation metrics, see
<<ml-dfanalytics-classification-evaluation>>.

.API example
[%collapsible]
====
First, we want to know the training error that represents how well the model
performed on the training data set.

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
 "index": "model-flight-delays-classification",
   "query": {
    "term": {
      "ml.is_training": {
        "value": true  <1>
      }
    }
  },
 "evaluation": {
   "classification": {
     "actual_field": "FlightDelay",
     "predicted_field": "ml.FlightDelay_prediction",
     "metrics": {  
       "multiclass_confusion_matrix" : {}
     }
   }
 }
}
--------------------------------------------------
// TEST[skip:TBD]
<1> We calculate the training error by evaluating only the training data.

Next, we calculate the generalization error that represents how well the model 
performed on previously unseen data:

[source,console]
--------------------------------------------------
POST _ml/data_frame/_evaluate
{
 "index": "model-flight-delays-classification",
   "query": {
    "term": {
      "ml.is_training": {
        "value": false  <1>
      }
    }
  },
 "evaluation": {
   "classification": {
     "actual_field": "FlightDelay",
     "predicted_field": "ml.FlightDelay_prediction",
     "metrics": {  
       "multiclass_confusion_matrix" : {}
     }
   }
 }
}
--------------------------------------------------
// TEST[skip:TBD]
<1> We evaluate only the documents that are not part of the training data.

The returned confusion matrix shows us how many data points were classified 
correctly (where the `actual_class` matches the `predicted_class`) and how many 
were misclassified (`actual_class` does not match `predicted_class`):

[source,console-result]
--------------------------------------------------
{
  "classification" : {
    "multiclass_confusion_matrix" : {
      "confusion_matrix" : [
        {
          "actual_class" : "false", <1>
          "actual_class_doc_count" : 8802, <2>
          "predicted_classes" : [
            {
              "predicted_class" : "false", <3>
              "count" : 7262 <4>
            },
            {
              "predicted_class" : "true",
              "count" : 1540
            }
          ],
          "other_predicted_class_doc_count" : 0
        },
        {
          "actual_class" : "true",
          "actual_class_doc_count" : 2952,
          "predicted_classes" : [
            {
              "predicted_class" : "false",
              "count" : 794
            },
            {
              "predicted_class" : "true",
              "count" : 2158
            }
          ],
          "other_predicted_class_doc_count" : 0
        }
      ],
      "other_actual_class_count" : 0
    }
  }
}
--------------------------------------------------
<1> The name of the actual class. In this example, there are two actual classes: 
`true` and `false`.
<2> The number of documents in the data set that belong to the actual class.
<3> The name of the predicted class.
<4> The number of documents that belong to the actual class and are labeled as
the predicted class.
====

If you don't want to keep the {dfanalytics-job}, you can delete it in {kib} or
by using the {ref}/delete-dfanalytics.html[delete {dfanalytics-job} API]. When
you delete {dfanalytics-jobs} in {kib}, you have the option to also remove the 
destination indices and {data-sources}.


[discrete]
[[dfa-classification-readings]]
=== Further readings

* https://github.com/elastic/examples/tree/master/Machine%20Learning/Analytics%20Jupyter%20Notebooks[{classanalysis-cap} example (Jupyter notebook)]
* https://www.elastic.co/blog/benchmarking-binary-classification-results-in-elastic-machine-learning[Benchmarking binary {classification} results in Elastic {ml}]
* https://www.elastic.co/blog/using-elastic-supervised-machine-learning-for-binary-classification[Using Elastic supervised {ml} for binary {classification}]
* https://www.elastic.co/blog/machine-learning-in-cybersecurity-training-supervised-models-to-detect-dga-activity[{ml-cap} in cybersecurity – part 1: Training supervised models to detect DGA activity]
* https://www.elastic.co/blog/machine-learning-in-cybersecurity-detecting-dga-activity-in-network-data[{ml-cap} in cybersecurity – part 2: Detecting DGA activity in network data]
* https://www.elastic.co/blog/supervised-and-unsupervised-machine-learning-for-dga-detection[Combining supervised and unsupervised machine learning for DGA detection]