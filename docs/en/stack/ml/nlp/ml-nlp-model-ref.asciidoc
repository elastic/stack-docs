[[ml-nlp-model-ref]]
= Compatible third party NLP models
++++
<titleabbrev>Compatible third party models</titleabbrev>
++++

:frontmatter-description: The list of compatible third party NLP models.
:frontmatter-tags-products: [ml] 
:frontmatter-tags-content-type: [reference] 
:frontmatter-tags-user-goals: [analyze]

The minimum dedicated ML node size for deploying and using the ELSER model 
is 16 GB in Elasticsearch Service if 
{cloud}/ec-autoscaling.html[deployment autoscaling] is turned off. Turning on 
autoscaling is recommended because it allows your deployment to dynamically 
adjust resources based on demand. Better performance can be achieved by using 
more allocations or more threads per allocation, which requires bigger ML nodes. 
Autoscaling provides bigger nodes when required. If autoscaling is turned off, 
you must provide suitably sized nodes yourself.

The {stack-ml-features} support transformer models that conform to the standard
BERT model interface and use the WordPiece tokenization algorithm.

The current list of supported architectures is:

* BERT
* BART
* DPR bi-encoders
* DistilBERT
* ELECTRA
* MobileBERT
* RoBERTa
* RetriBERT
* MPNet
* SentenceTransformers bi-encoders with the above transformer architectures

In general, any trained model that has a supported architecture is deployable in
{es} by using eland. However, it is not possible to test every third party
model. The following lists are therefore provided for informational purposes
only and may not be current. Elastic makes no warranty or assurance that the
{ml-features} will continue to interoperate with these third party models in the
way described, or at all.

These models are listed by NLP task; for more information about those tasks,
refer to <<ml-nlp-overview>>.

**Models highlighted in bold** in the list below are recommended for evaluation 
purposes and to get started with the Elastic {nlp} features. 


[discrete]
[[ml-nlp-model-ref-mask]]
== Third party fill-mask models

* https://huggingface.co/bert-base-uncased[BERT base model]
* https://huggingface.co/distilroberta-base[DistilRoBERTa base model]
* https://huggingface.co/microsoft/mpnet-base[MPNet base model]
* https://huggingface.co/roberta-large[RoBERTa large model]

[discrete]
[[ml-nlp-model-ref-ner]]
== Third party named entity recognition models

* https://huggingface.co/dslim/bert-base-NER[BERT base NER]
* https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english[**DistilBERT base cased finetuned conll03 English**]
* https://huggingface.co/philschmid/distilroberta-base-ner-conll2003[DistilRoBERTa base NER conll2003]
* https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english[**DistilBERT base uncased finetuned conll03 English**]
* https://huggingface.co/HooshvareLab/distilbert-fa-zwnj-base-ner[DistilBERT fa zwnj base NER]

[discrete]
[[ml-nlp-model-ref-question-answering]]
== Third party question answering models

* https://huggingface.co/sentence-transformers/all-mpnet-base-v2[**All MPNet base v2**]
* https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad[BERT large model (uncased) whole word masking finetuned on SQuAD]
* https://huggingface.co/distilbert-base-cased-distilled-squad[DistilBERT base cased distilled SQuAD]
* https://huggingface.co/deepset/electra-base-squad2[Electra base squad2]
* https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1[**Multi QA MPNet base cos v1**]
* https://huggingface.co/deepset/tinyroberta-squad2[TinyRoBERTa squad2]


[discrete]
[[ml-nlp-model-ref-text-embedding]]
== Third party text embedding models

Text Embedding models are designed to work with specific scoring functions
for calculating the similarity between the embeddings they produce. 
Examples of typical scoring functions are: `cosine`, `dot product` and 
`euclidean distance` (also known as `l2_norm`).

The embeddings produced by these models should be indexed in {es} using the
{ref}/dense-vector.html[dense vector field type]
with an appropriate 
{ref}/dense-vector.html#dense-vector-params[similarity function] chosen for the 
model. 

To find similar embeddings in {es} use the efficient 
{ref}/knn-search.html#approximate-knn[Approximate k-nearest neighbor (kNN)]
search API with a text embedding as the query vector. Approximate kNN search 
uses the similarity function defined in the dense vector field mapping is used 
to calculate the relevance. For the best results the function must be one of 
the suitable similarity functions for the model.


Using `SentenceTransformerWrapper`:

* https://huggingface.co/sentence-transformers/all-distilroberta-v1[All DistilRoBERTa v1]
Suitable similarity functions:	`dot_product`, `cosine`, `l2_norm`
* https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2[All MiniLM L12 v2]
Suitable similarity functions:	`dot_product`, `cosine`, `l2_norm`
* https://huggingface.co/sentence-transformers/all-mpnet-base-v2[**All MPNet base v2**]
Suitable similarity functions:	`dot_product`, `cosine`, `l2_norm`
* https://huggingface.co/sentence-transformers/facebook-dpr-ctx_encoder-multiset-base[Facebook dpr-ctx_encoder multiset base]
Suitable similarity functions:	`dot_product`
* https://huggingface.co/sentence-transformers/facebook-dpr-question_encoder-single-nq-base[Facebook dpr-question_encoder single nq base]
Suitable similarity functions:	`dot_product`
* https://huggingface.co/sentence-transformers/LaBSE[LaBSE]
Suitable similarity functions:	`cosine`
* https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b[msmarco DistilBERT base tas b]
Suitable similarity functions:	`dot_product`
* https://huggingface.co/sentence-transformers/msmarco-MiniLM-L12-cos-v5[msmarco MiniLM L12 v5]
Suitable similarity functions:	`dot_product`, `cosine`, `l2_norm`
* https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2[paraphrase mpnet base v2]
Suitable similarity functions:	`cosine`

Using `DPREncoderWrapper`:

* https://huggingface.co/castorini/ance-dpr-context-multi[ance dpr-context multi]
* https://huggingface.co/castorini/ance-dpr-question-multi[ance dpr-question multi]
* https://huggingface.co/castorini/bpr-nq-ctx-encoder[bpr nq-ctx-encoder]
* https://huggingface.co/castorini/bpr-nq-question-encoder[bpr nq-question-encoder]
* https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base[dpr-ctx_encoder single nq base]
* https://huggingface.co/facebook/dpr-ctx_encoder-multiset-base[dpr-ctx_encoder multiset base]
* https://huggingface.co/facebook/dpr-question_encoder-single-nq-base[dpr-question_encoder single nq base]
* https://huggingface.co/facebook/dpr-question_encoder-multiset-base[dpr-question_encoder multiset base]


[discrete]
[[ml-nlp-model-ref-text-classification]]
=== Third party text classification models

* https://huggingface.co/nateraw/bert-base-uncased-emotion[BERT base uncased emotion]
* https://huggingface.co/Hate-speech-CNERG/dehatebert-mono-english[DehateBERT mono english]
* https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion[DistilBERT base uncased emotion]
* https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english[DistilBERT base uncased finetuned SST-2]
* https://huggingface.co/ProsusAI/finbert[FinBERT]
* https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment[Twitter roBERTa base for Sentiment Analysis]


[discrete]
[[ml-nlp-model-ref-zero-shot]]
== Third party zero-shot text classification models

* https://huggingface.co/facebook/bart-large-mnli[BART large mnli]
* https://huggingface.co/typeform/distilbert-base-uncased-mnli[DistilBERT base model (uncased)]
* https://huggingface.co/valhalla/distilbart-mnli-12-6[**DistilBart MNLI**]
* https://huggingface.co/typeform/mobilebert-uncased-mnli[MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices]
* https://huggingface.co/cross-encoder/nli-distilroberta-base[NLI DistilRoBERTa base]
* https://huggingface.co/cross-encoder/nli-roberta-base[NLI RoBERTa base]
* https://huggingface.co/typeform/squeezebert-mnli[SqueezeBERT]


[discrete]
== Expected model output

Models used for each NLP task type must output tensors of a specific format to 
be used in the Elasticsearch NLP pipelines.

Here are the expected outputs for each task type.


[discrete]
=== Fill mask expected model output

Fill mask is a specific kind of token classification; it is the base training 
task of many transformer models.

For the Elastic stack's fill mask NLP task to understand the model output, it 
must have a specific format. It needs to
be a float tensor with 
`shape(<number of sequences>, <number of tokens>, <vocab size>)`.

Here is an example with a single sequence `"The capital of [MASK] is Paris"` and 
with vocabulary `["The", "capital", "of", "is", "Paris", "France", "[MASK]"]`.

Should output:

[source]
----
 [
   [
     [ 0, 0, 0, 0, 0, 0, 0 ], // The
     [ 0, 0, 0, 0, 0, 0, 0 ], // capital
     [ 0, 0, 0, 0, 0, 0, 0 ], // of
     [ 0.01, 0.01, 0.3, 0.01, 0.2, 1.2, 0.1 ], // [MASK]
     [ 0, 0, 0, 0, 0, 0, 0 ], // is
     [ 0, 0, 0, 0, 0, 0, 0 ] // Paris
   ] 
]
----

The predicted value here for `[MASK]` is `"France"` with a score of 1.2.

[discrete]
=== Named entity recognition expected model output

Named entity recognition is a specific token classification task. Each token in 
the sequence is scored related to a specific set of classification labels. For 
the Elastic Stack, we use Inside-Outside-Beginning (IOB) tagging. Elastic supports any NER entities
as long as they are IOB tagged. The default values are:
"O", "B_MISC", "I_MISC", "B_PER", "I_PER", "B_ORG", "I_ORG", "B_LOC", "I_LOC".

The `"O"` entity label indicates that the current token is outside any entity.
`"I"` indicates that the token is inside an entity.
`"B"` indicates the beginning of an entity.
`"MISC"` is a miscellaneous entity.
`"LOC"` is a location.
`"PER"` is a person.
`"ORG"` is an organization.

The response format must be a float tensor with 
`shape(<number of sequences>, <number of tokens>, <number of classification labels>)`.

Here is an example with a single sequence `"Waldo is in Paris"`:

[source]
----
 [
   [
//    "O", "B_MISC", "I_MISC", "B_PER", "I_PER", "B_ORG", "I_ORG", "B_LOC", "I_LOC"
     [ 0,  0,         0,       0.4,     0.5,     0,       0.1,     0,       0 ], // Waldo 
     [ 1,  0,         0,       0,       0,       0,       0,       0,       0 ], // is
     [ 1,  0,         0,       0,       0,       0,       0,       0,       0 ], // in
     [ 0,  0,         0,       0,       0,       0,       0,       0,       1.0 ] // Paris
   ] 
]
----


[discrete]
=== Text embedding expected model output

Text embedding allows for semantic embedding of text for dense information 
retrieval.

The output of the model must be the specific embedding directly without any 
additional pooling.

Eland does this wrapping for the aforementioned models. But if supplying your 
own, the model must output the embedding for each inferred sequence.


[discrete]
=== Text classification expected model output

With text classification (for example, in tasks like sentiment analysis), the 
entire sequence is classified. The output of the model must be a float tensor 
with `shape(<number of sequences>, <number of classification labels>)`.

Here is an example with two sequences for a binary classification model of 
"happy" and "sad":

[source]
----
 [
   [
//     happy, sad
     [ 0,     1], // first sequence 
     [ 1,     0] // second sequence
   ] 
]
----


[discrete]
=== Zero-shot text classification expected model output

Zero-shot text classification allows text to be classified for arbitrary labels 
not necessarily part of the original training. Each sequence is combined with 
the label given some hypothesis template. The model then scores each of these
combinations according to `[entailment, neutral, contradiction]`. The output of 
the model must be a float tensor with 
`shape(<number of sequences>, <number of labels>, 3)`.

Here is an example with a single sequence classified against 4 labels:

[source]
----
 [
   [
//     entailment, neutral, contradiction
     [ 0.5,        0.1,     0.4], // first label 
     [ 0,          0,       1], // second label 
     [ 1,          0,       0], // third label 
     [ 0.7,        0.2,     0.1] // fourth label
   ] 
]
----
