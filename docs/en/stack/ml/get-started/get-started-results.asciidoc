[role="xpack"]
[[ml-gs-results]]
=== View the sample web log job results

After the {dfeeds} are started and the {anomaly-jobs} have processed some data,
you can view the results in {kib}.

TIP: Depending on the capacity of your machine, you might need to wait a few
seconds for the {ml} analysis to generate initial results.

[role="screenshot"]
image::images/ml-gs-web-results.jpg["Create jobs for the sample web logs"]

The {ml-features} analyze the input stream of data, model its behavior, and
perform analysis based on the detectors in each job. When an event occurs
outside of the model, that event is identified as an anomaly. You can
immediately see that all three jobs have found anomalies, which are indicated by
red blocks in the swim lanes for each job.

There are two tools for examining the results from {anomaly-jobs} in {kib}: the
**Anomaly Explorer** and the **Single Metric Viewer**.

[discrete]
[[ml-gs-results-smv]]
=== Explore results in the Single Metric Viewer

One of the sample jobs (`low_request_rate`), is a _single metric job_; it has a
single detector and limited job properties. Let's start by looking at this
simple job in the **Single Metric Viewer**:

[role="screenshot"]
image::images/ml-gs-job1-analysis.jpg["Single Metric Viewer for low_request_rate job"]

This view contains a chart that represents the actual and expected values over
time. It is available only if the job has `model_plot_config` enabled. It can
display only a single time series.

The blue line in the chart represents the actual data values. The shaded blue
area represents the bounds for the expected values. The area between the upper
and lower bounds are the most likely values for the model. If a value is outside
of this area then it can be said to be anomalous.

If you slide the time selector from the beginning to the end of the data, you
can see how the model improves as it processes more data. At the beginning, the
expected range of values is pretty broad and the model is not capturing the
periodicity in the data. But it quickly learns and begins to reflect the
patterns in your data.

.Anomaly scores
****
Any data points outside the range that was predicted by the model are marked
as anomalies. When you have high volumes of real-life data, your {anomaly-job}
might find many anomalies. These vary in probability from very likely to highly
unlikely--that is to say, from not particularly anomalous to highly anomalous.
There can be none, one, or hundreds of anomalies found within each bucket. There
can be many thousands found per job. In order to provide a sensible view of the
results, an _anomaly score_ is calculated for each bucket time interval. The
anomaly score is a value from 0 to 100, which indicates the significance of the
observed anomaly compared to previously seen anomalies. The highly anomalous
values are shown in red and the low scored values are indicated in blue. An
interval with a high anomaly score is significant and requires investigation.
****

Slide the time selector to a section of the time series that contains a red
anomaly data point. If you hover over the point, you can see more information.
You can also see details in the **Anomalies** section of the viewer. For example:

[role="screenshot"]
image::images/ml-gs-job1-anomalies.jpg["Single Metric Viewer Anomalies for low_request_rate job"]

For each anomaly you can see key details such as the time, the actual and
expected ("typical") values, and their probability.

By default, the table contains all anomalies that have a severity of "warning"
or higher in the selected section of the timeline. If you are only interested in
critical anomalies, for example, you can change the severity threshold for this
table.

The anomalies table also automatically calculates an interval for the data in
the table. If the time difference between the earliest and latest records in the
table is less than two days, the data is aggregated by hour to show the details
of the highest severity anomaly for each detector. Otherwise, it is aggregated
by day. You can change the interval for the table, for example, to show all
anomalies.

After you have identified anomalies, often the next step is to try to determine
the context of those situations. For example, are there other factors that are
contributing to the problem? Are the anomalies confined to particular
applications or servers? You can begin to troubleshoot these situations by
layering additional jobs or creating multi-metric jobs.

[discrete]
[[ml-gs-results-ae]]
=== Explore results in the Anomaly Explorer

The `low_request_rate` job has a single detector that uses the `low_count`
function. You might also want to track other metrics like average response time
or the maximum number of denied requests. Instead of creating jobs for each of
those metrics, you can combine them in _multi-metric jobs_.

You can also use multi-metric jobs to split a single time series into multiple
time series based on a categorical field. For example, the `response_code_rates`
job splits the data based on the `response.keyword`. When you view the results
in the **Anomaly Explorer**, you can see separate swim lanes for the `404` and
`200` responses:

[role="screenshot"]
image::images/ml-gs-job2-explorer.jpg["Anomaly explorer for response_code_rates job"]

In this scenario, the analysis is segmented such that you have completely
different baselines for each distinct value of the _partition field_. By looking
at temporal patterns on a per entity basis, you might spot things that might
have otherwise been hidden in the lumped view.

// TO-DO: Clarify difference between this partition field behaviour and the other jobs' over fields.

Conceptually, you can think of multi-metric jobs as running multiple independent
single metric jobs. By bundling them together in a multi-metric job, however,
you can see an overall score and shared influencers for all the metrics and all
the entities in the job. Multi-metric jobs therefore scale better than having
many independent single metric jobs and provide better results when you have
influencers that are shared across the detectors.

.Influencers
****
When you create an {anomaly-job}, you can identify fields as _influencers_.
These are fields that you think contain information about someone or something
that influences or contributes to anomalies. In the `response_code_rates` job,
there are two influencers: `clientip` and `response.keyword`.

As a best practice, do not pick too many influencers. For example, you generally
do not need more than three. If you pick many influencers, the results can be
overwhelming and there is a small overhead to the analysis. For more details,
see <<ml-influencers>>.

****

On the left side of the **Anomaly Explorer**, there is a list of the top
influencers for all of the detected anomalies in that same time period. The list
includes maximum anomaly scores, which in this case are aggregated for each
influencer, for each bucket, across all detectors. There is also a total sum of
the anomaly scores for each influencer. You can use this list to help you narrow
down the contributing factors and focus on the most anomalous entities.

Click on a section in the swim lanes to obtain more information about the
anomalies in that time period. For example, click on the red section in the
swim lane for the `response.keyword` value of `200`:

[role="screenshot"]
image::images/ml-gs-job2-explorer-anomaly.jpg["Anomaly charts for the response_code_rates job"]

You can see exact times when anomalies occurred. If there are multiple detectors
or metrics in the job, you can see which caught the anomaly. Below the charts,
there is a table that provides more information, such as the typical and actual
values and the influencers that contributed to the anomaly. For example:

[role="screenshot"]
image::images/ml-gs-job2-explorer-table.jpg["Anomaly tables for the response_code_rates job"]

If your job has multiple detectors, the table aggregates the anomalies to show
the highest severity anomaly per detector and entity, which is the field value
that is displayed in the **found for** column. To view all the anomalies without
any aggregation, set the **Interval** to `Show all`.

TIP: The anomaly scores that you see in each section of the **Anomaly Explorer**
might differ slightly. This disparity occurs because for each job there are
bucket results, influencer results, and record results. Anomaly scores are
generated for each type of result. The anomaly timeline uses the bucket-level
anomaly scores. The list of top influencers uses the influencer-level anomaly
scores. The list of anomalies uses the record-level anomaly scores.

By investigating multiple metrics in a single job, you might see relationships
between events in your data that would otherwise be overlooked.

