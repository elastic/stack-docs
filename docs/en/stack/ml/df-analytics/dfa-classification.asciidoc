[role="xpack"]
[[dfa-classification]]
=== {classification-cap}

experimental[]

{classification-cap} is a {ml} process for predicting the class or category of a 
given data point in a data set. Typical examples of {classification} problems are 
predicting loan risk, classifying music, or detecting cancer in a DNA sequence. 
In the first case, for example, our data set consists of data on loan applicants 
that covers investment history, employment status, debit status, and so on. 
Based on historical data, the {classification} analysis predicts whether it is 
safe or risky to lend money to a given loan applicant. In the second case, the 
data we have represents songs and the analysis – based on the features of the 
data points – classifies the songs as hip-hop, country, classical, or any 
other genres available in the set of categories we have. Therefore, 
{classification} is for predicting discrete, categorical values, unlike 
{reganalysis} which predicts continuous, numerical values. The maximum number of 
classes for {classanalysis} is 30.


[discrete]
[[dfa-classification-depvar]]
==== {depvar-cap}

When you create a {classification} job, you must specify which field contains 
the classes that you want to predict. This field is known as the {depvar}. The 
rest of the fields that are included in the analysis are used to create the 
model for predicting the value of the {depvar} (these are the {feature-vars}). 
All {ref}/put-dfanalytics.html#dfa-supported-fields[supported fields] are 
included by default, however, you can explicitly exclude and/or include fields. 
Since the runtime and the necessary resources for the analysis increase as 
the number of included fields grow, consider including only relevant 
fields. For more information about field selection, see 
{ref}/explain-dfanalytics.html[Explain data frame analytics API].


[discrete]
[[dfa-classification-supervised]]
==== Training the {classification} model

{classification-cap} – just like {regression} – is a supervised {ml} process. It 
means that you need to supply a labeled training data set that has a {depvar} 
and some fields that are related to it. The {classification} algorithm learns 
the relationships between these fields and the {depvar}. Once you’ve trained the 
model on your training data set, you can reuse the knowledge that the model has 
learned about the relationships between the data points to classify new data.

The effects of imbalanced data are automatically mitigated before the 
training. Nonetheless, it is a good idea to train your model with a data set that is approximately balanced.
That is to say, ideally your data set should have a similar number of data points for each class.


[discrete]
[[dfa-classification-algorithm]]
===== {classification-cap} algorithms

//tag::classification-algorithms[]
The ensemble algorithm that we use in the {stack} for {classification} is a type 
of boosting called boosted tree regression model which combines multiple weak 
models into a composite one. We use decision trees to learn to predict the 
probability that a data point belongs to a certain class. A sequence of decision 
trees are trained and every decision tree learns from the mistakes of the 
previous one. Every tree is an iteration of the last one, hence it improves the 
decision made by the previous tree.
//end::classification-algorithms[]


[discrete]
[[dfa-classification-performance]]
==== {classification-cap} performance

As a rule of thumb, a {classanalysis} with many classes takes more time to run 
than a binary {classification} process when there are only two classes. The 
relationship between the number of classes and the runtime is linear.

The runtime also scales approximately linearly with the number of involved documents 
below 200.000 data points. Therefore, if you double the number of documents, 
then the runtime of the analysis doubles respectively. Above 200.000 data 
points, runtime increases faster.

To achieve faster runtime, consider to set a smaller training percent, prepare 
If possible, prepare your input data such that is has less classes. You can also remove the fields 
that are not relevant from the analysis by specifying `excludes` patterns in the 
`analyzed_fields` object when configuring the {dfanalytics-job}.
 

[discrete]
[[dfa-classification-interpret]]
==== Interpreting {classification} results

The following sections help you understand and interpret the results of a 
{classanalysis}.

[discrete]
[[dfa-classification-class-probability]]
===== `class_probability`

The value of `class_probability` shows how likely it is that a given data point 
belongs to a certain class. It is a value between 0 and 1. The higher the 
number, the higher the probability that the data point belongs to the named 
class. This information is stored in the `top_classes` array for each document 
in your destination index. See the
{ml-docs}/flightdata-classification.html#flightdata-classification-results[Viewing {classification} results]
section in the {classification} example.


[discrete]
[[dfa-classification-class-score]]
===== `class_score`

The value of `class_score` controls the probability at which a class label is 
assigned to a data point. In normal case – that you maximize the number of 
correct labels – a class label is assigned when its predicted probability is 
greater than 0.5. The `class_score` makes it possible to change this behavior, 
so it can be less than or greater than 0.5. For example, suppose our two classes 
are denoted `class 0` and `class 1`, then the value of `class_score` is always 
non-negative and its definition is:

```
class_score(class 0) = 0.5 / (1.0 - k) * probability(class 0)
class_score(class 1) = 0.5 / k * probability(class 1)
```

Here, `k` is a positive constant less than one. It represents the predicted 
probability of `class 1` for a data point at which to label it `class 1` and is 
chosen to maximize the minimum recall of any class. This is useful for example 
in case of highly imbalanced data. If `class 0` is much more frequent in the 
training data than `class 1`, then it can mean that you achieve the best 
accuracy by assigning `class 0` to every data point. This is equivalent to zero 
recall for `class 1`. Instead of this behavior, the default scheme of the 
{stack} {classanalysis} is to choose `k < 0.5` and accept a higher rate of 
actual `class 0` predicted `class 1` errors, or in other words, a slight 
degradation of the overall accuracy.


[discrete]
[[dfa-classification-feature-importance]]
===== Feature importance

include::{docdir}/shared-ml-concepts.asciidoc[tag=feature-importance]


[discrete]
[[dfa-classification-evaluation]]
==== Measuring model performance

You can measure how well the model has performed on your data set by using the 
`classification` evaluation type of the 
{ref}/evaluate-dfanalytics.html[evaluate {dfanalytics} API]. The metric that the 
evaluation provides you is a confusion matrix. The more classes you have, the 
more complex the confusion matrix is. The matrix tells you how many data points 
that belong to a given class were classified correctly and incorrectly.

Another crucial measurement is how well your model performs on unseen data 
points. To assess how well the trained model will perform on data it has never 
seen before, you must set aside a proportion of the training data set for 
testing. This split of the data set is the _testing data set_. Once the model has 
been trained, you can let the model predict the value of the data points it has 
never seen before and compare the prediction to the actual value by using the 
evaluate {dfanalytics} API.
