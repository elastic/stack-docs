[[ml-nlp-deploy-models]]
= Deploy trained models

:frontmatter-description: You can import trained models into your cluster and configure them for specific NLP tasks.
:frontmatter-tags-products: [ml] 
:frontmatter-tags-content-type: [how-to] 
:frontmatter-tags-user-goals: [analyze]

If you want to perform {nlp} tasks in your cluster, you must deploy an
appropriate trained model. There is tooling support in
https://github.com/elastic/eland[Eland] and {kib} to help you prepare and manage
models.

. <<ml-nlp-select-model,Select a trained model>>.
. <<ml-nlp-import-model,Import the trained model and vocabulary>>.
. <<ml-nlp-deploy-model,Deploy the model in your cluster>>.
. <<ml-nlp-test-inference,Try it out>>.


[[ml-nlp-select-model]]
== Select a trained model

Per the <<ml-nlp-overview>>, there are multiple ways that you can use NLP
features within the {stack}. After you determine which type of NLP task you want
to perform, you must choose an appropriate trained model. 

The simplest method is to use a model that has already been fine-tuned for the
type of analysis that you want to perform. For example, there are models and
data sets available for specific NLP tasks on
https://huggingface.co/models[Hugging Face]. These instructions assume you're
using one of those models and do not describe how to create new models. For the
current list of supported model architectures, refer to <<ml-nlp-model-ref>>.

If you choose to perform {lang-ident} by using the `lang_ident_model_1` that is 
provided in the cluster, no further steps are required to import or deploy the 
model. You can skip to using the model in 
<<ml-nlp-inference,ingestion pipelines>>.


[[ml-nlp-import-model]]
== Import the trained model and vocabulary

IMPORTANT: If you want to install a trained model in a restricted or closed 
network, refer to 
https://www.elastic.co/guide/en/elasticsearch/client/eland/current/machine-learning.html#ml-nlp-pytorch-air-gapped[these instructions].

After you choose a model, you must import it and its tokenizer vocabulary to
your cluster. When you import the model, it must be chunked and imported one
chunk at a time for storage in parts due to its size.

NOTE: Trained models must be in a TorchScript representation for use with
{stack-ml-features}.

https://github.com/elastic/eland[Eland] is an {es} Python client that provides a 
simple script to perform the conversion of Hugging Face transformer models to 
their TorchScript representations, the chunking process, and upload to {es}; it 
is therefore the recommended import method. You can either install the Python 
Eland client on your machine or use a Docker image to build Eland and run the 
model import script.


[discrete]
[[ml-nlp-import-script]]
=== Import with the Eland client installed

. Install the {eland-docs}/installation.html[Eland Python client] with PyTorch 
extra dependencies.
+
--
[source,shell]
--------------------------------------------------
python -m pip install 'eland[pytorch]'
--------------------------------------------------
// NOTCONSOLE
--

. Run the `eland_import_hub_model` script to download the model from Hugging 
Face, convert it to TorchScript format, and upload to the {es} cluster. For 
example:
+
--
[source, shell]
--------------------------------------------------
eland_import_hub_model \
--cloud-id <cloud-id> \ <1>
-u <username> -p <password> \ <2>
--hub-model-id elastic/distilbert-base-cased-finetuned-conll03-english \ <3>
--task-type ner  <4>
--------------------------------------------------
// NOTCONSOLE
--
<1> Specify the Elastic Cloud identifier. Alternatively, use `--url`.
<2> Provide authentication details to access your cluster. Refer to 
<<ml-nlp-authentication>> to learn more.
<3> Specify the identifier for the model in the Hugging Face model hub.
<4> Specify the type of NLP task. Supported values are `fill_mask`, `ner`,
`question_answering`, `text_classification`, `text_embedding`, `text_expansion`,
`text_similarity`, and `zero_shot_classification`.

For more details, refer to 
https://www.elastic.co/guide/en/elasticsearch/client/eland/current/machine-learning.html#ml-nlp-pytorch.

[discrete]
[[ml-nlp-import-docker]]
=== Import with Docker

If you want to use Eland without installing it, run the following command:

```bash
$ docker run -it --rm --network host docker.elastic.co/eland/eland
```

The `eland_import_hub_model` script can be run directly in the docker command:

```bash
docker run -it --rm docker.elastic.co/eland/eland \
    eland_import_hub_model \
      --url $ELASTICSEARCH_URL \
      --hub-model-id elastic/distilbert-base-uncased-finetuned-conll03-english \
      --start
```

Replace the `$ELASTICSEARCH_URL` with the URL for your {es} cluster. Refer to 
<<ml-nlp-authentication>> to learn more.


[[ml-nlp-authentication]]
=== Authentication methods

The following authentication options are available when using the import script:

* username/password authentication (specified with the `-u` and `-p` options):
+
--  
[source, bash]
--------------------------------------------------
eland_import_hub_model --url https://<hostname>:<port> -u <username> -p <password> ...
--------------------------------------------------
--

* username/password authentication (embedded in the URL):
+
--
[source, bash]
--------------------------------------------------
eland_import_hub_model --url https://<user>:<password>@<hostname>:<port> ...
--------------------------------------------------
--
* API key authentication:
+
--
[source, bash]
--------------------------------------------------
eland_import_hub_model --url https://<hostname>:<port> --es-api-key <api-key> ...
--------------------------------------------------
--


[[ml-nlp-deploy-model]]
== Deploy the model in your cluster

After you import the model and vocabulary, you can use {kib} to view and manage
their deployment across your cluster under **{ml-app}** > *Model Management*.
Alternatively, you can use the
{ref}/start-trained-model-deployment.html[start trained model deployment API].

You can deploy a model multiple times by assigning a unique deployment ID when starting the deployment.

You can optimize your deplyoment for typical use cases, such as search and ingest.
When you optimize for ingest, the throughput will be higher, which increases the number of {infer} requests that can be performed in parallel.
When you optimize for search, the latency will be lower during search processes.
When you have dedicated deployments for different purposes, you ensure that the search speed remains unaffected by ingest workloads, and vice versa.
Having separate deployments for search and ingest mitigates performance issues resulting from interactions between the two, which can be hard to diagnose.

[role="screenshot"]
image::images/ml-nlp-deployment-id-elser-v2.png["Model deployment on the Trained Models UI.",width=640]

Each deployment will be fine-tuned automatically based on its specific purpose you choose.

NOTE: Since eland uses APIs to deploy the models, you cannot see the models in {kib} until the saved objects are synchronized.
You can follow the prompts in {kib}, wait for automatic synchronization, or use the {kibana-ref}/machine-learning-api-sync.html[sync {ml} saved objects API].

You can define the resource usage level of the NLP model during model deployment.
The resource usage levels behave differently depending on <<nlp-model-adaptive-resources, adaptive resources>> being enabled or disabled.
When adaptive resources are disabled but {ml} autoscaling is enabled, vCPU usage of Cloud deployments derived from the Cloud console and functions as follows:

* Low: This level limits resources to two vCPUs, which may be suitable for development, testing, and demos depending on your parameters.
It is not recommended for production use
* Medium: This level limits resources to 32 vCPUs, which may be suitable for development, testing, and demos depending on your parameters.
It is not recommended for production use.
* High: This level may use the maximum number of vCPUs available for this deployment from the Cloud console.
If the maximum is 2 vCPUs or fewer, this level is equivalent to the medium or low level.

For the resource levels when adaptive resources are enabled, refer to <<<ml-nlp-auto-scale>>.


[discrete]
[[infer-request-queues]]
=== Request queues and search priority

Each allocation of a model deployment has a dedicated queue to buffer {infer} 
requests. The size of this queue is determined by the `queue_capacity` parameter
in the 
{ref}/start-trained-model-deployment.html[start trained model deployment API]. 
When the queue reaches its maximum capacity, new requests are declined until 
some of the queued requests are processed, creating available capacity once 
again. When multiple ingest pipelines reference the same deployment, the queue 
can fill up, resulting in rejected requests. Consider using dedicated 
deployments to prevent this situation.

{infer-cap} requests originating from search, such as the 
{ref}/query-dsl-text-expansion-query.html[`text_expansion` query], have a higher 
priority compared to non-search requests. The {infer} ingest processor generates 
normal priority requests. If both a search query and an ingest processor use the 
same deployment, the search requests with higher priority skip ahead in the 
queue for processing before the lower priority ingest requests. This 
prioritization accelerates search responses while potentially slowing down 
ingest where response time is less critical.


[[ml-nlp-test-inference]]
== Try it out

When the model is deployed on at least one node in the cluster, you can begin to
perform inference. _{infer-cap}_ is a {ml} feature that enables you to use your
trained models to perform NLP tasks (such as text extraction, classification, or
embeddings) on incoming data.

The simplest method to test your model against new data is to use the
*Test model* action in {kib}. You can either provide some input text or use a 
field of an existing index in your cluster to test the model:

[role="screenshot"]
image::images/ml-nlp-test-ner.png["Testing a sentence with two named entities against a NER trained model in the *{ml}* app",width=640]

Alternatively, you can use the
{ref}/infer-trained-model.html[infer trained model API].
For example, to try a named entity recognition task, provide some sample text:

[source,console]
--------------------------------------------------
POST /_ml/trained_models/elastic__distilbert-base-cased-finetuned-conll03-english/_infer
{
  "docs":[{"text_field": "Sasha bought 300 shares of Acme Corp in 2022."}]
}
--------------------------------------------------
// TEST[skip:TBD]

In this example, the response contains the annotated text output and the
recognized entities:

[source,console-result]
----
{
  "inference_results" : [
    {
      "predicted_value" : "[Sasha](PER&Sasha) bought 300 shares of [Acme Corp](ORG&Acme+Corp) in 2022.",
      "entities" : [
        {
          "entity" : "Sasha",
          "class_name" : "PER",
          "class_probability" : 0.9953193407987492,
          "start_pos" : 0,
          "end_pos" : 5
        },
        {
          "entity" : "Acme Corp",
          "class_name" : "ORG",
          "class_probability" : 0.9996392198381716,
          "start_pos" : 27,
          "end_pos" : 36
        }
      ]
    }
  ]
}
----
// NOTCONSOLE

If you are satisfied with the results, you can add these NLP tasks in your
<<ml-nlp-inference,ingestion pipelines>>.

:!keywords:
:!description:
