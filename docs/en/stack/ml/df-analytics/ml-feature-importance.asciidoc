[role="xpack"]
[[ml-feature-importance]]
= {feat-imp-cap}

beta::[]

{feat-imp-cap} values indicate which fields had the biggest impact on each 
prediction that is generated by {classification} or {regression} analysis. Each
{feat-imp} value has both a magnitude and a direction (positive or negative),
which indicate how each field (or _feature_ of a data point) affects a
particular prediction.

The purpose of {feat-imp} is to help you determine whether the predictions are
sensible. Is the relationship between the dependent variable and the important
features supported by your domain knowledge? The lessons you learn about the
importance of specific features might also affect your decision to include them
in future iterations of your trained model.

You can see the average magnitude of the {feat-imp} values for each field across
all the training data in {kib} or by using the
{ref}/get-inference.html[get trained model API]. For example, {kib} shows the
total feature importance for each field in {regression} or binary
{classanalysis} results as follows:

[role="screenshot"]
image::images/flights-regression-total-importance.jpg["Total {feat-imp} values for a {regression} {dfanalytics-job} in {kib}"]

If the {classanalysis} involves more than two classes, {kib} uses colors to show
how the impact of each field varies by class. For example:

[role="screenshot"]
image::images/diamonds-classification-total-importance.png["Total {feat-imp} values for a {classification} {dfanalytics-job} in {kib}"]

You can also examine the feature importance values for each individual
prediction. In {kib}, you can see these values in JSON objects or decision plots:

[role="screenshot"]
image::images/flights-regression-decision-plot.png["Feature importance values for a {regression} {dfanalytics-job} in {kib}"]

For {reganalysis}, each decision plot starts at a shared baseline, which is
the average of the prediction values for all the data points in the training
data set. When you add all of the feature importance values for a particular
data point to that baseline, you arrive at the numeric prediction value. If a 
{feat-imp} value is negative, it reduces the prediction value. If a {feat-imp}
value is positive, it increases the prediction value.

////
For {classanalysis}, the baseline is the average of the probability values for a
specific class across all the data points in the training data set. When you add
the feature importance values for a particular data point to that baseline, you
arrive at the prediction probability for that class. If a {feat-imp} value is
negative, it reduces the prediction probability. If a {feat-imp} value is
positive, it increases the prediction probability.
////

By default, {feat-imp} values are not calculated. To generate this information,
when you create a {dfanalytics-job} you must specify the
`num_top_feature_importance_values` property. For example, see
<<flightdata-regression>> and <<flightdata-classification>>.

The {feat-imp} values are stored in the {ml} results field for each document in
the destination index. The number of {feat-imp} values for each document might
be less than the `num_top_feature_importance_values` property value. For example,
it returns only features that had a positive or negative effect on the
prediction.

[[ml-feature-importance-readings]]
== Further reading

{feat-imp-cap} in the {stack} is calculated using the SHAP (SHapley Additive 
exPlanations) method as described in
https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf[Lundberg, S. M., & Lee, S.-I. A Unified Approach to Interpreting Model Predictions. In NeurIPS 2017].

See also
https://www.elastic.co/blog/feature-importance-for-data-frame-analytics-with-elastic-machine-learning[{feat-imp-cap} for {dfanalytics} with Elastic {ml}].
