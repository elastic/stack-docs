[[ml-nlp-deploy-models]]
= Import and deploy trained models

:keywords: {ml-init}, {stack}, {nlp}
:description:  To make trained models available for inference, you must import \
and deploy them in {es}.

If you want to perform {nlp} tasks in your cluster, you must import and deploy
an appropriate trained model. There are {es} APIs and tooling support in eland
to prepare and manage models. 

. <<ml-nlp-select-model,Choose a trained model.>>
. <<ml-nlp-upload-model,Upload the trained model and vocabulary.>>
. <<ml-nlp-deploy-model,Deploy the model in your cluster.>>

[discrete]
[[ml-nlp-select-model]]
== Select a trained model

Per <<ml-nlp-overview>>, there are multiple ways that you can use NLP features
within the {stack}. After you determine which type of NLP task you want to
perform, you must choose an appropriate trained model. You can either create the
model yourself or find one that meets your needs.

IMPORTANT: The {stack-ml-features} support transformer models that conform to
the standard BERT model interface and use the WordPiece tokenization algorithm.

The simplest method is to use a model that has already been fine-tuned for the
type of analysis that you want to perform. For example, there are models and 
data sets available for specific NLP tasks in
https://huggingface.co/models[Hugging Face].

[discrete]
[[ml-nlp-upload-model]]
== Upload the trained model and vocabulary

After you find or create a model, you must upload it and its tokenizer
vocabulary to your cluster. 

Trained models must be in a TorchScript representation for use with
{stack-ml-features}. To use scripts that format and upload the necessary
artifacts, refer to https://github.com/elastic/eland#nlp-with-pytorch

Alternatively, you can use the
{ref}/put-trained-model-vocabulary.html[create trained model vocabulary API] and
{ref}/put-trained-models.html[create trained models API] or
{ref}/put-trained-model-definition-part.html[create trained model definition part API].
// When you upload the model, it must be chunked and uploaded one chunk at a time. 
//TBD Why? How?
//Since eland encapsulates this process in a single Python method, it is the recommended method.

[discrete]
[[ml-nlp-deploy-model]]
== Deploy the model in your cluster

After you upload the model and vocabulary, you can use {kib} to view and manage
their deployment across your cluster. Alternatively, you can use the
{ref}/start-trained-model-deployment.html[start trained model deployment API].

When you deploy the model, it is loaded on all available {ml} nodes. You can
optionally specify the number of CPU cores it has access to on each node. In
general, the total size of threading settings across all models on a node should
not exceed the number of physical CPU cores available on the node, minus one
(for non-inference operations). In {ecloud} environments, the core count is
virtualized CPUs (vCPUs) and this total size should typically be no more than
half the available vCPUs, minus one.

The model is loaded into memory in a native process that encapsulates
libtorch, which is an underlying machine learning library for PyTorch.

You can view the allocation status in {kib} or by using the
{ref}/get-trained-models-stats.html[get trained model stats API]. When the
model is deployed on at least one node in the cluster, you can begin to perform
inference. For example, you can use the
{ref}/infer-trained-model-deployment.html[infer trained model deployment API],
{ref}/search-aggregations-pipeline-inference-bucket-aggregation.html[inference bucket aggregation],
or add {ref}/inference-processor.html[inference processors] in your ingest
pipelines.

//TO-DO: Link to expanded inference details