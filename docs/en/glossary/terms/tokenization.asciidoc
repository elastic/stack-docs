
[[glossary-tokenization]] tokenization::
Process of breaking unstructured text down into smaller, searchable chunks
called <<glossary-token,tokens>>. See
{ref}/analysis-overview.html#tokenization[Tokenization].
//Source: Elasticsearch
