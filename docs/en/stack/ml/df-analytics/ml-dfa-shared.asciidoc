tag::dfa-evaluation-intro[]
Using the {dfanalytics} features to gain insights from a data set is an 
iterative process. After you defined the problem you want to solve, and chose 
the analytics type that can help you to do so, you need to produce a 
high-quality data set and create the appropriate {dfanalytics-job}. You might 
need to experiment with different configurations, parameters, and ways to 
transform data before you arrive at a result that satisfies your use case. A 
valuable companion to this process is the 
{ref}/evaluate-dfanalytics.html[{evaluatedf-api}], which enables you to evaluate 
the {dfanalytics} performance. It helps you understand error distributions and 
identifies the points where the {dfanalytics} model performs well or less 
trustworthily.

To evaluate the analysis with this API, you need to annotate your index that 
contains the results of the analysis with a field that marks each document with 
the ground truth. The {evaluatedf-api} evaluates the performance of the 
{dfanalytics} against this manually provided ground truth.
end::dfa-evaluation-intro[]

tag::dfa-inference[]
{infer-cap} is a {ml} feature that enables you to use supervised {ml} processes 
– like {regression} or {classification} – not only as a batch analysis but in a 
continuous fashion. This means that {infer} makes it possible to use 
<<ml-trained-models,trained {ml} models>> against incoming data.

For instance, suppose you have an online service and you would like to predict 
whether a customer is likely to churn. You have an index with historical data – 
information on the customer behavior throughout the years in your business – and 
a {classification} model that is trained on this data. The new information comes 
into a destination index of a {ctransform}. With {infer}, you can perform the 
{classanalysis} against the new data with the same input fields that you've 
trained the model on, and get a prediction.
end::dfa-inference[]

tag::dfa-inference-processor[]
{infer-cap} can be used as a processor specified in an 
{ref}/pipeline.html[ingest pipeline]. It uses a trained model to infer against
the data that is being ingested in the pipeline. The model is used on the
{ref}/ingest.html[ingest node]. {infer-cap} pre-processes the data by using the
model and provides a prediction. After the process, the pipeline continues
executing (if there is any other processor in the pipeline), finally the new
data together with the results are indexed into the destination index.

Check the {ref}/inference-processor.html[{infer} processor] and 
{ref}/ml-df-analytics-apis.html[the {ml} {dfanalytics} API documentation] to 
learn more about the feature.
end::dfa-inference-processor[]

tag::dfa-inference-aggregation[]
{infer-cap} can also be used as a pipeline aggregation. You can reference a 
trained model in the aggregation to infer on the result field of the parent
bucket aggregation. The {infer} aggregation uses the model on the results to
provide a prediction. This aggregation enables you to run {classification} or
{reganalysis} at search time. If you want to perform the analysis on a small set
of data, this aggregation enables you to generate predictions without the need
to set up a processor in the ingest pipeline.

Check the 
{ref}/search-aggregations-pipeline-inference-bucket-aggregation.html[{infer} bucket aggregation] 
and {ref}/ml-df-analytics-apis.html[the {ml} {dfanalytics} API documentation] to 
learn more about the feature.

NOTE: If you use trained model aliases to reference your trained model in an 
{infer} processor or {infer} aggregation, you can replace your trained model 
with a new one without the need of updating the processor or the aggregation. 
Reassign the alias you used to a new trained model ID by using the 
{ref}/put-trained-models-aliases.html[Create or update trained model aliases API].
The new trained model needs to use the same type of {dfanalytics} as the old 
one.
end::dfa-inference-aggregation[]

tag::dfa-model-jvm[]
NOTE: When you create a {dfanalytics-job}, the inference step of the process 
might fail if the model is too large to fit into JVM. For a workaround, refer 
to https://github.com/elastic/elasticsearch/issues/76093[this GitHub issue].
end::dfa-model-jvm[]
