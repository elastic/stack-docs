[role="xpack"]
[[hyperparameters]]
=== Hyperparameter optimization

experimental[]

When you create a {dfanalytics-job} for {classification} or {reganalysis}, there
are advanced configuration options known as _hyperparameters_. The ideal
hyperparameter values vary from one data set to another. Therefore, by default
the job calculates the best combination of values through a process of
_hyperparameter optimization_.

Hyperparameter optimization involves multiple rounds of analysis. Each round
involves a different combination of hyperparameter values, which are determined
through a combination of random search and Bayesian optimization techniques. If
you explicitly set a hyperparameter, that value is not optimized and remains the
same in each round. To determine which round produces the best results,
stratified K-fold cross-validation methods are used to split the data set, train
a model, and calculate its performance on validation data.

You can view the hyperparameter values that were ultimately chosen by expanding
the job details in {kib} or by using the
{ref}/get-dfanalytics-stats.html[get {dfanalytics-job} stats API]. You can also
see the specific type of validation loss (such as mean squared error or binomial
logistic regression) that was used to compare each round of optimization. 

TIP: Unless you fully understand the purpose of a hyperparameter, it is highly
recommended that you leave it unset and allow hyperparameter optimization to
occur.