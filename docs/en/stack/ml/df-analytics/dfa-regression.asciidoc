[role="xpack"]
[[dfa-regression]]
= {regression-cap}

beta[]

{reganalysis-cap} is a {ml} process for estimating the relationships among 
different fields in your data, then making further predictions based on these 
relationships.

For example, suppose we are interested in finding the relationship between 
apartment size and monthly rent in a city. Our imaginary data set consists of 
three data points:

|===
| Size (m2) | Monthly rent 
| 44        | 1600
| 24        | 1055
| 63        | 2300
|===

After the model determines the relationship between the apartment size and the
rent, it can make predictions such as the monthly rent of a hundred square
meter-size apartment.

This is a simple example. Usually {regression} problems are multi-dimensional, 
so the relationships that {reganalysis} tries to find are between multiple 
fields. To extend our example, a more complex {reganalysis} could take into
account additional factors such as the location of the apartment in the city, on
which floor it is, and whether the apartment has a riverside view or not, and so
on. All of these factors can be considered _features_; they are measurable
properties or characteristics of the phenomenon we're studying.

[[dfa-regression-features]]
== {feature-vars-cap}

When you perform {reganalysis}, you must identify a subset of fields that you 
want to use to create a model for predicting other fields. We refer to these 
fields as _feature variables_ and _dependent variables_, respectively.
{feature-vars-cap} are the values that the {depvar} value depends on. If one or 
more of the {feature-vars} changes, the {depvar} value also changes. There are 
three different types of {feature-vars} that you can use with our {regression} 
algorithm:

* Numerical. In our example, the size of the apartment was a 
  numerical {feature-var}.
* Categorical. A variable that can have one value from a set of values. The 
  value set has a fixed and limited number of possible items. In the example, 
  the location of the apartment in the city (borough) is a categorical variable.
* Boolean. The riverside view in the example is a boolean value because an 
  apartment either has a riverside view or doesn't have one.
Arrays are not supported.

[[dfa-regression-supervised]]
== Training the {regression} model

{regression-cap} is a supervised {ml} method, which means that you need to 
supply a labeled training data set that has some {feature-vars} and a {depvar}. 
The {regression} algorithm identifies the relationships between the
{feature-vars} and the {depvar}. Once you've trained the model on your training
data set, you can reuse the knowledge that the model has learned to make
inferences about new data.

The relationships between the {feature-vars} and the {depvar} are described as a 
mathematical function. {reganalysis-cap} tries to find the best prediction for 
the {depvar} by combining the predictions from multiple base learners â€“ 
algorithms that generalize from the data set. The performance of an ensemble is 
usually better than the performance of each individual base learner because the 
individual learners will make different errors. These average out when their 
predictions are combined.

{regression-cap} works as a batch analysis. If new data comes into your index, 
you must restart the {dfanalytics-job}.


[[dfa-regression-algorithm]]
=== {regression-cap} algorithms

//tag::regression-algorithms[]
The ensemble learning technique that we use in the {stack} is a type of boosting 
called extreme gradient boost (XGboost) which combines decision trees with 
gradient boosting methodologies.
//end::regression-algorithms[]

[[dfa-regression-lossfunction]]
=== Loss functions for {regression} analyses

A loss function measures how well a given {ml} model fits the specific data set. 
It boils down all the different under- and overestimations of the model to a 
single number, known as the prediction error. The bigger the difference between 
the prediction and the ground truth, the higher the value of the loss function. 
Loss functions are used automatically in the background during 
<<hyperparameters,hyperparameter optimization>> and when training the decision 
trees to compare the performance of various iterations of the model.

In the {stack}, there are three different types of loss function:

* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]: 
It is the default choice when no additional information about the data set is 
available.
* mean squared logarithmic error (`msle`; a variation of `mse`): It is for 
cases where the target values are all positive with a long tail distribution 
(for example, prices or population).
* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]:
Use it when you want to prevent the model trying to fit the outliers instead of 
regular data.

The various types of loss function calculate the prediction error differently. 
The appropriate loss function for your use case depends on the target 
distribution in your data set, the problem that you want to model, the number of 
outliers in the data, and so on.

You can specify the loss function to be used during {reganalysis} when you 
create the {dfanalytics-job}. The default is mean squared error (`mse`). If you 
choose `msle` or `huber`, you can also set up a parameter for the loss function. 
With the parameter, you can further refine the behavior of the chosen functions.

Consult 
https://github.com/elastic/examples/tree/master/Machine%20Learning/Regression%20Loss%20Functions[the Jupyter notebook on regression loss functions] 
to learn more.

TIP: The default loss function parameter values work fine for most of the cases. 
It is highly recommended to use the default values, unless you fully understand 
the impact of the different loss function parameters.


[[dfa-regression-deploy]]
=== Deploying the model

The model that you created is stored as {es} documents in internal indices. In 
other words, the characteristics of your trained model are saved and ready to be 
deployed and used as functions. The <<ml-inference,{infer}>> feature enables you 
to use your model in a preprocessor of an ingest pipeline or in a pipeline 
aggregation of a search query to make predictions about your data.


[[dfa-regression-feature-importance]]
== {feat-imp-cap}

{feat-imp-cap} provides further information about the results of an analysis and 
helps to interpret the results in a more subtle way. If you want to learn more 
about {feat-imp}, <<ml-feature-importance,click here>>.

[[dfa-regression-evaluation]]
== Measuring model performance

You can measure how well the model has performed on your training data set by 
using the `regression` evaluation type of the 
{ref}/evaluate-dfanalytics.html[evaluate {dfanalytics} API]. The mean squared 
error (MSE) value that the evaluation provides you on the training data set is 
the _training error_. Training the {regression} model means finding the 
combination of model parameters that produces the lowest possible training 
error.

Another crucial measurement is how well your model performs on unseen 
data points. To assess how well the trained model will perform on data it has 
never seen before, you must set aside a proportion of the training data set for 
testing. This split of the data set is the testing data set. Once the model has 
been trained, you can let the model 
predict the value of the data points it has never seen before and compare the 
prediction to the actual value. This test provides an estimate of a quantity 
known as the _model generalization error_.

Two concepts describe how well the {regression} algorithm was able to learn the 
relationship between the {feature-vars} and the {depvar}. _Underfitting_ is when 
the model cannot capture the complexity of the data set. _Overfitting_ is when 
the model is too specific to the training data set and is capturing details 
which do not generalize to new data. A model that overfits the data has a 
low MSE value on the training data set and a high MSE value on the testing 
data set. For more information about the evaluation metrics, see 
<<ml-dfanalytics-regression-evaluation>>.

[[dfa-regression-readings]]
== Further readings

* https://github.com/elastic/examples/tree/master/Machine%20Learning/Feature%20Importance[Feature importance for {dfanalytics} (Jupyter notebook)]

* https://github.com/elastic/examples/tree/master/Machine%20Learning/Regression%20Loss%20Functions[Regression loss functions (Jupyter notebook)]