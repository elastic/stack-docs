[[ml-nlp-deploy-models]]
= Deploy trained models

:keywords: {ml-init}, {stack}, {nlp}
:description: You can import trained models into your cluster and configure them \
for specific NLP tasks.

If you want to perform {nlp} tasks in your cluster, you must deploy an
appropriate trained model. There is tooling support in
https://github.com/elastic/eland[Eland] and {kib} to help you prepare and manage
models.

. <<ml-nlp-select-model,Select a trained model>>.
. <<ml-nlp-import-model,Import the trained model and vocabulary>>.
. <<ml-nlp-deploy-model,Deploy the model in your cluster>>.
. <<ml-nlp-test-inference,Try it out>>.

[discrete]
[[ml-nlp-select-model]]
== Select a trained model

Per the <<ml-nlp-overview>>, there are multiple ways that you can use NLP
features within the {stack}. After you determine which type of NLP task you want
to perform, you must choose an appropriate trained model. 

The simplest method is to use a model that has already been fine-tuned for the
type of analysis that you want to perform. For example, there are models and
data sets available for specific NLP tasks on
https://huggingface.co/models[Hugging Face]. These instructions assume you're
using one of those models and do not describe how to create new models. For the
current list of supported model architectures, refer to <<ml-nlp-model-ref>>.

If you choose to perform {lang-ident} by using the `lang_ident_model_1` that is 
provided in the cluster, no further steps are required to import or deploy the 
model. You can skip to using the model in 
<<ml-nlp-inference,ingestion pipelines>>.

[discrete]
[[ml-nlp-import-model]]
== Import the trained model and vocabulary

After you choose a model, you must import it and its tokenizer vocabulary to
your cluster. When you import the model, it must be chunked and imported one
chunk at a time for storage in parts due to its size.

NOTE: Trained models must be in a TorchScript representation for use with
{stack-ml-features}.

Eland encapsulates both the conversion of Hugging Face transformer models to
their TorchScript representations and the chunking process in a single Python
method; it is therefore the recommended import method.

. Install the {eland-docs}/installation.html[Eland Python client] with PyTorch 
extra dependencies.
+
--
[source,shell]
--------------------------------------------------
python -m pip install 'eland[pytorch]'
--------------------------------------------------
// NOTCONSOLE
--

. Run the `eland_import_hub_model` script. For example:
+
--
[source, shell]
--------------------------------------------------
eland_import_hub_model 
--cloud-id <cloud-id> \ <1>
-u <username> -p <password> \ <2>
--hub-model-id elastic/distilbert-base-cased-finetuned-conll03-english \ <3>
--task-type ner \ <4>
--------------------------------------------------
// NOTCONSOLE
--
<1> Specify the Elastic Cloud identifier. Alternatively, use `--url`.
<2> Provide authentication details to access your cluster. Refer to 
<<authentication>> to learn more.
<3> Specify the identifier for the model in the Hugging Face model hub.
<4> Specify the type of NLP task. Supported values are `fill_mask`, `ner`,
`text_classification`, `text_embedding`, and `zero_shot_classification`.

For more details, refer to https://github.com/elastic/eland#nlp-with-pytorch.

[discrete]
[[authentication]]
=== Authentication methods

The following authentication options are available when using the import script:

* username/password authentication (specified with the `-u` and `-p` options):
+
--  
[source, shell]
--------------------------------------------------
eland_import_hub_model --url https://<hostname>:<port> -u <username> -p <password> ...
--------------------------------------------------
--

* username/password authentication (embedded in the URL):
+
--
[source, shell]
--------------------------------------------------
eland_import_hub_model --url https://<user>:<password>@<hostname>:<port> ...
--------------------------------------------------
--
* API key authentication:
+
--
[source, shell]
--------------------------------------------------
eland_import_hub_model --url https://<hostname>:<port> --es-api-key <api-key> ...
--------------------------------------------------
--

[discrete]
[[ml-nlp-deploy-model]]
== Deploy the model in your cluster

After you import the model and vocabulary, you can use {kib} to view and manage
their deployment across your cluster under **{ml-app}** > *Model Management*.
Alternatively, you can use the
{ref}/start-trained-model-deployment.html[start trained model deployment API] or
specify the `--start` option when you run the `eland_import_hub_model` script.

NOTE: Since eland uses APIs to deploy the models, you cannot see the models in
{kib} until the saved objects are synchronized. You can follow the prompts in
{kib}, wait for automatic synchronization, or use the
{kibana-ref}/machine-learning-api-sync.html[sync {ml} saved objects API].

When you deploy the model, its allocations are distributed across available {ml} 
nodes. Model allocations are independent units of work for NLP tasks. Throughput 
can be scaled by adding more allocations to the deployment; it increases the 
number of {infer} requests that can be performed in parallel. All allocations 
assigned to a node share the same copy of the model in memory. The model is 
loaded into memory in a native process that encapsulates `libtorch`, which is 
the underlying {ml} library of PyTorch.

You can set the number of allocations and the number of threads an allocation 
uses for your deployment to affect model performance.

The number of allocations setting affects the amount of model allocations across 
all the {ml} nodes. Increasing the number of allocations generally increases the 
throughput. Model allocations are distributed in such a way that the total 
number of used threads does not exceed the allocated processors of a node.

The threads per allocation setting affects the number of threads used by each 
model allocation during {infer}. Increasing the number of threads generally 
increases the speed of {infer} requests. The value of this setting must not 
exceed the number of available allocated processors per node.

You can view the allocation status in {kib} or by using the
{ref}/get-trained-models-stats.html[get trained model stats API]. If you to
change the number of allocations, you can use the
{ref}/update-trained-model-deployment.html[update trained model stats API] after
the allocation status is `started`.


[discrete]
[[ml-nlp-deploy-model-air-gapped]]
=== Deploy an Elastic NLP model in an air-gapped environment

If you want to deploy a downloadable NLP model provided and trained by Elastic 
in a restricted or closed network, you have two options:

* create your own HTTP/HTTPS endpoint with the model artifacts on it,
* put the model artifacts into a directory inside the config directory on all 
{ref}/modules-node.html#master-node[master-eligible nodes].

For example, for <<ml-nlp-elser,ELSER>>, you need the following files in your 
system:

```
https://ml-models.elastic.co/elser_model_1.metadata.json
https://ml-models.elastic.co/elser_model_1.pt
https://ml-models.elastic.co/elser_model_1.vocab.json
```


[discrete]
==== Using an HTTP server

INFO: If you use an existing HTTP server, note that the model downloader only 
supports passwordless HTTP servers.

You can use any HTTP service to deploy a model. This example uses the official 
Nginx Docker image to set a new HTTP download service up.

. Download the model artifact files from https://ml-models.elastic.co/.
. Put the files into a subdirectory of your choice.
. Run the following commands:
+
--
[source, shell]
--------------------------------------------------
export ELASTIC_ML_MODELS="/path/to/models"
docker run --rm -d -p 8080:80 --name ml-models -v ${ELASTIC_ML_MODELS}:/usr/share/nginx/html nginx
--------------------------------------------------

Don't forget to change `/path/to/models` to the path of the subdirectory where 
the model artifact files are located.

These commands start a local Docker image with an Nginx server with the 
subdirectory containing the model files. As the Docker image has to be 
downloaded and built, the first start might take a longer period of time. 
Subsequent runs start quicker.
--
. Verify that Nginx runs properly by visiting the following URL in your 
browser:
+
--
```
http://{IP_ADDRESS}:8080/elser_model_1.metadata.json
```

This example uses the ELSER model artefacts. If you want to work with another 
model provided by Elastic, modify the URL accordingly.

If Nginx runs properly, you see the content of the metdata file of the model.
--
. Point your Elasticsearch deployment to the model artifacts on the HTTP server
by adding the following line to the `config/elasticsearch.yml` file: 
+
--
```
xpack.ml.model_repository: http://{IP_ADDRESS}:8080
```
--
. Repeat step 5 on all master-eligible nodes.
. {ref}/restart-cluster.html#restart-cluster-rolling[Restart] the 
master-eligible nodes one by one. 

The HTTP server is only required for downloading the model. After the download 
has finished, you can stop and delete the service. You can stop the Docker image 
used in this example by running the following command:

[source, shell]
--------------------------------------------------
docker stop ml-models
--------------------------------------------------


[discrete]
==== Using file-based access

For a file-based access, follow these steps:

. Download the model artifact files from https://ml-models.elastic.co/.
. Put the files into a `models` subdirectory inside the `config` directory of 
your Elasticsearch deployment.
. Point your Elasticsearch deployment to the model directory by adding the 
following line to the `config/elasticsearch.yml` file:
+
--
```
xpack.ml.model_repository: file://${path.home}/config/models/`
```
--
. Repeat step 2 and step 3 on all master-eligible nodes.
. {ref}/restart-cluster.html#restart-cluster-rolling[Restart] the 
master-eligible nodes one by one.


[discrete]
[[ml-nlp-test-inference]]
== Try it out

When the model is deployed on at least one node in the cluster, you can begin to
perform inference. _{infer-cap}_ is a {ml} feature that enables you to use your
trained models to perform NLP tasks (such as text extraction, classification, or
embeddings) on incoming data.

The simplest method to test your model against new data is to use the
*Test model* action in {kib}. You can either provide some input text or use a 
field of an existing index in your cluster to test the model:

[role="screenshot"]
image::images/ml-nlp-test-ner.png[Testing a sentence with two named entities against a NER trained model in the *{ml}* app]

Alternatively, you can use the
{ref}/infer-trained-model.html[infer trained model API].
For example, to try a named entity recognition task, provide some sample text:

[source,console]
--------------------------------------------------
POST /_ml/trained_models/elastic__distilbert-base-cased-finetuned-conll03-english/_infer
{
  "docs":[{"text_field": "Sasha bought 300 shares of Acme Corp in 2022."}]
}
--------------------------------------------------
// TEST[skip:TBD]

In this example, the response contains the annotated text output and the
recognized entities:

[source,console-result]
----
{
  "inference_results" : [
    {
      "predicted_value" : "[Sasha](PER&Sasha) bought 300 shares of [Acme Corp](ORG&Acme+Corp) in 2022.",
      "entities" : [
        {
          "entity" : "Sasha",
          "class_name" : "PER",
          "class_probability" : 0.9953193407987492,
          "start_pos" : 0,
          "end_pos" : 5
        },
        {
          "entity" : "Acme Corp",
          "class_name" : "ORG",
          "class_probability" : 0.9996392198381716,
          "start_pos" : 27,
          "end_pos" : 36
        }
      ]
    }
  ]
}
----
// NOTCONSOLE

If you are satisfied with the results, you can add these NLP tasks in your
<<ml-nlp-inference,ingestion pipelines>>.
