[role="xpack"]
[[ml-gs-jobs]]
=== Create sample {ml} jobs in {kib}
++++
<titleabbrev>Create jobs</titleabbrev>
++++

Now that you're familiar with the data in the `kibana_sample_data_logs` index,
you can create some {ml} jobs to analyze it.

The {kib} sample data sets include some pre-configured {anomaly-jobs} for you to
play with. You can use either of the following methods to add the jobs:

* After you load the sample web logs data set on the {kib} home page, click
*View data* > *ML jobs*.
* In the Machine Learning app, when you select the `kibana_sample_data_logs`
index pattern in the *Data Visualizer* or the *Anomaly Detection* job wizards,
it recommends that you create a job using its known configuration. Select the
*Kibana sample data web logs* configuration.

[role="screenshot"]
image::images/ml-gs-create-web-jobs-1.jpg["Create jobs for the sample web logs"]

Accept the default values and click *Create Jobs*.

The wizard creates three jobs and three {dfeeds}. <<ml-dfeeds,{dfeeds-cap}>>
retrieve data from {es} indices and provide it to {anomaly-jobs} for analysis.
When you think about how {anomaly-jobs} process data, it's also important to
understand the concept of <<ml-buckets,buckets>>, which are used to divide a
time series into batches for processing. All three jobs for the sample web logs
use a bucket span of 1 hour.

{anomaly-jobs-cap} contain one or more _detectors_, which define the type of
analysis that occurs (for example, `max`, `average`, or `rare` analytical
functions) and the fields that are analyzed. Some of the analytical functions
look for single anomalous data points. For example, `max` identifies the maximum
value that is seen within a bucket. Others perform some aggregation over the
length of the bucket. For example, `mean` calculates the mean of all the data
points seen within the bucket. For descriptions of all the functions, see
<<ml-functions>>.

If you want to see all of the configuration details for your jobs and {dfeeds},
you can do so on the *Machine Learning* > *Anomaly Detection* > *Job Management*
page. For the purposes of this tutorial, however, here's a quick overview of the
goal of each job:

* `low_request_rate` uses the `low_count` function to find unusually low request
rates
* `response_code_rates` uses the `count` function and partitions the analysis by
`response.keyword` values to find unusual event rates by HTTP response code
* `url_scanning` uses the `high_distinct_count` function and performs population
analysis on the `clientip` field to find client IPs accessing an unusually high
distinct count of URLs

The next step is to view the results and see what types of insights these jobs
have generated!